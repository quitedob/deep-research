==== docs ====
---- API.md ----
# DeerFlow API Documentation
## Overview
DeerFlow API is a comprehensive backend service for advanced research and content generation. It provides endpoints for chat-based research, text-to-speech conversion, content generation (podcasts, presentations, prose), prompt enhancement, and RAG (Retrieval-Augmented Generation) functionality.
**API Version:** 0.1.0
**Base URL:** `http://localhost:8000`
---
## Authentication
Currently, the API does not require authentication. CORS is configured with origins defined by the `ALLOWED_ORIGINS` environment variable (default: `http://localhost:3000`).
---
## API Endpoints
### Chat & Research
#### `POST /api/chat/stream`
Initiates a streaming chat session with the research agent. Returns Server-Sent Events (SSE) with message chunks, tool calls, and intermediate results.
**Request Body:**
```json
{
"messages": [
{
"role": "user",
"content": "What is quantum computing?"
}
],
"thread_id": "__default__",
"max_plan_iterations": 1,
"max_step_num": 3,
"max_search_results": 3,
"auto_accepted_plan": false,
"report_style": "ACADEMIC",
"enable_background_investigation": true,
"enable_deep_thinking": false,
"enable_clarification": false
}
```
**Query Parameters:**
- `messages` (required, array[ChatMessage]): History of messages between user and assistant
- `resources` (optional, array[Resource]): Resources for the research
- `thread_id` (optional, string, default: `"__default__"`): Conversation identifier
- `max_plan_iterations` (optional, integer, default: 1): Maximum number of plan iterations
- `max_step_num` (optional, integer, default: 3): Maximum number of steps in a plan
- `max_search_results` (optional, integer, default: 3): Maximum number of search results
- `auto_accepted_plan` (optional, boolean, default: false): Automatically accept the plan
- `interrupt_feedback` (optional, string): User feedback on the plan
- `mcp_settings` (optional, object): MCP settings (requires `ENABLE_MCP_SERVER_CONFIGURATION=true`)
- `enable_background_investigation` (optional, boolean, default: true): Get background investigation before plan
- `report_style` (optional, enum): Style of the report - `ACADEMIC`, `POPULAR_SCIENCE`, `NEWS`, `SOCIAL_MEDIA`, `STRATEGIC_INVESTMENT`
- `enable_deep_thinking` (optional, boolean, default: false): Enable deep thinking
- `enable_clarification` (optional, boolean): Enable multi-turn clarification
- `max_clarification_rounds` (optional, integer): Maximum clarification rounds
**Response:**
- Content-Type: `text/event-stream`
- Server-sent events with various message types:
- `message_chunk`: Raw message tokens
- `tool_calls`: Tool invocations
- `tool_call_chunks`: Partial tool call data
- `tool_call_result`: Result of a tool call
- `interrupt`: Plan interruption for user feedback
- `error`: Error messages
**Example Response (Server-Sent Events):**
```
event: message_chunk
data: {"thread_id":"abc123","agent":"researcher","role":"assistant","content":"I'll search for information about quantum computing..."}
event: tool_calls
data: {"thread_id":"abc123","agent":"researcher","tool_calls":[{"name":"web_search","args":"{\"query\":\"quantum computing\"}"}]}
event: tool_call_result
data: {"thread_id":"abc123","agent":"researcher","content":"Found 10 results about quantum computing"}
```
**Error Responses:**
- `403`: MCP server configuration is disabled
- `500`: Internal server error during graph execution
---
### Text-to-Speech
#### `POST /api/tts`
Converts text to speech using Volcengine TTS API.
**Requirements:**
- Environment variables must be set:
- `VOLCENGINE_TTS_APPID`
- `VOLCENGINE_TTS_ACCESS_TOKEN`
**Request Body:**
```json
{
"text": "Hello, this is a test",
"encoding": "mp3",
"voice_type": "BV700_V2_streaming",
"speed_ratio": 1.0,
"volume_ratio": 1.0,
"pitch_ratio": 1.0,
"text_type": "plain",
"with_frontend": 1,
"frontend_type": "unitTson"
}
```
**Parameters:**
- `text` (required, string, max 1024 chars): Text to convert to speech
- `encoding` (optional, string, default: `"mp3"`): Audio format - `mp3`, `wav`
- `voice_type` (optional, string, default: `"BV700_V2_streaming"`): Voice type
- `speed_ratio` (optional, float, default: 1.0): Speech speed ratio
- `volume_ratio` (optional, float, default: 1.0): Speech volume ratio
- `pitch_ratio` (optional, float, default: 1.0): Speech pitch ratio
- `text_type` (optional, string, default: `"plain"`): `plain` or `ssml`
- `with_frontend` (optional, integer, default: 1): Enable frontend processing
- `frontend_type` (optional, string, default: `"unitTson"`): Frontend type
**Response:**
- Content-Type: `audio/mp3` or `audio/wav` (depends on encoding)
- Binary audio data
- Header: `Content-Disposition: attachment; filename=tts_output.{encoding}`
**Error Responses:**
- `400`: Missing required environment variables
- `500`: Internal server error during TTS processing
---
### Content Generation
#### `POST /api/podcast/generate`
Generates an audio podcast from provided text content.
**Request Body:**
```json
{
"content": "# Podcast Content\nThis is the content of the podcast..."
}
```
**Parameters:**
- `content` (required, string): Podcast content in text format
**Response:**
- Content-Type: `audio/mp3`
- Binary audio data
**Error Responses:**
- `500`: Error during podcast generation
---
#### `POST /api/ppt/generate`
Generates a PowerPoint presentation from provided content.
**Request Body:**
```json
{
"content": "# Presentation Title\n## Slide 1\nContent here..."
}
```
**Parameters:**
- `content` (required, string): Content for the presentation
**Response:**
- Content-Type: `application/vnd.openxmlformats-officedocument.presentationml.presentation`
- Binary PowerPoint file (.pptx)
- Header: `Content-Disposition: attachment; filename=output.pptx`
**Error Responses:**
- `500`: Error during PPT generation
---
#### `POST /api/prose/generate`
Generates prose content with streaming output based on prompt and option.
**Request Body:**
```json
{
"prompt": "Write a creative story about",
"option": "story",
"command": "make it exciting"
}
```
**Parameters:**
- `prompt` (required, string): Content/prompt for prose generation
- `option` (required, string): Prose writing option
- `command` (optional, string, default: `""`): User custom command
**Response:**
- Content-Type: `text/event-stream`
- Server-sent events with prose content chunks
**Example Response:**
```
data: "Once upon a time, there was..."
data: " a mysterious kingdom..."
```
**Error Responses:**
- `500`: Error during prose generation
---
### Prompt Enhancement
#### `POST /api/prompt/enhance`
Enhances and refines user prompts with specified report style and context.
**Request Body:**
```json
{
"prompt": "Tell me about climate change",
"context": "For a scientific audience",
"report_style": "ACADEMIC"
}
```
**Parameters:**
- `prompt` (required, string): Original prompt to enhance
- `context` (optional, string, default: `""`): Additional context about intended use
- `report_style` (optional, string, default: `"academic"`): Style - `academic`, `popular_science`, `news`, `social_media`, `strategic_investment`
**Response:**
```json
{
"result": "Enhanced and refined prompt here..."
}
```
**Error Responses:**
- `500`: Error during prompt enhancement
---
### MCP Integration
#### `POST /api/mcp/server/metadata`
Retrieves metadata and available tools from a Model Context Protocol (MCP) server.
**Requirements:**
- Environment variable: `ENABLE_MCP_SERVER_CONFIGURATION=true`
**Request Body - For stdio transport:**
```json
{
"transport": "stdio",
"command": "python",
"args": ["-m", "mcp_server"],
"env": {
"VAR_NAME": "value"
},
"timeout_seconds": 300
}
```
**Request Body - For SSE transport:**
```json
{
"transport": "sse",
"url": "https://mcp-server.example.com",
"headers": {
"Authorization": "Bearer token"
}
}
```
**Parameters:**
- `transport` (required, string): `stdio`, `sse`, or `streamable_http`
- `command` (optional, string): Command to execute (stdio type)
- `args` (optional, array[string]): Command arguments (stdio type)
- `url` (optional, string): Server URL (sse/streamable_http type)
- `env` (optional, object): Environment variables (stdio type)
- `headers` (optional, object): HTTP headers (sse/streamable_http type)
- `timeout_seconds` (optional, integer): Custom timeout in seconds (default: 300)
**Response:**
```json
{
"transport": "stdio",
"command": "python",
"args": ["-m", "mcp_server"],
"tools": [
{
"name": "tool_1",
"description": "Description of tool",
"parameters": {}
}
]
}
```
**Error Responses:**
- `403`: MCP server configuration is disabled
- `500`: Error retrieving MCP server metadata
---
### RAG Configuration
#### `GET /api/rag/config`
Returns the current RAG (Retrieval-Augmented Generation) provider configuration.
**Response:**
```json
{
"provider": "ragflow"
}
```
**Error Responses:**
- None (always returns 200)
---
#### `GET /api/rag/resources`
Retrieves available resources from the RAG system based on optional query.
**Query Parameters:**
- `query` (optional, string): Search query for resources
**Response:**
```json
{
"resources": [
{
"id": "resource_1",
"name": "Document",
"type": "pdf"
}
]
}
```
**Error Responses:**
- None (returns empty resources array if retriever not configured)
---
### Server Configuration
#### `GET /api/config`
Returns the complete server configuration including RAG settings and available models.
**Response:**
```json
{
"rag": {
"provider": "ragflow"
},
"models": {
"llm": ["gpt-4", "gpt-3.5-turbo"],
"embedding": ["openai-embedding"]
}
}
```
**Error Responses:**
- None (always returns 200)
---
## Data Structures
### ChatMessage
```json
{
"role": "user or assistant",
"content": "string or array of ContentItem"
}
```
### ContentItem
```json
{
"type": "text or image",
"text": "string (for text type)",
"image_url": "string (for image type)"
}
```
### ReportStyle Enum
- `ACADEMIC`
- `POPULAR_SCIENCE`
- `NEWS`
- `SOCIAL_MEDIA`
- `STRATEGIC_INVESTMENT`
---
## Error Handling
All endpoints follow standard HTTP status codes:
| Status | Meaning |
|--------|---------|
| 200 | Success |
| 400 | Bad Request - Invalid parameters |
| 403 | Forbidden - Feature disabled or unauthorized |
| 500 | Internal Server Error |
Error response format:
```json
{
"detail": "Error message describing what went wrong"
}
```
---
## Streaming Responses
Several endpoints return streaming responses using Server-Sent Events (SSE):
- `/api/chat/stream` - Chat streaming
- `/api/prose/generate` - Prose generation streaming
To consume SSE in your client:
```javascript
const eventSource = new EventSource('/api/chat/stream', {
method: 'POST',
headers: {
'Content-Type': 'application/json'
},
body: JSON.stringify({...})
});
eventSource.addEventListener('message_chunk', (event) => {
console.log(event.data);
});
```
---
## Rate Limiting & Quotas
Currently no rate limiting is implemented. The system respects the following limits:
- TTS text input: max 1024 characters
- Search results: configurable via `max_search_results`
- Plan iterations: configurable via `max_plan_iterations`
---
## Environment Variables
Key environment variables for API configuration:
```bash
# CORS
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173
# TTS
VOLCENGINE_TTS_APPID=your_app_id
VOLCENGINE_TTS_ACCESS_TOKEN=your_access_token
VOLCENGINE_TTS_CLUSTER=volcano_tts
VOLCENGINE_TTS_VOICE_TYPE=BV700_V2_streaming
# MCP
ENABLE_MCP_SERVER_CONFIGURATION=false
# Checkpointing
LANGGRAPH_CHECKPOINT_SAVER=false
LANGGRAPH_CHECKPOINT_DB_URL=postgresql://user:pass@localhost/db
# RAG
RAG_PROVIDER=ragflow
```
---
## Examples
### Example 1: Chat with Research
```bash
curl -X POST http://localhost:8000/api/chat/stream \
-H "Content-Type: application/json" \
-d '{
"messages": [{"role": "user", "content": "What are the latest AI trends?"}],
"thread_id": "conversation_1",
"max_search_results": 5,
"report_style": "POPULAR_SCIENCE"
}'
```
### Example 2: Text-to-Speech
```bash
curl -X POST http://localhost:8000/api/tts \
-H "Content-Type: application/json" \
-d '{
"text": "Hello world",
"encoding": "mp3",
"speed_ratio": 1.2
}' \
--output audio.mp3
```
### Example 3: Enhance Prompt
```bash
curl -X POST http://localhost:8000/api/prompt/enhance \
-H "Content-Type: application/json" \
-d '{
"prompt": "Tell me about space",
"context": "For kids aged 8-10",
"report_style": "POPULAR_SCIENCE"
}'
```
### Example 4: Get Server Configuration
```bash
curl -X GET http://localhost:8000/api/config
```
---
## Changelog
### Version 0.1.0
- Initial API release
- Chat streaming with research capabilities
- Text-to-speech conversion
- Content generation (podcasts, presentations, prose)
- Prompt enhancement
- MCP server integration
- RAG configuration and resources
---
## Support
For issues or questions about the API, please refer to the project documentation or file an issue in the repository.
---- FAQ.md ----
# FAQ
## Table of Contents
- [Where's the name DeerFlow come from?](#wheres-the-name-deerflow-come-from)
- [Which models does DeerFlow support?](#which-models-does-deerflow-support)
## Where's the name DeerFlow come from?
DeerFlow is short for **D**eep **E**xploration and **E**fficient **R**esearch **Flow**. It is named after the deer, which is a symbol of gentleness and elegance. We hope DeerFlow can bring a gentle and elegant deep research experience to you.
## Which models does DeerFlow support?
Please refer to the [Configuration Guide](configuration_guide.md) for more details.
---- configuration_guide.md ----
# Configuration Guide
## Quick Settings
Copy the `conf.yaml.example` file to `conf.yaml` and modify the configurations to match your specific settings and requirements.
```bash
cd deer-flow
cp conf.yaml.example conf.yaml
```
## Which models does DeerFlow support?
In DeerFlow, we currently only support non-reasoning models. This means models like OpenAI's o1/o3 or DeepSeek's R1 are not supported yet, but we plan to add support for them in the future. Additionally, all Gemma-3 models are currently unsupported due to the lack of tool usage capabilities.
### Supported Models
`doubao-1.5-pro-32k-250115`, `gpt-4o`, `qwen-max-latest`,`qwen3-235b-a22b`,`qwen3-coder`, `gemini-2.0-flash`, `deepseek-v3`, and theoretically any other non-reasoning chat models that implement the OpenAI API specification.
### Local Model Support
DeerFlow supports local models through OpenAI-compatible APIs:
- **Ollama**: `http://localhost:11434/v1` (tested and supported for local development)
See the `conf.yaml.example` file for detailed configuration examples.
> [!NOTE]
> The Deep Research process requires the model to have a **longer context window**, which is not supported by all models.
> A work-around is to set the `Max steps of a research plan` to `2` in the settings dialog located on the top right corner of the web page,
> or set `max_step_num` to `2` when invoking the API.
### How to switch models?
You can switch the model in use by modifying the `conf.yaml` file in the root directory of the project, using the configuration in the [litellm format](https://docs.litellm.ai/docs/providers/openai_compatible).
---
### How to use OpenAI-Compatible models?
DeerFlow supports integration with OpenAI-Compatible models, which are models that implement the OpenAI API specification. This includes various open-source and commercial models that provide API endpoints compatible with the OpenAI format. You can refer to [litellm OpenAI-Compatible](https://docs.litellm.ai/docs/providers/openai_compatible) for detailed documentation.
The following is a configuration example of `conf.yaml` for using OpenAI-Compatible models:
```yaml
# An example of Doubao models served by VolcEngine
BASIC_MODEL:
base_url: "https://ark.cn-beijing.volces.com/api/v3"
model: "doubao-1.5-pro-32k-250115"
api_key: YOUR_API_KEY
# An example of Aliyun models
BASIC_MODEL:
base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
model: "qwen-max-latest"
api_key: YOUR_API_KEY
# An example of deepseek official models
BASIC_MODEL:
base_url: "https://api.deepseek.com"
model: "deepseek-chat"
api_key: YOUR_API_KEY
# An example of Google Gemini models using OpenAI-Compatible interface
BASIC_MODEL:
base_url: "https://generativelanguage.googleapis.com/v1beta/openai/"
model: "gemini-2.0-flash"
api_key: YOUR_API_KEY
```
The following is a configuration example of `conf.yaml` for using best opensource OpenAI-Compatible models:
```yaml
# Use latest deepseek-v3 to handle basic tasks, the open source SOTA model for basic tasks
BASIC_MODEL:
base_url: https://api.deepseek.com
model: "deepseek-v3"
api_key: YOUR_API_KEY
temperature: 0.6
top_p: 0.90
# Use qwen3-235b-a22b to handle reasoning tasks, the open source SOTA model for reasoning
REASONING_MODEL:
base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
model: "qwen3-235b-a22b-thinking-2507"
api_key: YOUR_API_KEY
temperature: 0.6
top_p: 0.90
# Use qwen3-coder-480b-a35b-instruct to handle coding tasks, the open source SOTA model for coding
CODE_MODEL:
base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
model: "qwen3-coder-480b-a35b-instruct"
api_key: YOUR_API_KEY
temperature: 0.6
top_p: 0.90
```
In addition, you need to set the `AGENT_LLM_MAP` in `src/config/agents.py` to use the correct model for each agent. For example:
```python
# Define agent-LLM mapping
AGENT_LLM_MAP: dict[str, LLMType] = {
"coordinator": "reasoning",
"planner": "reasoning",
"researcher": "reasoning",
"coder": "basic",
"reporter": "basic",
"podcast_script_writer": "basic",
"ppt_composer": "basic",
"prose_writer": "basic",
"prompt_enhancer": "basic",
}
### How to use Google AI Studio models?
DeerFlow supports native integration with Google AI Studio (formerly Google Generative AI) API. This provides direct access to Google's Gemini models with their full feature set and optimized performance.
To use Google AI Studio models, you need to:
1. Get your API key from [Google AI Studio](https://aistudio.google.com/app/apikey)
2. Set the `platform` field to `"google_aistudio"` in your configuration
3. Configure your model and API key
The following is a configuration example for using Google AI Studio models:
```yaml
# Google AI Studio native API (recommended for Google models)
BASIC_MODEL:
platform: "google_aistudio"
model: "gemini-2.5-flash" # or "gemini-1.5-pro" ,...
api_key: YOUR_GOOGLE_API_KEY # Get from https://aistudio.google.com/app/apikey
```
**Note:** The `platform: "google_aistudio"` field is required to distinguish from other providers that may offer Gemini models through OpenAI-compatible APIs.
```
### How to use models with self-signed SSL certificates?
If your LLM server uses self-signed SSL certificates, you can disable SSL certificate verification by adding the `verify_ssl: false` parameter to your model configuration:
```yaml
BASIC_MODEL:
base_url: "https://your-llm-server.com/api/v1"
model: "your-model-name"
api_key: YOUR_API_KEY
verify_ssl: false # Disable SSL certificate verification for self-signed certificates
```
> [!WARNING]
> Disabling SSL certificate verification reduces security and should only be used in development environments or when you trust the LLM server. In production environments, it's recommended to use properly signed SSL certificates.
### How to use Ollama models?
DeerFlow supports the integration of Ollama models. You can refer to [litellm Ollama](https://docs.litellm.ai/docs/providers/ollama). <br>
The following is a configuration example of `conf.yaml` for using Ollama models(you might need to run the 'ollama serve' first):
```yaml
BASIC_MODEL:
model: "model-name" # Model name, which supports the completions API(important), such as: qwen3:8b, mistral-small3.1:24b, qwen2.5:3b
base_url: "http://localhost:11434/v1" # Local service address of Ollama, which can be started/viewed via ollama serve
api_key: "whatever" # Mandatory, fake api_key with a random string you like :-)
```
### How to use OpenRouter models?
DeerFlow supports the integration of OpenRouter models. You can refer to [litellm OpenRouter](https://docs.litellm.ai/docs/providers/openrouter). To use OpenRouter models, you need to:
1. Obtain the OPENROUTER_API_KEY from OpenRouter (https://openrouter.ai/) and set it in the environment variable.
2. Add the `openrouter/` prefix before the model name.
3. Configure the correct OpenRouter base URL.
The following is a configuration example for using OpenRouter models:
1. Configure OPENROUTER_API_KEY in the environment variable (such as the `.env` file)
```ini
OPENROUTER_API_KEY=""
```
2. Set the model name in `conf.yaml`
```yaml
BASIC_MODEL:
model: "openrouter/google/palm-2-chat-bison"
```
Note: The available models and their exact names may change over time. Please verify the currently available models and their correct identifiers in [OpenRouter's official documentation](https://openrouter.ai/docs).
### How to use Azure OpenAI chat models?
DeerFlow supports the integration of Azure OpenAI chat models. You can refer to [AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html). Configuration example of `conf.yaml`:
```yaml
BASIC_MODEL:
model: "azure/gpt-4o-2024-08-06"
azure_endpoint: $AZURE_OPENAI_ENDPOINT
api_version: $OPENAI_API_VERSION
api_key: $AZURE_OPENAI_API_KEY
```
### How to configure context length for different models
Different models have different context length limitations. DeerFlow provides a method to control the context length between different models. You can configure the context length between different models in the `conf.yaml` file. For example:
```yaml
BASIC_MODEL:
base_url: https://ark.cn-beijing.volces.com/api/v3
model: "doubao-1-5-pro-32k-250115"
api_key: ""
token_limit: 128000
```
This means that the context length limit using this model is 128k.
The context management doesn't work if the token_limit is not set.
## About Search Engine
### How to control search domains for Tavily?
DeerFlow allows you to control which domains are included or excluded in Tavily search results through the configuration file. This helps improve search result quality and reduce hallucinations by focusing on trusted sources.
`Tips`: it only supports Tavily currently.
You can configure domain filtering and search results in your `conf.yaml` file as follows:
```yaml
SEARCH_ENGINE:
engine: tavily
# Only include results from these domains (whitelist)
include_domains:
- trusted-news.com
- gov.org
- reliable-source.edu
# Exclude results from these domains (blacklist)
exclude_domains:
- unreliable-site.com
- spam-domain.net
# Include images in search results, default: true
include_images: false
# Include image descriptions in search results, default: true
include_image_descriptions: false
# Include raw content in search results, default: true
include_raw_content: false
```
### How to post-process Tavily search results
DeerFlow can post-process Tavily search results:
* Remove duplicate content
* Filter low-quality content: Filter out results with low relevance scores
* Clear base64 encoded images
* Length truncation: Truncate each search result according to the user-configured length
The filtering of low-quality content and length truncation depend on user configuration, providing two configurable parameters:
* min_score_threshold: Minimum relevance score threshold, search results below this threshold will be filtered. If not set, no filtering will be performed;
* max_content_length_per_page: Maximum length limit for each search result content, parts exceeding this length will be truncated. If not set, no truncation will be performed;
These two parameters can be configured in `conf.yaml` as shown below:
```yaml
SEARCH_ENGINE:
engine: tavily
include_images: true
min_score_threshold: 0.4
max_content_length_per_page: 5000
```
That's meaning that the search results will be filtered based on the minimum relevance score threshold and truncated to the maximum length limit for each search result content.
## RAG (Retrieval-Augmented Generation) Configuration
DeerFlow supports multiple RAG providers for document retrieval. Configure the RAG provider by setting environment variables.
### Supported RAG Providers
- **RAGFlow**: Document retrieval using RAGFlow API
- **VikingDB Knowledge Base**: ByteDance's VikingDB knowledge base service
- **Milvus**: Open-source vector database for similarity search
### Milvus Configuration
To use Milvus as your RAG provider, set the following environment variables:
```bash
# RAG_PROVIDER: milvus (using free milvus instance on zilliz cloud: https://docs.zilliz.com/docs/quick-start )
RAG_PROVIDER=milvus
MILVUS_URI=<endpoint_of_self_hosted_milvus_or_zilliz_cloud>
MILVUS_USER=<username_of_self_hosted_milvus_or_zilliz_cloud>
MILVUS_PASSWORD=<password_of_self_hosted_milvus_or_zilliz_cloud>
MILVUS_COLLECTION=documents
MILVUS_EMBEDDING_PROVIDER=openai
MILVUS_EMBEDDING_BASE_URL=
MILVUS_EMBEDDING_MODEL=
MILVUS_EMBEDDING_API_KEY=
# RAG_PROVIDER: milvus (using milvus lite on Mac or Linux)
RAG_PROVIDER=milvus
MILVUS_URI=./milvus_demo.db
MILVUS_COLLECTION=documents
MILVUS_EMBEDDING_PROVIDER=openai
MILVUS_EMBEDDING_BASE_URL=
MILVUS_EMBEDDING_MODEL=
MILVUS_EMBEDDING_API_KEY=
```
---
## Multi-Turn Clarification (Optional)
An optional feature that helps clarify vague research questions through conversation. **Disabled by default.**
### Enable via Command Line
```bash
# Enable clarification for vague questions
uv run main.py "Research AI" --enable-clarification
# Set custom maximum clarification rounds
uv run main.py "Research AI" --enable-clarification --max-clarification-rounds 3
# Interactive mode with clarification
uv run main.py --interactive --enable-clarification --max-clarification-rounds 3
```
### Enable via API
```json
{
"messages": [{"role": "user", "content": "Research AI"}],
"enable_clarification": true,
"max_clarification_rounds": 3
}
```
### Enable via UI Settings
1. Open DeerFlow web interface
2. Navigate to **Settings** → **General** tab
3. Find **"Enable Clarification"** toggle
4. Turn it **ON** to enable multi-turn clarification. Clarification is **disabled** by default. You need to manually enable it through any of the above methods. When clarification is enabled, you'll see **"Max Clarification Rounds"** field appear below the toggle
6. Set the maximum number of clarification rounds (default: 3, minimum: 1)
7. Click **Save** to apply changes
**When enabled**, the Coordinator will ask up to the specified number of clarifying questions for vague topics before starting research, improving report relevance and depth. The `max_clarification_rounds` parameter controls how many rounds of clarification are allowed.
**Note**: The `max_clarification_rounds` parameter only takes effect when `enable_clarification` is set to `true`. If clarification is disabled, this parameter is ignored.
---- mcp_integrations.md ----
# MCP Integrations（Beta）
Now This feature is diabled by default. You can enable it by setting the environment ENABLE_MCP_SERVER_CONFIGURATION to be true
> [!WARNING]
> Please enable this feature before securing your frond-end and back-end in a managed environment.
> Otherwise, you system could be compromised.
This feature is diabled by default. You can enable it by setting the environment ENABLE_MCP_SERVER_CONFIGURATION
Please enable this feature before securing your frond-end and back-end in an internal environment.q
## Example of MCP Server Configuration
```json
{
"mcpServers": {
"mcp-github-trending": {
"transport": "stdio",
"command": "uvx",
"args": [
"mcp-github-trending"
]
}
}
}
```
## APIs
### Get metadata of MCP Server
**POST /api/mcp/server/metadata**
For `stdio` type:
```json
{
"transport": "stdio",
"command": "npx",
"args": ["-y", "tavily-mcp@0.1.3"],
"env": {"TAVILY_API_KEY": "tvly-dev-xxx"}
}
```
For `sse` type:
```json
{
"transport": "sse",
"url": "http://localhost:3000/sse",
"headers": {
"API_KEY": "value"
}
}
```
For `streamable_http` type:
```json
{
"transport": "streamable_http",
"url": "http://localhost:3000/mcp",
"headers": {
"API_KEY": "value"
}
}
```
### Chat Stream
**POST /api/chat/stream**
```json
{
...
"mcp_settings": {
"servers": {
"mcp-github-trending": {
"transport": "stdio",
"command": "uvx",
"args": ["mcp-github-trending"],
"env": {
"MCP_SERVER_ID": "mcp-github-trending"
},
"enabled_tools": ["get_github_trending_repositories"],
"add_to_agents": ["researcher"]
}
}
},
}
```