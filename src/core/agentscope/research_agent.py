#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AgentScopeæ·±åº¦ç ”ç©¶ä»£ç†
åŸºäºReActAgentçš„å¤šæ¨¡æ€ç ”ç©¶æ™ºèƒ½ä½“
"""

import asyncio
import json
from datetime import datetime
from typing import Any, Dict, List, Optional, Union
from agentscope.agent import ReActAgent
from agentscope.message import Msg
from agentscope.tool import Toolkit
from agentscope.memory import InMemoryMemory
from agentscope.formatter import DashScopeChatFormatter

# å¯¼å…¥è‡ªå®šä¹‰ç»„ä»¶
from src.core.agentscope.llm_adapter import DualLLMManager
from src.core.agentscope.memory.research_memory import ResearchMemoryManager
from src.core.agentscope.tools import (
    register_web_search_tools,
    register_wikipedia_tools,
    register_arxiv_tools,
    register_image_analysis_tools,
    register_synthesis_tools
)

# å¯¼å…¥æ•°æ®è®¿é—®å¯¹è±¡
from src.dao.research_dao import ResearchDAO


class DeepResearchAgent(ReActAgent):
    """
    æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“
    é›†æˆå¤šç§ç ”ç©¶å·¥å…·ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥å’Œæ™ºèƒ½æ¨ç†
    """

    def __init__(
        self,
        session_id: str,
        llm_instance: 'BaseLLM',
        multimodal_llm_instance: Optional['BaseLLM'] = None,
        web_search_api_key: str = "",
        max_iterations: int = 15,  # å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥æ”¯æŒæ›´å®Œæ•´çš„ç ”ç©¶
        parallel_tool_calls: bool = True
    ):
        """
        åˆå§‹åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“

        Args:
            session_id: ç ”ç©¶ä¼šè¯ID
            llm_instance: ä¸»LLMå®ä¾‹ï¼ˆé€šè¿‡LLMå·¥å‚åˆ›å»ºï¼Œç”¨äºæ–‡æœ¬ç”Ÿæˆï¼‰
            multimodal_llm_instance: å¤šæ¨¡æ€LLMå®ä¾‹ï¼ˆå¯é€‰ï¼Œç”¨äºå›¾åƒåˆ†æï¼‰
            web_search_api_key: ç½‘ç»œæœç´¢APIå¯†é’¥
            max_iterations: æœ€å¤§æ¨ç†è¿­ä»£æ¬¡æ•°
            parallel_tool_calls: æ˜¯å¦å¯ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨
        """
        # å¯¼å…¥LLMæŠ½è±¡å±‚
        from src.core.llm.base_llm import BaseLLM
        from src.core.llm.factory import LLMFactory
        
        # å‡†å¤‡æ‰€æœ‰éœ€è¦çš„ç»„ä»¶ï¼ˆä½¿ç”¨å±€éƒ¨å˜é‡ï¼Œä¸è®¾ç½®å®ä¾‹å±æ€§ï¼‰
        # å¦‚æœæ²¡æœ‰æä¾›å¤šæ¨¡æ€LLMï¼Œä½¿ç”¨ä¸»LLM
        if multimodal_llm_instance is None:
            multimodal_llm_instance = llm_instance
        
        # è·å–æ¨¡å‹åç§°ï¼ˆä½¿ç”¨é™æ€æ–¹æ³•ï¼‰
        primary_model_name = DeepResearchAgent._get_model_name_static(llm_instance)
        multimodal_model_name = DeepResearchAgent._get_model_name_static(multimodal_llm_instance)
        
        # åˆ›å»ºåŒLLMç®¡ç†å™¨
        llm_manager = DualLLMManager(
            primary_llm=llm_instance,
            multimodal_llm=multimodal_llm_instance,
            primary_model_name=primary_model_name,
            multimodal_model_name=multimodal_model_name
        )

        # åˆå§‹åŒ–æ•°æ®è®¿é—®å¯¹è±¡
        research_dao = ResearchDAO()

        # åˆ›å»ºè®°å¿†ç®¡ç†å™¨
        memory_manager = ResearchMemoryManager(research_dao)

        # åˆ›å»ºå·¥å…·åŒ…
        toolkit = Toolkit()

        # åˆ›å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆä½¿ç”¨é™æ€æ–¹æ³•ï¼‰
        system_prompt = DeepResearchAgent._create_system_prompt_static()

        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„å†…å­˜å¯¹è±¡ç”¨äºåˆå§‹åŒ–
        # å®é™…çš„ä¼šè¯è®°å¿†å°†åœ¨å¼‚æ­¥æ–¹æ³•ä¸­åˆå§‹åŒ–
        temp_memory = InMemoryMemory()

        # é¦–å…ˆè°ƒç”¨ super().__init__()ï¼ˆAgentScope è¦æ±‚ï¼‰
        super().__init__(
            name="DeepResearchAgent",
            sys_prompt=system_prompt,
            model=llm_manager,
            formatter=DashScopeChatFormatter(),
            toolkit=toolkit,
            memory=temp_memory,
            max_iters=max_iterations,
            parallel_tool_calls=parallel_tool_calls
        )

        # ç°åœ¨å¯ä»¥å®‰å…¨åœ°è®¾ç½®å®ä¾‹å±æ€§
        self.session_id = session_id
        self.web_search_api_key = web_search_api_key
        self.llm_instance = llm_instance
        self.research_dao = research_dao
        self.llm_manager = llm_manager
        self.memory_manager = memory_manager
        self.session_memory = None  # å°†åœ¨ async_init ä¸­åˆå§‹åŒ–
        self.toolkit = toolkit
        self._memory_initialized = False

        # ç ”ç©¶çŠ¶æ€è·Ÿè¸ª
        self.research_phase = "planning"
        self.research_progress = 0.0
        self.current_tools_used = []
        self.findings_count = 0

        # å·¥å…·è°ƒç”¨å¤±è´¥è·Ÿè¸ª
        self.tool_failure_tracker = {}  # {tool_name: failure_count}
        self.max_tool_failures = 3      # å•ä¸ªå·¥å…·æœ€å¤šå¤±è´¥3æ¬¡
        self.consecutive_failures = 0   # è¿ç»­å¤±è´¥è®¡æ•°
        self.max_consecutive_failures = 5  # æœ€å¤šè¿ç»­å¤±è´¥5æ¬¡
        
        # åŸåœ°è¸æ­¥æ£€æµ‹
        self.recent_actions = []  # è®°å½•æœ€è¿‘çš„æ“ä½œ
        self.max_stagnation_check = 3  # æ£€æŸ¥æœ€è¿‘3æ¬¡æ“ä½œï¼ˆé™ä½é˜ˆå€¼ä»¥æ›´å¿«æ£€æµ‹å¾ªç¯ï¼‰
        self.tool_call_history = {}  # {tool_name: {args_hash: call_count}} è·Ÿè¸ªç›¸åŒå‚æ•°çš„é‡å¤è°ƒç”¨

        # æ³¨å†Œæ‰€æœ‰ç ”ç©¶å·¥å…·
        self._register_research_tools()

    async def async_init(self):
        """
        å¼‚æ­¥åˆå§‹åŒ–æ–¹æ³•ï¼Œç”¨äºåˆå§‹åŒ–éœ€è¦å¼‚æ­¥æ“ä½œçš„ç»„ä»¶
        å¿…é¡»åœ¨ä½¿ç”¨ä»£ç†ä¹‹å‰è°ƒç”¨
        """
        if not self._memory_initialized:
            # åˆ›å»ºä¼šè¯è®°å¿†
            self.session_memory = await self.memory_manager.create_session(self.session_id)
            # æ›´æ–°ä»£ç†çš„è®°å¿†
            self.memory = self.session_memory.short_memory
            self._memory_initialized = True
    
    @staticmethod
    def _get_model_name_static(llm_instance: 'BaseLLM') -> str:
        """
        è·å–LLMå®ä¾‹çš„æ¨¡å‹åç§°ï¼ˆé™æ€æ–¹æ³•ï¼‰

        Args:
            llm_instance: LLMå®ä¾‹

        Returns:
            æ¨¡å‹åç§°
        """
        # å°è¯•ä»é…ç½®ä¸­è·å–æ¨¡å‹åç§°
        if hasattr(llm_instance, 'model'):
            return llm_instance.model
        elif hasattr(llm_instance, 'config') and 'model' in llm_instance.config:
            return llm_instance.config['model']
        else:
            # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤æ¨¡å‹
            from src.config.llm_config import get_config
            provider = llm_instance.get_provider_name()
            try:
                config = get_config()
                provider_config = config.get_provider_config(provider)
                return provider_config.default_model
            except:
                # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
                default_models = {
                    'deepseek': 'deepseek-chat',
                    'ollama': 'gemma3:4b',
                    'zhipu': 'glm-4.6'
                }
                return default_models.get(provider, 'unknown')

    @staticmethod
    def _create_system_prompt_static() -> str:
        """
        åˆ›å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆé™æ€æ–¹æ³•ï¼‰

        Returns:
            ç³»ç»Ÿæç¤ºè¯å­—ç¬¦ä¸²
        """
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ·±åº¦ç ”ç©¶åŠ©æ‰‹ï¼Œå…·å¤‡å¼ºå¤§çš„ä¿¡æ¯æœé›†ã€åˆ†æå’Œç»¼åˆèƒ½åŠ›ã€‚

## æ ¸å¿ƒèƒ½åŠ›
1. **å¤šæºä¿¡æ¯æœé›†**: å¯ä»¥åŒæ—¶ä½¿ç”¨ç½‘ç»œæœç´¢ã€ç»´åŸºç™¾ç§‘ã€å­¦æœ¯è®ºæ–‡æ•°æ®åº“ç­‰å¤šç§ä¿¡æ¯æº
2. **æ™ºèƒ½å†…å®¹åˆ†æ**: èƒ½å¤Ÿåˆ†ææ–‡æœ¬ã€å›¾åƒç­‰å¤šç§ç±»å‹çš„å†…å®¹
3. **ç»¼åˆæ¨ç†**: èƒ½å¤Ÿæ•´åˆä¸åŒæ¥æºçš„ä¿¡æ¯ï¼Œå½¢æˆå…¨é¢çš„ç ”ç©¶æŠ¥å‘Š
4. **å¤šæ¨¡æ€æ”¯æŒ**: æ”¯æŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œèƒ½å¤Ÿåˆ†æå›¾è¡¨ã€æµç¨‹å›¾ç­‰è§†è§‰å†…å®¹

## ç ”ç©¶æµç¨‹
1. **ç†è§£éœ€æ±‚**: ä»”ç»†åˆ†æç”¨æˆ·çš„ç ”ç©¶éœ€æ±‚å’Œé—®é¢˜
2. **åˆ¶å®šç­–ç•¥**: æ ¹æ®éœ€æ±‚åˆ¶å®šåˆé€‚çš„ä¿¡æ¯æœé›†ç­–ç•¥
3. **å¹¶è¡Œæœé›†**: åŒæ—¶ä½¿ç”¨å¤šä¸ªå·¥å…·æœé›†ç›¸å…³ä¿¡æ¯
4. **ä¿¡æ¯éªŒè¯**: éªŒè¯ä¿¡æ¯çš„å¯é æ€§å’Œç›¸å…³æ€§
5. **ç»¼åˆåˆ†æ**: æ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œè¿›è¡Œæ·±å…¥åˆ†æ
6. **ç”ŸæˆæŠ¥å‘Š**: æä¾›ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Š

## ä½¿ç”¨å·¥å…·æŒ‡å—
- **web_search**: æœç´¢æœ€æ–°çš„ç½‘ç»œä¿¡æ¯å’Œæ–°é—»ï¼ˆç¬¬ä¸€æ­¥ï¼‰
- **search_wikipedia**: æŸ¥æ‰¾ç»´åŸºç™¾ç§‘ä¸­çš„åŸºç¡€çŸ¥è¯†ï¼ˆç¬¬äºŒæ­¥ï¼‰
- **search_arxiv_papers**: æœç´¢ç›¸å…³çš„å­¦æœ¯è®ºæ–‡ï¼ˆç¬¬ä¸‰æ­¥ï¼Œå¿…é¡»æ‰§è¡Œï¼‰
- **analyze_image**: åˆ†æå›¾åƒå†…å®¹ï¼ˆå¦‚æœ‰å›¾åƒï¼‰
- **synthesize_research_findings**: ç»¼åˆç ”ç©¶å‘ç°ï¼ˆæœ€åä¸€æ­¥ï¼‰

## é‡è¦åŸåˆ™
- å§‹ç»ˆç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå¯é æ€§
- ç»¼åˆå¤šä¸ªæ¥æºçš„ä¿¡æ¯ï¼Œé¿å…å•ä¸€æ¥æºåè§
- **é¿å…é‡å¤è°ƒç”¨ç›¸åŒçš„å·¥å…·å’Œå‚æ•°** - å¦‚æœå·²ç»è·å–äº†æŸä¸ªé¡µé¢çš„å†…å®¹ï¼Œä¸è¦å†æ¬¡è¯·æ±‚
- **æŒ‰é¡ºåºå®Œæˆæ‰€æœ‰ç ”ç©¶æ­¥éª¤** - ä¸è¦è·³è¿‡ArXivå­¦æœ¯è®ºæ–‡æœç´¢
- ä¸ºç”¨æˆ·æä¾›ç»“æ„åŒ–ã€æ˜“äºç†è§£çš„å›ç­”
- åœ¨ä¸ç¡®å®šæ—¶æ˜ç¡®æŒ‡å‡ºï¼Œé¿å…çŒœæµ‹
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„ç ”ç©¶æ€åº¦
- **å½“è·å–è¶³å¤Ÿä¿¡æ¯åï¼Œç«‹å³è¿›å…¥ä¸‹ä¸€æ­¥** - ä¸è¦åœ¨åŒä¸€ä¸ªå·¥å…·ä¸Šåœç•™å¤ªä¹…

è¯·å¼€å§‹ä½ çš„ç ”ç©¶å·¥ä½œï¼Œä¸ºç”¨æˆ·æä¾›é«˜è´¨é‡çš„ç ”ç©¶æœåŠ¡ã€‚"""

    def _register_research_tools(self) -> None:
        """
        æ³¨å†Œæ‰€æœ‰ç ”ç©¶å·¥å…·åˆ°å·¥å…·åŒ…
        """
        # æ³¨å†Œç½‘ç»œæœç´¢å·¥å…·
        if self.web_search_api_key:
            register_web_search_tools(self.toolkit, self.web_search_api_key)

        # æ³¨å†Œç»´åŸºç™¾ç§‘å·¥å…·
        register_wikipedia_tools(self.toolkit)

        # æ³¨å†ŒArXivå­¦æœ¯è®ºæ–‡å·¥å…·
        register_arxiv_tools(self.toolkit)

        # æ³¨å†Œå›¾åƒåˆ†æå·¥å…·ï¼ˆä½¿ç”¨å¤šæ¨¡æ€LLMçš„é…ç½®ï¼‰
        # è·å–Ollamaä¸»æœºåœ°å€ï¼ˆå¦‚æœå¤šæ¨¡æ€LLMæ˜¯Ollamaï¼‰
        ollama_host = "http://localhost:11434"
        if hasattr(self.llm_manager.multimodal_adapter.base_llm, 'base_url'):
            ollama_host = self.llm_manager.multimodal_adapter.base_llm.base_url
        register_image_analysis_tools(self.toolkit, ollama_host)

        # æ³¨å†Œç ”ç©¶åˆæˆå·¥å…·
        register_synthesis_tools(self.toolkit)
        
        # åŒ…è£…æ‰€æœ‰å·¥å…·ä»¥è‡ªåŠ¨è®°å½•ç ”ç©¶å‘ç°
        self._wrap_tools_with_finding_recorder()

    def _wrap_tools_with_finding_recorder(self) -> None:
        """
        åŒ…è£…æ‰€æœ‰å·¥å…·å‡½æ•°ï¼Œä½¿å…¶è‡ªåŠ¨è®°å½•ç ”ç©¶å‘ç°
        
        æ³¨æ„ï¼šä¸ç›´æ¥åŒ…è£…å·¥å…·ï¼Œè€Œæ˜¯é€šè¿‡é‡å†™ reply æ–¹æ³•æ¥æ‹¦æˆªå·¥å…·è°ƒç”¨ç»“æœ
        è¿™æ ·å¯ä»¥é¿å…ç ´å AgentScope çš„å·¥å…·å¯¹è±¡ç»“æ„
        """
        # ä¿å­˜åŸå§‹çš„ reply æ–¹æ³•
        if not hasattr(self, '_original_reply'):
            self._original_reply = self.reply
            # ä¸éœ€è¦åŒ…è£…å·¥å…·ï¼Œè€Œæ˜¯åœ¨ reply æ–¹æ³•ä¸­å¤„ç†
            print("âœ“ å·¥å…·å‘ç°è®°å½•å™¨å·²å¯ç”¨ï¼ˆé€šè¿‡ reply æ‹¦æˆªï¼‰")
    
    async def reply(self, x=None):
        """
        é‡å†™ reply æ–¹æ³•ä»¥æ‹¦æˆªå·¥å…·è°ƒç”¨ç»“æœ
        """
        # è°ƒç”¨åŸå§‹çš„ reply æ–¹æ³•
        result = await super().reply(x)
        
        # å¦‚æœä¼šè¯è®°å¿†å·²åˆå§‹åŒ–ï¼Œå°è¯•ä»ç»“æœä¸­æå–å‘ç°
        if self.session_memory and result:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨è®°å½•
                if hasattr(self, 'memory') and self.memory:
                    # âœ… ä¿®å¤ï¼šä¸ä¼ é€’ limit å‚æ•°ï¼Œç›´æ¥è·å–æ‰€æœ‰æ¶ˆæ¯
                    recent_messages = await self.memory.get_memory()
                    
                    # åªæŸ¥çœ‹æœ€è¿‘5æ¡æ¶ˆæ¯
                    recent_messages = recent_messages[-5:] if len(recent_messages) > 5 else recent_messages
                    
                    # æŸ¥æ‰¾æœ€è¿‘çš„å·¥å…·è°ƒç”¨
                    for msg in reversed(recent_messages):
                        if hasattr(msg, 'metadata') and msg.metadata:
                            tool_name = msg.metadata.get('tool_name')
                            if tool_name:
                                # è®°å½•å·¥å…·ä½¿ç”¨
                                self.update_tool_usage(tool_name, success=True)
                                
                                # å°è¯•è®°å½•å‘ç°
                                await self._record_finding_from_message(msg, tool_name)
            except Exception as e:
                # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
                print(f"âš ï¸ è®°å½•å‘ç°æ—¶å‡ºé”™: {str(e)}")
        
        return result
    
    async def _extract_tools_and_findings_from_memory(self):
        """
        ä»å†…å­˜ä¸­æå–å·¥å…·ä½¿ç”¨è®°å½•å’Œç ”ç©¶å‘ç°
        """
        try:
            if not hasattr(self, 'memory') or not self.memory:
                return
            
            # è·å–æ‰€æœ‰æ¶ˆæ¯
            all_messages = await self.memory.get_memory()
            
            print(f"\nåˆ†æ {len(all_messages)} æ¡æ¶ˆæ¯ä»¥æå–å·¥å…·ä½¿ç”¨å’Œå‘ç°...")
            
            for msg in all_messages:
                # âœ… å®‰å…¨åœ°è·å–å†…å®¹ï¼Œå¤„ç†å¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
                content = msg.content if hasattr(msg, 'content') else str(msg)
                
                # å¦‚æœ content æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                if isinstance(content, list):
                    content = str(content)
                elif content is None:
                    content = ""
                else:
                    content = str(content)
                
                role = msg.role if hasattr(msg, 'role') else 'unknown'
                name = msg.name if hasattr(msg, 'name') else 'unknown'
                
                # æ£€æµ‹å·¥å…·è°ƒç”¨ï¼ˆé€šå¸¸åœ¨ system è§’è‰²çš„æ¶ˆæ¯ä¸­ï¼‰
                if role == 'system' and content:
                    content_lower = content.lower()
                    
                    # æ£€æµ‹å·¥å…·åç§°
                    tool_name = None
                    if 'web_search' in content_lower or '"name": "web_search"' in content:
                        tool_name = 'web_search'
                    elif 'wikipedia' in content_lower or 'search_wikipedia' in content_lower:
                        tool_name = 'search_wikipedia'
                    elif 'arxiv' in content_lower or 'search_arxiv' in content_lower:
                        tool_name = 'search_arxiv_papers'
                    elif 'news_search' in content_lower:
                        tool_name = 'news_search'
                    
                    if tool_name:
                        # è®°å½•å·¥å…·ä½¿ç”¨
                        if tool_name not in self.current_tools_used:
                            self.current_tools_used.append(tool_name)
                            print(f"âœ“ æ£€æµ‹åˆ°å·¥å…·ä½¿ç”¨: {tool_name}")
                        
                        # è®°å½•å‘ç°
                        await self._record_finding_from_content(content, tool_name)
            
            print(f"âœ“ å·¥å…·ä½¿ç”¨è®°å½•å®Œæˆ: {len(self.current_tools_used)} ä¸ªå·¥å…·")
            print(f"âœ“ å‘ç°è®°å½•å®Œæˆ: {self.findings_count} ä¸ªå‘ç°\n")
            
        except Exception as e:
            print(f"âš ï¸ æå–å·¥å…·å’Œå‘ç°æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
    
    async def _record_finding_from_content(self, content: str, tool_name: str):
        """
        ä»å†…å®¹ä¸­æå–å¹¶è®°å½•ç ”ç©¶å‘ç°
        
        Args:
            content: æ¶ˆæ¯å†…å®¹
            tool_name: å·¥å…·åç§°
        """
        try:
            # âœ… ç¡®ä¿ content æ˜¯å­—ç¬¦ä¸²
            if isinstance(content, list):
                content = str(content)
            elif not isinstance(content, str):
                content = str(content)
            
            # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œä¸è®°å½•
            if not content or len(content.strip()) < 100:
                return
            
            # âœ… å°è¯•ä» JSON æ ¼å¼çš„å·¥å…·è¾“å‡ºä¸­æå–å®é™…å†…å®¹
            try:
                import json
                # æ£€æŸ¥æ˜¯å¦æ˜¯ JSON æ ¼å¼
                if content.strip().startswith('{') or content.strip().startswith('['):
                    data = json.loads(content)
                    # æå–å®é™…çš„æ–‡æœ¬å†…å®¹
                    if isinstance(data, dict):
                        if 'output' in data:
                            output = data['output']
                            if isinstance(output, list) and len(output) > 0:
                                if isinstance(output[0], dict) and 'text' in output[0]:
                                    content = output[0]['text']
                        elif 'text' in data:
                            content = data['text']
            except:
                pass  # å¦‚æœä¸æ˜¯ JSONï¼Œç»§ç»­ä½¿ç”¨åŸå§‹å†…å®¹
            
            # ç¡®å®šæ¥æºç±»å‹
            source_type = "unknown"
            source_url = "unknown"
            
            if "web_search" in tool_name or "news_search" in tool_name:
                source_type = "web"
                source_url = f"web_search:{tool_name}"
            elif "wikipedia" in tool_name:
                source_type = "wikipedia"
                source_url = "wikipedia_search"
            elif "arxiv" in tool_name:
                source_type = "arxiv"
                source_url = "arxiv_search"
                # å°è¯•æå–å¼•ç”¨
                await self._extract_and_record_arxiv_citations(content)
            elif "image" in tool_name:
                source_type = "image"
                source_url = "image_analysis"
            elif "synthesis" in tool_name or "synthesize" in tool_name:
                source_type = "synthesis"
                source_url = "research_synthesis"
            
            # è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
            relevance_score = 0.8
            if len(content) > 1000:
                relevance_score = 0.9
            elif len(content) < 200:
                relevance_score = 0.6
            
            # è®°å½•ç ”ç©¶å‘ç°
            await self.session_memory.add_research_finding(
                source_type=source_type,
                source_url=source_url,
                content=content[:2000],
                relevance_score=relevance_score
            )
            
            # æ›´æ–°å‘ç°è®¡æ•°
            self.findings_count += 1
            
            print(f"  âœ“ å·²è®°å½•ç ”ç©¶å‘ç° [{source_type}]: {content[:80]}...")
            
        except Exception as e:
            print(f"  âš ï¸ è®°å½•å‘ç°æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
    

    
    async def _extract_and_record_arxiv_citations(self, content: str) -> None:
        """
        ä»ArXivæœç´¢ç»“æœä¸­æå–å¹¶è®°å½•å¼•ç”¨
        
        Args:
            content: ArXivæœç´¢ç»“æœå†…å®¹
        """
        try:
            # ç®€å•çš„è§£æé€»è¾‘ï¼šæŸ¥æ‰¾è®ºæ–‡æ ‡é¢˜å’Œä½œè€…
            lines = content.split('\n')
            current_paper = {}
            
            for line in lines:
                line = line.strip()
                
                # æ£€æµ‹è®ºæ–‡æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯æ•°å­—å¼€å¤´ï¼‰
                if line and line[0].isdigit() and '. ' in line:
                    # ä¿å­˜ä¸Šä¸€ç¯‡è®ºæ–‡
                    if current_paper.get('title'):
                        await self._save_arxiv_citation(current_paper)
                    
                    # å¼€å§‹æ–°è®ºæ–‡
                    title = line.split('. ', 1)[1] if '. ' in line else line
                    current_paper = {'title': title, 'authors': [], 'url': ''}
                
                # æ£€æµ‹ä½œè€…è¡Œ
                elif line.startswith('ä½œè€…:') or line.startswith('Authors:'):
                    authors_str = line.split(':', 1)[1].strip()
                    # åˆ†å‰²ä½œè€…åï¼ˆå¤„ç†ä¸­è‹±æ–‡ï¼‰
                    authors = [a.strip() for a in authors_str.replace('ç­‰', '').split(',')]
                    current_paper['authors'] = authors[:5]  # æœ€å¤šè®°å½•5ä¸ªä½œè€…
                
                # æ£€æµ‹ArXiv IDæˆ–é“¾æ¥
                elif 'arxiv.org' in line.lower() or line.startswith('ID:'):
                    if 'arxiv.org' in line.lower():
                        # æå–URL
                        import re
                        url_match = re.search(r'https?://[^\s]+', line)
                        if url_match:
                            current_paper['url'] = url_match.group(0)
                    elif line.startswith('ID:'):
                        arxiv_id = line.split(':', 1)[1].strip()
                        current_paper['url'] = f"https://arxiv.org/abs/{arxiv_id}"
                
                # æ£€æµ‹å‘å¸ƒå¹´ä»½
                elif 'å‘å¸ƒæ—¶é—´:' in line or 'Published:' in line:
                    year_str = line.split(':', 1)[1].strip()
                    # æå–å¹´ä»½
                    import re
                    year_match = re.search(r'(\d{4})', year_str)
                    if year_match:
                        current_paper['year'] = int(year_match.group(1))
            
            # ä¿å­˜æœ€åä¸€ç¯‡è®ºæ–‡
            if current_paper.get('title'):
                await self._save_arxiv_citation(current_paper)
                
        except Exception as e:
            print(f"âš ï¸ æå–ArXivå¼•ç”¨æ—¶å‡ºé”™: {str(e)}")
    
    async def _save_arxiv_citation(self, paper: dict) -> None:
        """
        ä¿å­˜ArXivè®ºæ–‡å¼•ç”¨
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
        """
        try:
            if not paper.get('title'):
                return
            
            await self.session_memory.add_citation(
                title=paper.get('title', 'Unknown'),
                authors=paper.get('authors', []),
                source_url=paper.get('url', ''),
                publication_year=paper.get('year'),
                doi=None
            )
            
            print(f"âœ“ å·²è®°å½•å¼•ç”¨: {paper.get('title', '')[:50]}...")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å¼•ç”¨æ—¶å‡ºé”™: {str(e)}")

    async def conduct_research(
        self,
        query: str,
        research_type: str = "comprehensive",
        sources: Optional[List[str]] = None,
        include_images: bool = False
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ·±åº¦ç ”ç©¶

        Args:
            query: ç ”ç©¶æŸ¥è¯¢
            research_type: ç ”ç©¶ç±»å‹ (comprehensive, academic, news, analysis)
            sources: æŒ‡å®šçš„ä¿¡æ¯æºç±»å‹
            include_images: æ˜¯å¦åŒ…å«å›¾åƒåˆ†æ

        Returns:
            ç ”ç©¶ç»“æœå­—å…¸
        """
        try:
            print(f"\n{'='*60}")
            print(f"å¼€å§‹ç ”ç©¶: {query}")
            print(f"ç ”ç©¶ç±»å‹: {research_type}")
            print(f"ä¿¡æ¯æº: {sources}")
            print(f"{'='*60}\n")
            
            # æ›´æ–°ç ”ç©¶çŠ¶æ€
            self.research_phase = "research"
            self.research_progress = 0.1

            # åˆ›å»ºç ”ç©¶æ¶ˆæ¯
            research_query = self._format_research_query(
                query, research_type, sources, include_images
            )
            
            print(f"ç ”ç©¶æç¤ºè¯:\n{research_query}\n")

            research_msg = Msg(
                name="user",
                role="user",
                content=research_query
            )

            # æ‰§è¡Œç ”ç©¶
            print("å¼€å§‹æ‰§è¡Œ ReActAgent æ¨ç†å¾ªç¯...")
            
            # é‡ç½®å¤±è´¥è®¡æ•°å™¨
            self.tool_failure_tracker = {}
            self.consecutive_failures = 0
            self.recent_actions = []
            
            result = await self(research_msg)
            print(f"ReActAgent æ‰§è¡Œå®Œæˆ\n")
            
            # æ£€æŸ¥æ˜¯å¦å› ä¸ºå¾ªç¯è€Œæå‰ç»ˆæ­¢
            if self.consecutive_failures >= self.max_consecutive_failures:
                print(f"âš ï¸ ç ”ç©¶å› è¿ç»­å¤±è´¥ {self.consecutive_failures} æ¬¡è€Œç»ˆæ­¢")

            # âœ… æ‰‹åŠ¨è®°å½•å·¥å…·ä½¿ç”¨å’Œå‘ç°ï¼ˆä»å†…å­˜ä¸­æå–ï¼‰
            await self._extract_tools_and_findings_from_memory()

            # æ›´æ–°è¿›åº¦
            self.research_progress = 0.8

            # ç”Ÿæˆç ”ç©¶æŠ¥å‘Š
            print("ç”Ÿæˆç ”ç©¶æŠ¥å‘Š...")
            report = await self._generate_research_report(query)
            print(f"æŠ¥å‘Šç”Ÿæˆå®Œæˆ\n")

            # å®Œæˆç ”ç©¶
            self.research_phase = "completed"
            self.research_progress = 1.0
            
            print(f"{'='*60}")
            print(f"ç ”ç©¶å®Œæˆ!")
            print(f"ä½¿ç”¨çš„å·¥å…·: {self.current_tools_used}")
            print(f"å‘ç°æ•°é‡: {self.findings_count}")
            print(f"{'='*60}\n")

            # å°† Msg å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            result_content = result.content if hasattr(result, 'content') else str(result)
            
            # âœ… å­˜å‚¨ç ”ç©¶ç»“æœåˆ°å®ä¾‹å˜é‡ï¼Œä»¥ä¾¿ export_session_data å¯ä»¥è®¿é—®
            self.research_result = {
                "session_id": self.session_id,
                "query": query,
                "research_type": research_type,
                "result": result_content,
                "report": report,
                "tools_used": self.current_tools_used,
                "findings_count": self.findings_count,
                "completed_at": datetime.now().isoformat()
            }
            
            return self.research_result

        except Exception as e:
            print(f"\n{'='*60}")
            print(f"ç ”ç©¶è¿‡ç¨‹å‡ºé”™: {e}")
            print(f"{'='*60}\n")
            import traceback
            traceback.print_exc()
            
            return {
                "session_id": self.session_id,
                "query": query,
                "error": str(e),
                "completed_at": datetime.now().isoformat()
            }

    def _format_research_query(
        self,
        query: str,
        research_type: str,
        sources: Optional[List[str]],
        include_images: bool
    ) -> str:
        """
        æ ¼å¼åŒ–ç ”ç©¶æŸ¥è¯¢

        Args:
            query: åŸå§‹æŸ¥è¯¢
            research_type: ç ”ç©¶ç±»å‹
            sources: æŒ‡å®šçš„ä¿¡æ¯æº
            include_images: æ˜¯å¦åŒ…å«å›¾åƒ

        Returns:
            æ ¼å¼åŒ–çš„æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        formatted_query = f"è¯·è¿›è¡Œä»¥ä¸‹æ·±åº¦ç ”ç©¶:\n\nä¸»é¢˜: {query}\n"

        formatted_query += f"ç ”ç©¶ç±»å‹: {research_type}\n"

        if sources:
            formatted_query += f"ä¼˜å…ˆä½¿ç”¨çš„ä¿¡æ¯æº: {', '.join(sources)}\n"

        if include_images:
            formatted_query += "åŒ…å«å›¾åƒåˆ†æåŠŸèƒ½\n"

        formatted_query += """
è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œç ”ç©¶:
1. é¦–å…ˆä½¿ç”¨ç½‘ç»œæœç´¢è·å–æœ€æ–°ä¿¡æ¯
2. æŸ¥æ‰¾ç»´åŸºç™¾ç§‘ä¸­çš„èƒŒæ™¯çŸ¥è¯†
3. æœç´¢ç›¸å…³çš„å­¦æœ¯è®ºæ–‡
4. å¦‚æœ‰å›¾åƒï¼Œè¿›è¡Œå›¾åƒåˆ†æ
5. ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆç ”ç©¶æŠ¥å‘Š

è¯·ç¡®ä¿ç ”ç©¶çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚"""

        return formatted_query

    async def _generate_research_report(self, query: str) -> str:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆç ”ç©¶æŠ¥å‘Šæ€»ç»“

        Args:
            query: ç ”ç©¶æŸ¥è¯¢

        Returns:
            AI ç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        try:
            # è·å–æ‰€æœ‰ç ”ç©¶å‘ç°
            findings = await self.session_memory.get_research_findings()

            # è·å–æ‰€æœ‰å¼•ç”¨
            citations = await self.session_memory.get_citations()

            print(f"âœ“ å¼€å§‹ç”Ÿæˆç ”ç©¶æŠ¥å‘Šï¼Œå‘ç°æ•°é‡: {len(findings)}, å¼•ç”¨æ•°é‡: {len(citations)}")

            # å‡†å¤‡å‘ç°å†…å®¹ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼Œå–å‰15ä¸ªï¼‰
            sorted_findings = sorted(
                findings,
                key=lambda x: x.get("relevance_score", 0),
                reverse=True
            )[:15]

            findings_text = ""
            for i, finding in enumerate(sorted_findings, 1):
                source_type = finding.get("source_type", "æœªçŸ¥")
                content = finding.get("content", "")
                findings_text += f"{i}. [{source_type}] {content}\n\n"

            # å‡†å¤‡å¼•ç”¨å†…å®¹
            citations_text = ""
            for i, citation in enumerate(citations[:10], 1):
                title = citation.get("title", "æ— æ ‡é¢˜")
                authors = citation.get("authors", [])
                year = citation.get("publication_year", "")
                citations_text += f"{i}. {title}"
                if authors:
                    citations_text += f" - {', '.join(authors[:3])}"
                if year:
                    citations_text += f" ({year})"
                citations_text += "\n"

            # æ„å»ºæç¤ºè¯ï¼Œè®© LLM ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            prompt = f"""è¯·åŸºäºä»¥ä¸‹ç ”ç©¶å‘ç°å’Œå¼•ç”¨ï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Šã€‚

ç ”ç©¶ä¸»é¢˜: {query}

ç ”ç©¶å‘ç°:
{findings_text}

å‚è€ƒæ–‡çŒ®:
{citations_text}

è¯·ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š
1. æ‰§è¡Œæ‘˜è¦ï¼ˆ200-300å­—ï¼‰
2. èƒŒæ™¯ä»‹ç»
3. ä¸»è¦å‘ç°ï¼ˆåˆ†ç‚¹è¯¦ç»†è¯´æ˜ï¼‰
4. æ·±å…¥åˆ†æ
5. ç»“è®ºä¸å»ºè®®
6. å‚è€ƒæ–‡çŒ®åˆ—è¡¨

è¦æ±‚ï¼š
- ä½¿ç”¨ Markdown æ ¼å¼
- å†…å®¹è¦ä¸“ä¸šã€å‡†ç¡®ã€æœ‰æ·±åº¦
- ç»¼åˆæ‰€æœ‰å‘ç°ï¼Œå½¢æˆè¿è´¯çš„å™è¿°
- çªå‡ºé‡ç‚¹å’Œå…³é”®ä¿¡æ¯
- æ€»å­—æ•°æ§åˆ¶åœ¨ 2000-3000 å­—"""

            # è°ƒç”¨ LLM ç”ŸæˆæŠ¥å‘Š
            print("âœ“ è°ƒç”¨ LLM ç”ŸæˆæŠ¥å‘Š...")
            
            # ä½¿ç”¨ LLM ç”ŸæˆæŠ¥å‘Š
            from agentscope.message import Msg
            
            report_msg = Msg(
                name="user",
                role="user",
                content=prompt
            )
            
            # è°ƒç”¨ LLM
            response = await self.llm_manager(report_msg)
            
            if response and hasattr(response, 'content'):
                report_content = response.content
                
                # âœ… å¤„ç† content å¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µï¼ˆAgentScope çš„ Msg.content å¯èƒ½æ˜¯ list[ContentBlock]ï¼‰
                if isinstance(report_content, list):
                    # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                    text_parts = []
                    for item in report_content:
                        if isinstance(item, dict) and 'text' in item:
                            text_parts.append(str(item['text']))
                        elif hasattr(item, 'text'):
                            text_parts.append(str(item.text))
                        else:
                            text_parts.append(str(item))
                    report_content = '\n'.join(text_parts)
                elif not isinstance(report_content, str):
                    report_content = str(report_content)
                
                print(f"âœ“ LLM æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(report_content)} å­—ç¬¦")
                
                # æ·»åŠ å…ƒæ•°æ®
                final_report = f"# æ·±åº¦ç ”ç©¶æŠ¥å‘Š\n\n"
                final_report += f"**ç ”ç©¶ä¸»é¢˜**: {query}\n\n"
                final_report += f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                final_report += f"**æ•°æ®æ¥æº**: {len(findings)} ä¸ªå‘ç°ï¼Œ{len(citations)} ä¸ªå¼•ç”¨\n\n"
                final_report += "---\n\n"
                final_report += report_content
                
                return final_report
            else:
                print("âš ï¸ LLM æœªè¿”å›æœ‰æ•ˆå†…å®¹ï¼Œä½¿ç”¨å¤‡ç”¨æŠ¥å‘Šæ ¼å¼")
                return self._generate_fallback_report(query, findings, citations)

        except Exception as e:
            error_msg = str(e)
            print(f"ç”Ÿæˆç ”ç©¶æŠ¥å‘Šæ—¶å‡ºé”™: {error_msg}")
            import traceback
            traceback.print_exc()
            
            # âœ… æ£€æŸ¥æ˜¯å¦æ˜¯å†…å®¹æ•æ„Ÿé”™è¯¯
            if "contentFilter" in error_msg or "1301" in error_msg or "æ•æ„Ÿå†…å®¹" in error_msg:
                return f"""# ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå—é™

## æç¤º

ç³»ç»Ÿæ£€æµ‹åˆ°ç ”ç©¶å†…å®¹å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´æŠ¥å‘Šã€‚

æ‚¨å¯ä»¥ï¼š
1. å°è¯•è°ƒæ•´ç ”ç©¶ä¸»é¢˜çš„è¡¨è¿°æ–¹å¼
2. ä½¿ç”¨æ›´å…·ä½“æˆ–æ›´å­¦æœ¯åŒ–çš„å…³é”®è¯
3. åˆ†æ®µè¿›è¡Œç ”ç©¶ï¼Œé¿å…è§¦å‘å†…å®¹è¿‡æ»¤

## å·²æ”¶é›†çš„ä¿¡æ¯

æœ¬æ¬¡ç ”ç©¶å·²æˆåŠŸæ”¶é›†äº† {self.findings_count} æ¡ç›¸å…³ä¿¡æ¯ï¼Œä½†ç”±äºå†…å®¹å®‰å…¨é™åˆ¶ï¼Œæ— æ³•ç”Ÿæˆç»¼åˆæŠ¥å‘Šã€‚

æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸é…åˆã€‚"""
            
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            try:
                findings = await self.session_memory.get_research_findings()
                citations = await self.session_memory.get_citations()
                return self._generate_fallback_report(query, findings, citations)
            except:
                return f"# ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥\n\né”™è¯¯: {error_msg}"

    def _generate_fallback_report(self, query: str, findings: List[Dict], citations: List[Dict]) -> str:
        """
        ç”Ÿæˆå¤‡ç”¨æŠ¥å‘Šï¼ˆå½“ LLM è°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
        
        Args:
            query: ç ”ç©¶æŸ¥è¯¢
            findings: ç ”ç©¶å‘ç°åˆ—è¡¨
            citations: å¼•ç”¨åˆ—è¡¨
            
        Returns:
            å¤‡ç”¨æ ¼å¼çš„æŠ¥å‘Š
        """
        report = f"# æ·±åº¦ç ”ç©¶æŠ¥å‘Š\n\n"
        report += f"**ç ”ç©¶ä¸»é¢˜**: {query}\n\n"
        report += f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        report += f"**æ•°æ®æ¥æº**: {len(findings)} ä¸ªå‘ç°ï¼Œ{len(citations)} ä¸ªå¼•ç”¨\n\n"
        report += "---\n\n"

        # æ‰§è¡Œæ‘˜è¦
        report += "## æ‰§è¡Œæ‘˜è¦\n\n"
        sorted_findings = sorted(findings, key=lambda x: x.get("relevance_score", 0), reverse=True)[:3]
        for finding in sorted_findings:
            content = finding.get("content", "")[:200]
            report += f"- {content}...\n\n"

        # ä¸»è¦å‘ç°
        if findings:
            report += "## ä¸»è¦å‘ç°\n\n"
            
            # æŒ‰æ¥æºåˆ†ç»„
            findings_by_source = {}
            for finding in findings:
                source = finding.get("source_type", "å…¶ä»–")
                if source not in findings_by_source:
                    findings_by_source[source] = []
                findings_by_source[source].append(finding)
            
            source_names = {
                "web": "ğŸŒ ç½‘ç»œæœç´¢",
                "wikipedia": "ğŸ“š ç»´åŸºç™¾ç§‘",
                "arxiv": "ğŸ“– å­¦æœ¯è®ºæ–‡",
                "synthesis": "ğŸ” ç»¼åˆåˆ†æ"
            }
            
            for source, source_findings in findings_by_source.items():
                report += f"### {source_names.get(source, source)}\n\n"
                top_findings = sorted(source_findings, key=lambda x: x.get("relevance_score", 0), reverse=True)[:3]
                for finding in top_findings:
                    content = finding.get("content", "")
                    report += f"- {content}\n\n"

        # å‚è€ƒæ–‡çŒ®
        if citations:
            report += "## å‚è€ƒæ–‡çŒ®\n\n"
            for i, citation in enumerate(citations[:10], 1):
                title = citation.get("title", "æ— æ ‡é¢˜")
                authors = citation.get("authors", [])
                year = citation.get("publication_year", "")
                url = citation.get("source_url", "")
                
                report += f"{i}. {title}"
                if authors:
                    report += f" - {', '.join(authors[:3])}"
                if year:
                    report += f" ({year})"
                if url:
                    report += f"\n   é“¾æ¥: {url}"
                report += "\n\n"

        # ç»Ÿè®¡ä¿¡æ¯
        report += "## ç ”ç©¶ç»Ÿè®¡\n\n"
        report += f"- å‘ç°æ•°é‡: {len(findings)}\n"
        report += f"- å¼•ç”¨æ•°é‡: {len(citations)}\n"
        report += f"- ä½¿ç”¨å·¥å…·: {len(self.current_tools_used)}\n"

        return report

    async def interrupt_research(self) -> Dict[str, Any]:
        """
        ä¸­æ–­å½“å‰ç ”ç©¶

        Returns:
            ä¸­æ–­çŠ¶æ€ä¿¡æ¯
        """
        try:
            # è°ƒç”¨çˆ¶ç±»ä¸­æ–­æ–¹æ³•
            await self.interrupt()

            # ä¿å­˜å½“å‰çŠ¶æ€
            current_state = await self.session_memory.export_session_data()

            return {
                "session_id": self.session_id,
                "status": "interrupted",
                "phase": self.research_phase,
                "progress": self.research_progress,
                "tools_used": self.current_tools_used,
                "findings_count": self.findings_count,
                "state_data": current_state,
                "interrupted_at": datetime.now().isoformat()
            }

        except Exception as e:
            return {
                "session_id": self.session_id,
                "status": "error",
                "error": str(e),
                "interrupted_at": datetime.now().isoformat()
            }

    async def resume_research(self, state_data: Dict[str, Any]) -> bool:
        """
        æ¢å¤è¢«ä¸­æ–­çš„ç ”ç©¶

        Args:
            state_data: ä¿å­˜çš„çŠ¶æ€æ•°æ®

        Returns:
            æ˜¯å¦æˆåŠŸæ¢å¤
        """
        try:
            # æ¢å¤è®°å¿†çŠ¶æ€
            if "short_memory" in state_data:
                for msg_data in state_data["short_memory"]:
                    msg = Msg(
                        name=msg_data["name"],
                        role=msg_data["role"],
                        content=msg_data["content"],
                        timestamp=msg_data.get("timestamp", datetime.now().isoformat())
                    )
                    await self.session_memory.add_message(msg)

            # æ¢å¤å…¶ä»–çŠ¶æ€
            self.research_phase = "resumed"
            self.current_tools_used = state_data.get("tools_used", [])
            self.findings_count = len(state_data.get("research_findings", []))

            return True

        except Exception as e:
            print(f"æ¢å¤ç ”ç©¶æ—¶å‡ºé”™: {str(e)}")
            return False

    async def get_research_status(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰ç ”ç©¶çŠ¶æ€

        Returns:
            ç ”ç©¶çŠ¶æ€ä¿¡æ¯
        """
        try:
            memory_stats = await self.memory_manager.get_memory_statistics()

            return {
                "session_id": self.session_id,
                "phase": self.research_phase,
                "progress": self.research_progress,
                "tools_used": self.current_tools_used,
                "findings_count": self.findings_count,
                "memory_stats": memory_stats,
                "last_updated": datetime.now().isoformat()
            }

        except Exception as e:
            return {
                "session_id": self.session_id,
                "status": "error",
                "error": str(e)
            }

    async def export_session_data(self) -> Dict[str, Any]:
        """
        å¯¼å‡ºä¼šè¯æ•°æ®

        Returns:
            ä¼šè¯æ•°æ®å­—å…¸
        """
        # ä» agent çš„ memory ä¸­è·å–å¯¹è¯å†å²
        memory_list = []
        if hasattr(self, 'memory') and self.memory:
            # AgentScope çš„ memory å¯¹è±¡æœ‰ get_memory() æ–¹æ³•
            if hasattr(self.memory, 'get_memory'):
                # âœ… ä¿®å¤ï¼šæ­£ç¡®è°ƒç”¨å¼‚æ­¥æ–¹æ³•å¹¶ await
                msgs = await self.memory.get_memory()
                for msg in msgs:
                    memory_list.append({
                        "role": msg.role if hasattr(msg, 'role') else 'unknown',
                        "name": msg.name if hasattr(msg, 'name') else 'unknown',
                        "content": msg.content if hasattr(msg, 'content') else str(msg),
                        "timestamp": msg.timestamp if hasattr(msg, 'timestamp') else datetime.now().isoformat()
                    })
        
        # è·å–ç ”ç©¶å‘ç°å’Œå¼•ç”¨
        findings = await self.session_memory.get_research_findings() if self.session_memory else []
        citations = await self.session_memory.get_citations() if self.session_memory else []
        
        # âœ… åŒ…å«ç ”ç©¶ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        result_data = {
            "session_id": self.session_id,
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "short_memory": memory_list,
            "research_findings": findings,
            "citations": citations,
            "tools_used": self.current_tools_used,
            "findings_count": self.findings_count,
            "research_phase": self.research_phase,
            "research_progress": self.research_progress
        }
        
        # å¦‚æœç ”ç©¶å·²å®Œæˆï¼ŒåŒ…å«å®Œæ•´çš„ç ”ç©¶ç»“æœ
        if hasattr(self, 'research_result') and self.research_result:
            result_data["report"] = self.research_result.get("report", "")
            result_data["result"] = self.research_result.get("result", "")
            result_data["query"] = self.research_result.get("query", "")
        
        return result_data

    def update_tool_usage(self, tool_name: str, success: bool = True, args_str: str = "") -> None:
        """
        æ›´æ–°å·¥å…·ä½¿ç”¨è®°å½•

        Args:
            tool_name: ä½¿ç”¨çš„å·¥å…·åç§°
            success: å·¥å…·è°ƒç”¨æ˜¯å¦æˆåŠŸ
            args_str: å·¥å…·å‚æ•°çš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼ˆç”¨äºæ£€æµ‹é‡å¤è°ƒç”¨ï¼‰
        """
        if tool_name not in self.current_tools_used:
            self.current_tools_used.append(tool_name)
        
        # è·Ÿè¸ªå¤±è´¥
        if not success:
            self.tool_failure_tracker[tool_name] = self.tool_failure_tracker.get(tool_name, 0) + 1
            self.consecutive_failures += 1
            
            if self.tool_failure_tracker[tool_name] >= self.max_tool_failures:
                print(f"âš ï¸ å·¥å…· {tool_name} å·²è¿ç»­å¤±è´¥ {self.tool_failure_tracker[tool_name]} æ¬¡")
        else:
            # æˆåŠŸåˆ™é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
            self.consecutive_failures = 0
        
        # è·Ÿè¸ªç›¸åŒå‚æ•°çš„é‡å¤è°ƒç”¨
        if args_str:
            import hashlib
            args_hash = hashlib.md5(args_str.encode()).hexdigest()
            if tool_name not in self.tool_call_history:
                self.tool_call_history[tool_name] = {}
            self.tool_call_history[tool_name][args_hash] = self.tool_call_history[tool_name].get(args_hash, 0) + 1
            
            # æ£€æµ‹é‡å¤è°ƒç”¨
            if self.tool_call_history[tool_name][args_hash] >= 2:
                print(f"âš ï¸ æ£€æµ‹åˆ°é‡å¤è°ƒç”¨: {tool_name} ä½¿ç”¨ç›¸åŒå‚æ•°å·²è¢«è°ƒç”¨ {self.tool_call_history[tool_name][args_hash]} æ¬¡")
                print(f"   å»ºè®®: å°è¯•ä¸åŒçš„å·¥å…·æˆ–å‚æ•°ï¼Œæˆ–ç»§ç»­ä¸‹ä¸€æ­¥ç ”ç©¶")
        
        # è®°å½•æœ€è¿‘çš„æ“ä½œç”¨äºæ£€æµ‹å¾ªç¯
        self.recent_actions.append(tool_name)
        if len(self.recent_actions) > self.max_stagnation_check:
            self.recent_actions.pop(0)
        
        # æ£€æµ‹æ˜¯å¦åœ¨åŸåœ°è¸æ­¥ï¼ˆè¿ç»­è°ƒç”¨åŒä¸€ä¸ªå·¥å…·ï¼‰
        if len(self.recent_actions) >= self.max_stagnation_check:
            if len(set(self.recent_actions)) == 1:
                print(f"âš ï¸ æ£€æµ‹åˆ°å¾ªç¯: è¿ç»­ {self.max_stagnation_check} æ¬¡è°ƒç”¨ {tool_name}")
                print(f"   å»ºè®®: åˆ‡æ¢åˆ°å…¶ä»–å·¥å…·ï¼ˆå¦‚ search_arxiv_papersï¼‰æˆ–ç”Ÿæˆç ”ç©¶æŠ¥å‘Š")