# -*- coding: utf-8 -*-
# æ–‡ä»¶è·¯å¾„: D:\python_code\AgentWork\tests\test_llm.py

"""
è¯´æ˜:
è¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ‰§è¡Œè„šæœ¬ï¼Œç”¨äºä¾æ¬¡æ£€æŸ¥æœ¬åœ° Ollama æ¨¡å‹å’Œ Kimi æ¨¡å‹çš„æœåŠ¡çŠ¶æ€ã€‚
å®ƒä¼šä»é¡¹ç›®æ ¹ç›®å½•çš„ .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ã€‚

æ‰§è¡Œæµç¨‹:
1. åŠ è½½ .env æ–‡ä»¶ä¸­çš„é…ç½®ã€‚
2. ä¾æ¬¡è¯·æ±‚ OLLAMA_MODELS_TO_TEST åˆ—è¡¨ä¸­çš„æ‰€æœ‰æœ¬åœ°æ¨¡å‹ï¼Œå¹¶æ‰“å°ç»“æœã€‚
3. è¯·æ±‚ Kimi APIï¼Œæµ‹è¯•å…¶è”ç½‘æœç´¢åŠŸèƒ½ã€‚
"""
import os
import re  # <-- æ–°å¢ï¼šå¼•å…¥æ­£åˆ™è¡¨è¾¾å¼åº“
import asyncio
import json
import httpx
from openai import AsyncOpenAI
from dotenv import load_dotenv

# --- å…¨å±€é…ç½® ---
# åŠ è½½é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '..', '.env'))

# éœ€è¦æµ‹è¯•çš„ Ollama æ¨¡å‹åˆ—è¡¨
OLLAMA_MODELS_TO_TEST = ["qwen3:12b", "gemma3:4b", "qwen3:8b"]
OLLAMA_API_BASE_URL = "http://localhost:11434/api/chat"

# Kimi æ¨¡å‹é…ç½®
KIMI_MODEL_NAME = "moonshot-v1-8k"
KIMI_API_BASE_URL = "https://api.moonshot.cn/v1"


async def run_ollama_checks():
    """
    ä¾æ¬¡æ£€æŸ¥æ‰€æœ‰æŒ‡å®šçš„ Ollama æ¨¡å‹ï¼Œå¹¶è¿‡æ»¤æ‰ <think> æ ‡ç­¾ã€‚
    """
    print("=========================================")
    print("===          å¼€å§‹æ£€æŸ¥ Ollama æ¨¡å‹        ===")
    print("=========================================\n")

    async with httpx.AsyncClient(timeout=120.0) as client:
        for model in OLLAMA_MODELS_TO_TEST:
            print(f"--- æ­£åœ¨è¯·æ±‚æ¨¡å‹: {model} ---")
            try:
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç”¨ä¸­æ–‡ç®€çŸ­ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}],
                    "stream": False
                }
                response = await client.post(OLLAMA_API_BASE_URL, json=payload)
                response.raise_for_status()

                response_data = response.json()
                content = response_data.get("message", {}).get("content", "æœªèƒ½è·å–å›å¤å†…å®¹ã€‚")

                # <-- ã€å…³é”®æ”¹åŠ¨ã€‘ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤ <think> å—
                # re.DOTALL æ ‡å¿—è®© '.' å¯ä»¥åŒ¹é…åŒ…æ‹¬æ¢è¡Œç¬¦åœ¨å†…çš„ä»»æ„å­—ç¬¦
                cleaned_content = re.sub(r'<think>.*?</think>\s*', '', content, flags=re.DOTALL).strip()

                print(f"âœ… [{model}] å›å¤æˆåŠŸ:\n{cleaned_content}\n")

            except httpx.ConnectError:
                print(f"âŒ é”™è¯¯: æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ at {OLLAMA_API_BASE_URL}ã€‚\n")
                return
            except httpx.HTTPStatusError as e:
                error_body = e.response.text
                print(f"âŒ é”™è¯¯: è¯·æ±‚æ¨¡å‹ '{model}' æ—¶å‘ç”Ÿ HTTP é”™è¯¯ (çŠ¶æ€ç : {e.response.status_code})ã€‚")
                if "model not found" in error_body:
                    print(f"   æç¤º: æ¨¡å‹æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ 'ollama pull {model}' æ‹‰å–æ¨¡å‹ã€‚\n")
                else:
                    print(f"   æœåŠ¡å™¨è¿”å›ä¿¡æ¯: {error_body}\n")
            except Exception as e:
                print(f"âŒ é”™è¯¯: æ£€æŸ¥æ¨¡å‹ '{model}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}\n")


async def run_kimi_check():
    """
    æ£€æŸ¥ Kimi æ¨¡å‹çš„è”ç½‘æœç´¢åŠŸèƒ½ã€‚
    """
    print("\n=========================================")
    print("===          å¼€å§‹æ£€æŸ¥ Kimi æ¨¡å‹          ===")
    print("=========================================\n")

    api_key = os.getenv("MOONSHOT_API_KEY")

    # <-- ã€å…³é”®æ”¹åŠ¨ã€‘æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼Œæ£€æŸ¥è¯»å–åˆ°çš„ API Key
    if api_key:
        # ä¸ºäº†å®‰å…¨ï¼Œåªæ‰“å°éƒ¨åˆ†keyï¼Œç¡®è®¤å®ƒä¸æ˜¯ç©ºçš„
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯: å·²ä» .env æ–‡ä»¶è¯»å–åˆ° MOONSHOT_API_KEYï¼Œå¼€å¤´ä¸º: {api_key[:5]}...\n")
    else:
        print("âŒ è­¦å‘Š: æœªåœ¨ .env æ–‡ä»¶ä¸­æ‰¾åˆ° 'MOONSHOT_API_KEY'ã€‚")
        print("   è¯·åœ¨ D:\\python_code\\AgentWork\\.env æ–‡ä»¶ä¸­è¿›è¡Œé…ç½®ã€‚\n")
        return

    try:
        client = AsyncOpenAI(api_key=api_key, base_url=KIMI_API_BASE_URL)
        print(f"--- ä½¿ç”¨æ¨¡å‹: {KIMI_MODEL_NAME} ---")
        query = "ç®€å•ä»‹ç»ä¸€ä¸‹2025å¹´çš„å¤§åŒå©šæ¡ˆå’Œå„å¤§åª’ä½“è®¾è®ºå›çš„è¯„ä»·æ˜¯ä»€ä¹ˆï¼Ÿ"
        print(f"è”ç½‘æœç´¢æŸ¥è¯¢: {query}\n")

        tools = [{"type": "builtin_function", "function": {"name": "$web_search"}}]
        messages = [{"role": "user", "content": query}]

        completion = await client.chat.completions.create(model=KIMI_MODEL_NAME, messages=messages, tools=tools)
        choice = completion.choices[0]
        messages.append(choice.message)

        if choice.finish_reason == "tool_calls":
            for tool_call in choice.message.tool_calls:
                tool_call_arguments = json.loads(tool_call.function.arguments)
                messages.append({"role": "tool", "tool_call_id": tool_call.id, "name": tool_call.function.name,
                                 "content": json.dumps(tool_call_arguments)})

            final_completion = await client.chat.completions.create(model=KIMI_MODEL_NAME, messages=messages)
            final_content = final_completion.choices[0].message.content
            print(f"âœ… Kimi è”ç½‘æœç´¢å›å¤æˆåŠŸ:\n{final_content}\n")
        else:
            print(f"âŒ è­¦å‘Š: Kimi æ¨¡å‹æœªæŒ‰é¢„æœŸè°ƒç”¨æœç´¢å·¥å…·ï¼Œç›´æ¥è¾“å‡ºç°æœ‰å†…å®¹:\n{choice.message.content}\n")

    except Exception as e:
        print(f"âŒ é”™è¯¯: Kimi æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\n")


async def main():
    """ä¸»å‡½æ•°ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰æ£€æŸ¥ä»»åŠ¡ã€‚"""
    await run_ollama_checks()
    await run_kimi_check()
    print("=========================================")
    print("===         æ‰€æœ‰æ£€æŸ¥ä»»åŠ¡å·²å®Œæˆ          ===")
    print("=========================================\n")


if __name__ == "__main__":
    asyncio.run(main())