# -*- coding: utf-8 -*-
"""
ä½¿ç”¨æŒ‡å®šçš„ä¸¤ä¸ªæ¨¡å‹åˆ†æ3.jpgå›¾åƒ
ä¾æ¬¡ä½¿ç”¨: qwen3vl-4b å’Œ gemma3:4b
"""

import os
import base64
import asyncio
import aiohttp
import json


class OllamaImageAnalyzer:
    """Ollama å›¾åƒåˆ†æå™¨"""

    def __init__(self, host='http://localhost:11434'):
        self.host = host.rstrip('/')
        self.timeout = aiohttp.ClientTimeout(total=300)

    def _encode_image_to_base64(self, image_path: str) -> str:
        """å°†å›¾åƒæ–‡ä»¶ç¼–ç ä¸ºbase64æ ¼å¼"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    async def analyze_image(self, model: str, image_path: str, prompt: str) -> dict:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆ†æå›¾åƒ"""
        image_b64 = self._encode_image_to_base64(image_path)

        data = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt,
                    "images": [image_b64]
                }
            ],
            "stream": False
        }

        url = f"{self.host}/api/chat"

        async with aiohttp.ClientSession(timeout=self.timeout) as session:
            async with session.post(url, json=data) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"Ollama APIé”™è¯¯: {response.status} - {error_text}")
                return await response.json()


async def analyze_with_two_models():
    """ä½¿ç”¨ä¸¤ä¸ªæŒ‡å®šæ¨¡å‹ä¾æ¬¡åˆ†æå›¾åƒ"""
    print("=" * 60)
    print("åŒæ¨¡å‹å›¾åƒåˆ†ææµ‹è¯•")
    print("ä½¿ç”¨æ¨¡å‹: qwen3-vl:4b, gemma3:4b")
    print("=" * 60)

    # å›¾åƒæ–‡ä»¶è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    image_path = os.path.join(script_dir, "1.png")

    print(f"åˆ†æå›¾åƒ: {image_path}")

    if not os.path.exists(image_path):
        print(f"âŒ é”™è¯¯: å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨ {image_path}")
        return

    # æŒ‡å®šçš„ä¸¤ä¸ªæ¨¡å‹
    models = ["qwen3-vl:4b", "gemma3:4b"]

    # åˆ†ææç¤ºè¯
    prompt = "è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š\n1. ä¸»è¦ç‰©ä½“å’Œåœºæ™¯\n2. æ–‡å­—ä¿¡æ¯\n3. é¢œè‰²å’Œå¸ƒå±€\n4. å¯èƒ½çš„åŠŸèƒ½å’Œç”¨é€”"

    # ä¾æ¬¡ä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹
    for i, model in enumerate(models, 1):
        print(f"\n{'='*25}")
        print(f"æ¨¡å‹ {i}: {model}")
        print(f"{'='*25}")

        try:
            print(f"æ­£åœ¨ä½¿ç”¨ {model} åˆ†æå›¾åƒ...")

            analyzer = OllamaImageAnalyzer()
            result = await analyzer.analyze_image(model, image_path, prompt)

            if "message" in result and "content" in result["message"]:
                content = result["message"]["content"]
                print(f"âœ… åˆ†æç»“æœ:\n{content}")
            else:
                print("âŒ æœªè·å–åˆ°æœ‰æ•ˆç»“æœ")

        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model} åˆ†æå¤±è´¥: {e}")
            if "not found" in str(e).lower():
                print(f"ğŸ’¡ å»ºè®®: è¯·ç¡®ä¿æ¨¡å‹å·²å®‰è£… - ollama pull {model}")

    print(f"\n{'='*60}")
    print("åŒæ¨¡å‹åˆ†æå®Œæˆ")
    print(f"{'='*60}")


if __name__ == "__main__":
    print("Ollama åŒæ¨¡å‹å›¾åƒåˆ†æå·¥å…·")
    print("ç¡®ä¿å·²å®‰è£…æŒ‡å®šæ¨¡å‹:")
    print("  ollama pull qwen3-vl:4b")
    print("  ollama pull gemma3:4b")
    print()

    asyncio.run(analyze_with_two_models())