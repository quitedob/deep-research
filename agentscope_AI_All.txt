==== agentscope ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The agentscope serialization module"""
import os
import requests
from . import exception
from . import module
from . import message
from . import model
from . import tool
from . import formatter
from . import memory
from . import agent
from . import session
from . import embedding
from . import token
from . import evaluate
from . import pipeline
from . import tracing
from . import rag
from ._logging import (
logger,
setup_logger,
)
from .hooks import _equip_as_studio_hooks
from ._version import __version__
def init(
project: str | None = None,
name: str | None = None,
logging_path: str | None = None,
logging_level: str = "INFO",
studio_url: str | None = None,
tracing_url: str | None = None,
) -> None:
"""Initialize the agentscope library.
Args:
project (`str | None`, optional):
The project name.
name (`str | None`, optional):
The name of the run.
logging_path (`str | None`, optional):
The path to saving the log file. If not provided, logs will not be
saved.
logging_level (`str | None`, optional):
The logging level. Defaults to "INFO".
studio_url (`str | None`, optional):
The URL of the AgentScope Studio to connect to.
tracing_url (`str | None`, optional):
The URL of the tracing endpoint, which can connect to third-party
OpenTelemetry tracing platforms like Arize-Phoenix and Langfuse.
If not provided and `studio_url` is provided, it will send traces
to the AgentScope Studio's tracing endpoint.
"""
from . import _config
if project:
_config.project = project
if name:
_config.name = name
setup_logger(logging_level, logging_path)
if studio_url:
# Register the run
data = {
"id": _config.run_id,
"project": _config.project,
"name": _config.name,
"timestamp": _config.created_at,
"pid": os.getpid(),
"status": "running",
# Deprecated fields
"run_dir": "",
}
response = requests.post(
url=f"{studio_url}/trpc/registerRun",
json=data,
)
response.raise_for_status()
from .agent import UserAgent, StudioUserInput
UserAgent.override_class_input_method(
StudioUserInput(
studio_url=studio_url,
run_id=_config.run_id,
max_retries=3,
),
)
_equip_as_studio_hooks(studio_url)
if tracing_url:
endpoint = tracing_url
else:
endpoint = studio_url.strip("/") + "/v1/traces" if studio_url else None
if endpoint:
from .tracing import setup_tracing
setup_tracing(endpoint=endpoint)
__all__ = [
# modules
"exception",
"module",
"message",
"model",
"tool",
"formatter",
"memory",
"agent",
"session",
"logger",
"embedding",
"token",
"evaluate",
"pipeline",
"tracing",
"rag",
# functions
"init",
"setup_logger",
"__version__",
]
---- _config.py ----
# -*- coding: utf-8 -*-
"""The runtime configuration in agentscope.
.. note:: You should import this module as ``import ._config``, then use the
variables defined in this module, instead of ``from ._config import xxx``.
Because when the variables are changed, the changes will not be reflected in
the imported module.
"""
from datetime import datetime
import shortuuid
def _generate_random_suffix(length: int) -> str:
"""Generate a random suffix."""
return shortuuid.uuid()[:length]
project = "UnnamedProject_At" + datetime.now().strftime("%Y%m%d")
name = datetime.now().strftime("%H%M%S_") + _generate_random_suffix(4)
run_id: str = shortuuid.uuid()
created_at: str = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
trace_enabled: bool = False
---- _logging.py ----
# -*- coding: utf-8 -*-
"""The logger for agentscope."""
import logging
_DEFAULT_FORMAT = (
"%(asctime)s | %(levelname)-7s | "
"%(module)s:%(funcName)s:%(lineno)s - %(message)s"
)
logger = logging.getLogger("as")
def setup_logger(
level: str,
filepath: str | None = None,
) -> None:
"""Set up the agentscope logger.
Args:
level (`str`):
The logging level, chosen from "INFO", "DEBUG", "WARNING",
"ERROR", "CRITICAL".
filepath (`str | None`, optional):
The filepath to save the logging output.
"""
if level not in ["INFO", "DEBUG", "WARNING", "ERROR", "CRITICAL"]:
raise ValueError(
f"Invalid logging level: {level}. Must be one of "
f"'INFO', 'DEBUG', 'WARNING', 'ERROR', 'CRITICAL'.",
)
logger.handlers.clear()
logger.setLevel(level)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter(_DEFAULT_FORMAT))
logger.addHandler(handler)
if filepath:
handler = logging.FileHandler(filepath)
handler.setFormatter(logging.Formatter(_DEFAULT_FORMAT))
logger.addHandler(handler)
logger.propagate = False
setup_logger("INFO")
---- _version.py ----
# -*- coding: utf-8 -*-
"""The version of agentscope."""
__version__ = "1.0.6"
==== _utils ====
---- __init__.py ----
---- _common.py ----
# -*- coding: utf-8 -*-
"""The common utilities for agentscope library."""
import asyncio
import base64
import functools
import inspect
import json
import os
import tempfile
import types
import typing
import uuid
from datetime import datetime
from typing import Union, Any, Callable, Type, Dict
import requests
from json_repair import repair_json
from pydantic import BaseModel
from .._logging import logger
if typing.TYPE_CHECKING:
from mcp.types import Tool
else:
Tool = "mcp.types.Tool"
def _json_loads_with_repair(
json_str: str,
) -> Union[dict, list, str, float, int, bool, None]:
"""The given json_str maybe incomplete, e.g. '{"key', so we need to
repair and load it into a Python object.
"""
repaired = json_str
try:
repaired = repair_json(json_str)
except Exception:
pass
try:
return json.loads(repaired)
except json.JSONDecodeError as e:
raise ValueError(
f"Failed to decode JSON string `{json_str}` after repairing it "
f"into `{repaired}`. Error: {e}",
) from e
def _is_accessible_local_file(url: str) -> bool:
"""Check if the given URL is a local URL."""
return os.path.isfile(url)
def _get_timestamp(add_random_suffix: bool = False) -> str:
"""Get the current timestamp in the format YYYY-MM-DD HH:MM:SS.sss."""
timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
if add_random_suffix:
# Add a random suffix to the timestamp
timestamp += f"_{os.urandom(3).hex()}"
return timestamp
async def _is_async_func(func: Callable) -> bool:
"""Check if the given function is an async function, including
coroutine functions, async generators, and coroutine objects.
"""
return (
inspect.iscoroutinefunction(func)
or inspect.isasyncgenfunction(func)
or isinstance(func, types.CoroutineType)
or isinstance(func, types.GeneratorType)
and asyncio.iscoroutine(func)
or isinstance(func, functools.partial)
and await _is_async_func(func.func)
)
async def _execute_async_or_sync_func(
func: Callable,
*args: Any,
**kwargs: Any,
) -> Any:
"""Execute an async or sync function based on its type.
Args:
func (`Callable`):
The function to be executed, which can be either async or sync.
*args (`Any`):
Positional arguments to be passed to the function.
**kwargs (`Any`):
Keyword arguments to be passed to the function.
Returns:
`Any`:
The result of the function execution.
"""
if await _is_async_func(func):
return await func(*args, **kwargs)
return func(*args, **kwargs)
def _get_bytes_from_web_url(
url: str,
max_retries: int = 3,
) -> str:
"""Get the bytes from a given URL.
Args:
url (`str`):
The URL to fetch the bytes from.
max_retries (`int`, defaults to `3`):
The maximum number of retries.
"""
for _ in range(max_retries):
try:
response = requests.get(url)
response.raise_for_status()
return response.content.decode("utf-8")
except UnicodeDecodeError:
return base64.b64encode(response.content).decode("ascii")
except Exception as e:
logger.info(
"Failed to fetch bytes from URL %s. Error %s. Retrying...",
url,
str(e),
)
raise RuntimeError(
f"Failed to fetch bytes from URL `{url}` after {max_retries} retries.",
)
def _save_base64_data(
media_type: str,
base64_data: str,
) -> str:
"""Save the base64 data to a temp file and return the file path. The
extension is guessed from the MIME type.
Args:
media_type (`str`):
The MIME type of the data, e.g. "image/png", "audio/mpeg".
base64_data (`str):
The base64 data to be saved.
"""
extension = "." + media_type.split("/")[-1]
with tempfile.NamedTemporaryFile(
suffix=f".{extension}",
delete=False,
) as temp_file:
decoded_data = base64.b64decode(base64_data)
temp_file.write(decoded_data)
temp_file.close()
return temp_file.name
def _extract_json_schema_from_mcp_tool(tool: Tool) -> dict[str, Any]:
"""Extract JSON schema from MCP tool."""
return {
"type": "function",
"function": {
"name": tool.name,
"description": tool.description,
"parameters": {
"type": "object",
"properties": tool.inputSchema.get(
"properties",
{},
),
"required": tool.inputSchema.get(
"required",
[],
),
},
},
}
def _remove_title_field(schema: dict) -> None:
"""Remove the title field from the JSON schema to avoid
misleading the LLM."""
# The top level title field
if "title" in schema:
schema.pop("title")
# properties
if "properties" in schema:
for prop in schema["properties"].values():
if isinstance(prop, dict):
_remove_title_field(prop)
# items
if "items" in schema and isinstance(schema["items"], dict):
_remove_title_field(schema["items"])
# additionalProperties
if "additionalProperties" in schema and isinstance(
schema["additionalProperties"],
dict,
):
_remove_title_field(
schema["additionalProperties"],
)
def _create_tool_from_base_model(
structured_model: Type[BaseModel],
tool_name: str = "generate_structured_output",
) -> Dict[str, Any]:
"""Create a function tool definition from a Pydantic BaseModel.
This function converts a Pydantic BaseModel class into a tool definition
that can be used with function calling API. The resulting tool
definition includes the model's JSON schema as parameters, enabling
structured output generation by forcing the model to call this function
with properly formatted data.
Args:
structured_model (`Type[BaseModel]`):
A Pydantic BaseModel class that defines the expected structure
for the tool's output.
tool_name (`str`, default `"generate_structured_output"`):
The tool name that used to force the LLM to generate structured
output by calling this function.
Returns:
`Dict[str, Any]`: A tool definition dictionary compatible with
function calling API, containing type ("function") and
function dictionary with name, description, and parameters
(JSON schema).
.. code-block:: python
:caption: Example usage
from pydantic import BaseModel
class PersonInfo(BaseModel):
name: str
age: int
email: str
tool = _create_tool_from_base_model(PersonInfo, "extract_person")
print(tool["function"]["name"]) # extract_person
print(tool["type"]) # function
.. note:: The function automatically removes the 'title' field from
the JSON schema to ensure compatibility with function calling
format. This is handled by the internal ``_remove_title_field()``
function.
"""
schema = structured_model.model_json_schema()
_remove_title_field(schema)
tool_definition = {
"type": "function",
"function": {
"name": tool_name,
"description": "Generate the required structured output with "
"this function",
"parameters": schema,
},
}
return tool_definition
def _map_text_to_uuid(text: str) -> str:
"""Map the given text to a deterministic UUID string.
Args:
text (`str`):
The input text to be mapped to a UUID.
Returns:
`str`:
A deterministic UUID string derived from the input text.
"""
return str(uuid.uuid3(uuid.NAMESPACE_DNS, text))
---- _mixin.py ----
# -*- coding: utf-8 -*-
"""The mixin for agentscope."""
class DictMixin(dict):
"""The dictionary mixin that allows attribute-style access."""
__setattr__ = dict.__setitem__
__getattr__ = dict.__getitem__
==== agent ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The agent base class."""
from ._agent_base import AgentBase
from ._react_agent_base import ReActAgentBase
from ._react_agent import ReActAgent
from ._user_input import (
UserInputBase,
UserInputData,
TerminalUserInput,
StudioUserInput,
)
from ._user_agent import UserAgent
__all__ = [
"AgentBase",
"ReActAgentBase",
"ReActAgent",
"UserInputData",
"UserInputBase",
"TerminalUserInput",
"StudioUserInput",
"UserAgent",
]
---- _agent_base.py ----
# -*- coding: utf-8 -*-
"""The agent base class in agentscope."""
import asyncio
import io
import json
from asyncio import Task, Queue
from collections import OrderedDict
from copy import deepcopy
from typing import Callable, Any
import base64
import shortuuid
import numpy as np
from typing_extensions import deprecated
from ._agent_meta import _AgentMeta
from .._logging import logger
from ..module import StateModule
from ..message import (
Msg,
AudioBlock,
ToolUseBlock,
ToolResultBlock,
ImageBlock,
VideoBlock,
)
from ..types import AgentHookTypes
class AgentBase(StateModule, metaclass=_AgentMeta):
"""Base class for asynchronous agents."""
id: str
"""The agent's unique identifier, generated using shortuuid."""
supported_hook_types: list[str] = [
"pre_reply",
"post_reply",
"pre_print",
"post_print",
"pre_observe",
"post_observe",
]
"""Supported hook types for the agent base class."""
_class_pre_reply_hooks: dict[
str,
Callable[
[
"AgentBase", # self
dict[str, Any], # kwargs
],
dict[str, Any] | None, # The modified kwargs or None
],
] = OrderedDict()
"""The class-level hook functions that will be called before the reply
function, taking `self` object, the input arguments as input, and
generating the modified arguments (if needed). Then input arguments of the
reply function will be re-organized into a keyword arguments dictionary.
If the one hook returns a new dictionary, the modified arguments will be
passed to the next hook or the original reply function."""
_class_post_reply_hooks: dict[
str,
Callable[
[
"AgentBase", # self
dict[str, Any], # kwargs
Msg, # output, the output message
],
Msg | None,
],
] = OrderedDict()
"""The class-level hook functions that will be called after the reply
function, which takes the `self` object and deep copied
positional and keyword arguments (args and kwargs), and the output message
as input. If the hook returns a message, the new message will be passed
to the next hook or the original reply function. Otherwise, the original
output will be passed instead."""
_class_pre_print_hooks: dict[
str,
Callable[
[
"AgentBase", # self
dict[str, Any], # kwargs
],
dict[str, Any] | None, # The modified kwargs or None
],
] = OrderedDict()
"""The class-level hook functions that will be called before printing,
which takes the `self` object, a deep copied arguments dictionary as input,
and output the modified arguments (if needed). """
_class_post_print_hooks: dict[
str,
Callable[
[
"AgentBase", # self
dict[str, Any], # kwargs
Any, # output, `None` if no output
],
Any,
],
] = OrderedDict()
"""The class-level hook functions that will be called after the speak
function, which takes the `self` object as input."""
_class_pre_observe_hooks: dict[
str,
Callable[
[
"AgentBase", # self
dict[str, Any], # kwargs
],
dict[str, Any] | None, # The modified kwargs or None
],
] = OrderedDict()
"""The class-level hook functions that will be called before the observe
function, which takes the `self` object and a deep copied input
arguments dictionary as input. To change the input arguments, the hook
function needs to output the modified arguments dictionary, which will be
used as the input of the next hook function or the original observe
function."""
_class_post_observe_hooks: dict[
str,
Callable[
[
"AgentBase", # self
dict[str, Any], # kwargs
None, # The output, `None` if no output
],
None,
],
] = OrderedDict()
"""The class-level hook functions that will be called after the observe
function, which takes the `self` object as input."""
def __init__(self) -> None:
"""Initialize the agent."""
super().__init__()
self.id = shortuuid.uuid()
# The replying task and identify of the current replying
self._reply_task: Task | None = None
self._reply_id: str | None = None
# Initialize the instance-level hooks
self._instance_pre_print_hooks = OrderedDict()
self._instance_post_print_hooks = OrderedDict()
self._instance_pre_reply_hooks = OrderedDict()
self._instance_post_reply_hooks = OrderedDict()
self._instance_pre_observe_hooks = OrderedDict()
self._instance_post_observe_hooks = OrderedDict()
# The prefix used in streaming printing, which will save the
# accumulated text and audio streaming data for each message id.
# e.g. {"text": "xxx", "audio": (stream_obj, "{base64_data}")}
self._stream_prefix = {}
# The subscribers that will receive the reply message by their
# `observe` method. The key is the MsgHub id, and the value is the
# list of agents.
self._subscribers: dict[str, list[AgentBase]] = {}
# We add this variable in case developers want to disable the console
# output of the agent, e.g., in a production environment.
self._disable_console_output: bool = False
# The streaming message queue used to export the messages as a
# generator
self._disable_msg_queue: bool = True
self.msg_queue = None
async def observe(self, msg: Msg | list[Msg] | None) -> None:
"""Receive the given message(s) without generating a reply.
Args:
msg (`Msg | list[Msg] | None`):
The message(s) to be observed.
"""
raise NotImplementedError(
f"The observe function is not implemented in"
f" {self.__class__.__name__} class.",
)
async def reply(self, *args: Any, **kwargs: Any) -> Msg:
"""The main logic of the agent, which generates a reply based on the
current state and input arguments."""
raise NotImplementedError(
"The reply function is not implemented in "
f"{self.__class__.__name__} class.",
)
async def print(self, msg: Msg, last: bool = True) -> None:
"""The function to display the message.
Args:
msg (`Msg`):
The message object to be printed.
last (`bool`, defaults to `True`):
Whether this is the last one in streaming messages. For
non-streaming message, this should always be `True`.
"""
if not self._disable_msg_queue:
await self.msg_queue.put((deepcopy(msg), last))
if self._disable_console_output:
return
# The accumulated textual content to print, including the text blocks
# and the thinking blocks
thinking_and_text_to_print = []
for block in msg.get_content_blocks():
if block["type"] == "audio":
self._process_audio_block(msg.id, block)
elif block["type"] == "text":
self._print_text_block(
msg.id,
name_prefix=msg.name,
text_content=block["text"],
thinking_and_text_to_print=thinking_and_text_to_print,
)
elif block["type"] == "thinking":
self._print_text_block(
msg.id,
name_prefix=f"{msg.name}(thinking)",
text_content=block["thinking"],
thinking_and_text_to_print=thinking_and_text_to_print,
)
elif last:
self._print_last_block(block, msg)
# Clean up resources if this is the last message in streaming
if last and msg.id in self._stream_prefix:
if "audio" in self._stream_prefix[msg.id]:
player, _ = self._stream_prefix[msg.id]["audio"]
# Close the miniaudio player
player.close()
stream_prefix = self._stream_prefix.pop(msg.id)
if "text" in stream_prefix and not stream_prefix["text"].endswith(
"\n",
):
print()
def _process_audio_block(
self,
msg_id: str,
audio_block: AudioBlock,
) -> None:
"""Process audio block content.
Args:
msg_id (`str`):
The unique identifier of the message
audio_block (`AudioBlock`):
The audio content block
"""
if "source" not in audio_block:
raise ValueError(
"The audio block must contain the 'source' field.",
)
if audio_block["source"]["type"] == "url":
import urllib.request
import wave
import sounddevice as sd
url = audio_block["source"]["url"]
try:
with urllib.request.urlopen(url) as response:
audio_data = response.read()
with wave.open(io.BytesIO(audio_data), "rb") as wf:
samplerate = wf.getframerate()
n_frames = wf.getnframes()
audio_frames = wf.readframes(n_frames)
# Convert byte data to numpy array
audio_np = np.frombuffer(audio_frames, dtype=np.int16)
# Play audio
sd.play(audio_np, samplerate)
sd.wait()
except Exception as e:
logger.error(
"Failed to play audio from url %s: %s",
url,
str(e),
)
elif audio_block["source"]["type"] == "base64":
data = audio_block["source"]["data"]
if msg_id not in self._stream_prefix:
self._stream_prefix[msg_id] = {}
audio_prefix = self._stream_prefix[msg_id].get("audio", None)
import sounddevice as sd
# The player and the prefix data is cached for streaming audio
if audio_prefix:
player, audio_prefix_data = audio_prefix
else:
player = sd.OutputStream(
samplerate=24000,
channels=1,
dtype=np.float32,
blocksize=1024,
latency="low",
)
player.start()
audio_prefix_data = ""
# play the audio data
new_audio_data = data[len(audio_prefix_data) :]
if new_audio_data:
audio_bytes = base64.b64decode(new_audio_data)
audio_np = np.frombuffer(audio_bytes, dtype=np.int16)
audio_float = audio_np.astype(np.float32) / 32768.0
# Write to the audio output stream
player.write(audio_float)
# save the player and the prefix data
self._stream_prefix[msg_id]["audio"] = (
player,
data,
)
else:
raise ValueError(
"Unsupported audio source type: "
f"{audio_block['source']['type']}",
)
def _print_text_block(
self,
msg_id: str,
name_prefix: str,
text_content: str,
thinking_and_text_to_print: list[str],
) -> None:
"""Print the text block and thinking block content.
Args:
msg_id (`str`):
The unique identifier of the message
name_prefix (`str`):
The prefix for the message, e.g. "{name}: " for text block and
"{name}(thinking): " for thinking block.
text_content (`str`):
The textual content to be printed.
thinking_and_text_to_print (`list[str]`):
A list of textual content to be printed together. Here we
gather the text and thinking blocks to print them together.
"""
thinking_and_text_to_print.append(
f"{name_prefix}: {text_content}",
)
# The accumulated text and thinking blocks to print
to_print = "\n".join(thinking_and_text_to_print)
# The text prefix that has been printed
if msg_id not in self._stream_prefix:
self._stream_prefix[msg_id] = {}
text_prefix = self._stream_prefix[msg_id].get("text", "")
# Only print when there is new text content
if len(to_print) > len(text_prefix):
print(to_print[len(text_prefix) :], end="")
# Save the printed text prefix
self._stream_prefix[msg_id]["text"] = to_print
def _print_last_block(
self,
block: ToolUseBlock | ToolResultBlock | ImageBlock | VideoBlock,
msg: Msg,
) -> None:
"""Process and print the last content block, and the block type
is not audio, text, or thinking.
Args:
block (`ToolUseBlock | ToolResultBlock | ImageBlock | VideoBlock`):
The content block to be printed
msg (`Msg`):
The message object
"""
text_prefix = self._stream_prefix.get(msg.id, {}).get("text", "")
if text_prefix:
# Add a newline to separate from previous text content
print_newline = "" if text_prefix.endswith("\n") else "\n"
print(
f"{print_newline}"
f"{json.dumps(block, indent=4, ensure_ascii=False)}",
)
else:
print(
f"{msg.name}:"
f" {json.dumps(block, indent=4, ensure_ascii=False)}",
)
async def __call__(self, *args: Any, **kwargs: Any) -> Msg:
"""Call the reply function with the given arguments."""
self._reply_id = shortuuid.uuid()
reply_msg: Msg | None = None
try:
self._reply_task = asyncio.current_task()
reply_msg = await self.reply(*args, **kwargs)
# The interruption is triggered by calling the interrupt method
except asyncio.CancelledError:
reply_msg = await self.handle_interrupt(*args, **kwargs)
finally:
# Broadcast the reply message to all subscribers
if reply_msg:
await self._broadcast_to_subscribers(reply_msg)
self._reply_task = None
return reply_msg
async def _broadcast_to_subscribers(
self,
msg: Msg | list[Msg] | None,
) -> None:
"""Broadcast the message to all subscribers."""
for subscribers in self._subscribers.values():
for subscriber in subscribers:
await subscriber.observe(msg)
async def handle_interrupt(
self,
*args: Any,
**kwargs: Any,
) -> Msg:
"""The post-processing logic when the reply is interrupted by the
user or something else."""
raise NotImplementedError(
f"The handle_interrupt function is not implemented in "
f"{self.__class__.__name__}",
)
async def interrupt(self, msg: Msg | list[Msg] | None = None) -> None:
"""Interrupt the current reply process."""
if self._reply_task and not self._reply_task.done():
self._reply_task.cancel(msg)
def register_instance_hook(
self,
hook_type: AgentHookTypes,
hook_name: str,
hook: Callable,
) -> None:
"""Register a hook to the agent instance, which only takes effect
for the current instance.
Args:
hook_type (`str`):
The type of the hook, indicating where the hook is to be
triggered.
hook_name (`str`):
The name of the hook. If the name is already registered, the
hook will be overwritten.
hook (`Callable`):
The hook function.
"""
if not isinstance(self, AgentBase):
raise TypeError(
"The register_instance_hook method should be called on an "
f"instance of AsyncAgentBase, but got {self} of "
f"type {type(self)}.",
)
hooks = getattr(self, f"_instance_{hook_type}_hooks")
hooks[hook_name] = hook
def remove_instance_hook(
self,
hook_type: AgentHookTypes,
hook_name: str,
) -> None:
"""Remove an instance-level hook from the agent instance.
Args:
hook_type (`AgentHookTypes`):
The type of the hook, indicating where the hook is to be
triggered.
hook_name (`str`):
The name of the hook to remove.
"""
if not isinstance(self, AgentBase):
raise TypeError(
"The remove_instance_hook method should be called on an "
f"instance of AsyncAgentBase, but got {self} of "
f"type {type(self)}.",
)
hooks = getattr(self, f"_instance_{hook_type}_hooks")
if hook_name in hooks:
del hooks[hook_name]
else:
raise ValueError(
f"Hook '{hook_name}' not found in '{hook_type}' hooks of "
f"{self.__class__.__name__} instance.",
)
@classmethod
def register_class_hook(
cls,
hook_type: AgentHookTypes,
hook_name: str,
hook: Callable,
) -> None:
"""The universal function to register a hook to the agent class, which
will take effect for all instances of the class.
Args:
hook_type (`AgentHookTypes`):
The type of the hook, indicating where the hook is to be
triggered.
hook_name (`str`):
The name of the hook. If the name is already registered, the
hook will be overwritten.
hook (`Callable`):
The hook function.
"""
assert (
hook_type in cls.supported_hook_types
), f"Invalid hook type: {hook_type}"
hooks = getattr(cls, f"_class_{hook_type}_hooks")
hooks[hook_name] = hook
@classmethod
def remove_class_hook(
cls,
hook_type: AgentHookTypes,
hook_name: str,
) -> None:
"""Remove a class-level hook from the agent class.
Args:
hook_type (`AgentHookTypes`):
The type of the hook, indicating where the hook is to be
triggered.
hook_name (`str`):
The name of the hook to remove.
"""
assert (
hook_type in cls.supported_hook_types
), f"Invalid hook type: {hook_type}"
hooks = getattr(cls, f"_class_{hook_type}_hooks")
if hook_name in hooks:
del hooks[hook_name]
else:
raise ValueError(
f"Hook '{hook_name}' not found in '{hook_type}' hooks of "
f"{cls.__name__} class.",
)
@classmethod
def clear_class_hooks(
cls,
hook_type: AgentHookTypes | None = None,
) -> None:
"""Clear all class-level hooks.
Args:
hook_type (`AgentHookTypes`, optional):
The type of the hook to clear. If not specified, all
class-level hooks will be cleared.
"""
if hook_type is None:
for typ in cls.supported_hook_types:
hooks = getattr(cls, f"_class_{typ}_hooks")
hooks.clear()
else:
assert (
hook_type in cls.supported_hook_types
), f"Invalid hook type: {hook_type}"
hooks = getattr(cls, f"_class_{hook_type}_hooks")
hooks.clear()
def clear_instance_hooks(
self,
hook_type: AgentHookTypes | None = None,
) -> None:
"""If `hook_type` is not specified, clear all instance-level hooks.
Otherwise, clear the specified type of instance-level hooks."""
if hook_type is None:
for typ in self.supported_hook_types:
if not hasattr(self, f"_instance_{typ}_hooks"):
raise ValueError(
f"Call super().__init__() in the constructor "
f"to initialize the instance-level hooks for "
f"{self.__class__.__name__}.",
)
hooks = getattr(self, f"_instance_{typ}_hooks")
hooks.clear()
else:
assert (
hook_type in self.supported_hook_types
), f"Invalid hook type: {hook_type}"
if not hasattr(self, f"_instance_{hook_type}_hooks"):
raise ValueError(
f"Call super().__init__() in the constructor "
f"to initialize the instance-level hooks for "
f"{self.__class__.__name__}.",
)
hooks = getattr(self, f"_instance_{hook_type}_hooks")
hooks.clear()
def reset_subscribers(
self,
msghub_name: str,
subscribers: list["AgentBase"],
) -> None:
"""Reset the subscribers of the agent.
Args:
msghub_name (`str`):
The name of the MsgHub that manages the subscribers.
subscribers (`list[AgentBase]`):
A list of agents that will receive the reply message from
this agent via their `observe` method.
"""
self._subscribers[msghub_name] = [_ for _ in subscribers if _ != self]
def remove_subscribers(self, msghub_name: str) -> None:
"""Remove the msghub subscribers by the given msg hub name.
Args:
msghub_name (`str`):
The name of the MsgHub that manages the subscribers.
"""
if msghub_name not in self._subscribers:
logger.warning(
"MsgHub named '%s' not found",
msghub_name,
)
else:
self._subscribers.pop(msghub_name)
@deprecated("Please use set_console_output_enabled() instead.")
def disable_console_output(self) -> None:
"""This function will disable the console output of the agent, e.g.
in a production environment to avoid messy logs."""
self._disable_console_output = True
def set_console_output_enabled(self, enabled: bool) -> None:
"""Enable or disable the console output of the agent. E.g. in a
production environment, you may want to disable the console output to
avoid messy logs.
Args:
enabled (`bool`):
If `True`, enable the console output. If `False`, disable
the console output.
"""
self._disable_console_output = not enabled
def set_msg_queue_enabled(
self,
enabled: bool,
queue: Queue | None = None,
) -> None:
"""Enable or disable the message queue for streaming outputs.
Args:
enabled (`bool`):
If `True`, enable the message queue to allow streaming
outputs. If `False`, disable the message queue.
queue (`Queue | None`, optional):
The queue instance that will be used to initialize the
message queue when `enable` is `True`.
"""
if enabled:
if queue is None:
if self.msg_queue is None:
self.msg_queue = asyncio.Queue(maxsize=100)
else:
self.msg_queue = queue
else:
self.msg_queue = None
self._disable_msg_queue = not enabled
---- _agent_meta.py ----
# -*- coding: utf-8 -*-
"""The metaclass for agents in agentscope."""
import inspect
from copy import deepcopy
from functools import wraps
from typing import (
Any,
Dict,
TYPE_CHECKING,
Callable,
)
from .._utils._common import _execute_async_or_sync_func
if TYPE_CHECKING:
from ._agent_base import AgentBase
else:
AgentBase = "AgentBase"
def _normalize_to_kwargs(
func: Callable,
self: Any,
*args: Any,
**kwargs: Any,
) -> dict:
"""Normalize the provided positional and keyword arguments into a
keyword arguments dictionary that matches the function signature."""
sig = inspect.signature(func)
try:
# Bind the provided arguments to the function signature
bound = sig.bind(self, *args, **kwargs)
# Apply the default values for parameters
bound.apply_defaults()
# Return the arguments in a dictionary format
res = dict(bound.arguments)
res.pop("self")
return res
except TypeError as e:
# If failed to bind, we raise a TypeError with more context
param_names = list(sig.parameters.keys())
provided_args = len(args)
provided_kwargs = list(kwargs.keys())
raise TypeError(
f"Failed to bind parameters for function '{func.__name__}': {e}\n"
f"Expected parameters: {param_names}\n"
f"Provided {provided_args} positional args and kwargs: "
f"{provided_kwargs}",
) from e
def _wrap_with_hooks(
original_func: Callable,
) -> Callable:
"""A decorator to wrap the original async function with pre- and post-hooks
Args:
original_func (`Callable`):
The original async function to be wrapped with hooks.
"""
func_name = original_func.__name__.replace("_", "")
@wraps(original_func)
async def async_wrapper(
self: AgentBase,
*args: Any,
**kwargs: Any,
) -> Any:
"""The wrapped function, which call the pre- and post-hooks before and
after the original function."""
# Unify all positional and keyword arguments into a keyword arguments
normalized_kwargs = _normalize_to_kwargs(
original_func,
self,
*args,
**kwargs,
)
current_normalized_kwargs = normalized_kwargs
assert (
hasattr(self, f"_instance_pre_{func_name}_hooks")
and hasattr(self, f"_instance_post_{func_name}_hooks")
and hasattr(self.__class__, f"_class_pre_{func_name}_hooks")
and hasattr(self.__class__, f"_class_post_{func_name}_hooks")
), f"Hooks for {func_name} not found in {self.__class__.__name__}"
# pre-hooks
pre_hooks = list(
getattr(self, f"_instance_pre_{func_name}_hooks").values(),
) + list(
getattr(self, f"_class_pre_{func_name}_hooks").values(),
)
for pre_hook in pre_hooks:
modified_keywords = await _execute_async_or_sync_func(
pre_hook,
self,
deepcopy(current_normalized_kwargs),
)
if modified_keywords is not None:
assert isinstance(modified_keywords, dict), (
f"Pre-hook must return a dict of keyword arguments, rather"
f" than {type(modified_keywords)} from hook "
f"{pre_hook.__name__}"
)
current_normalized_kwargs = modified_keywords
# original function
# handle positional and keyword arguments specifically
args = current_normalized_kwargs.get("args", [])
kwargs = current_normalized_kwargs.get("kwargs", {})
others = {
k: v
for k, v in current_normalized_kwargs.items()
if k not in ["args", "kwargs"]
}
current_output = await original_func(
self,
*args,
**others,
**kwargs,
)
# post_hooks
post_hooks = list(
getattr(self, f"_instance_post_{func_name}_hooks").values(),
) + list(
getattr(self, f"_class_post_{func_name}_hooks").values(),
)
for post_hook in post_hooks:
modified_output = await _execute_async_or_sync_func(
post_hook,
self,
deepcopy(current_normalized_kwargs),
deepcopy(current_output),
)
if modified_output is not None:
current_output = modified_output
return current_output
return async_wrapper
class _AgentMeta(type):
"""The agent metaclass that wraps the agent's reply, observe and print
functions with pre- and post-hooks."""
def __new__(mcs, name: Any, bases: Any, attrs: Dict) -> Any:
"""Wrap the agent's functions with hooks."""
for func_name in [
"reply",
"print",
"observe",
]:
if func_name in attrs:
attrs[func_name] = _wrap_with_hooks(attrs[func_name])
return super().__new__(mcs, name, bases, attrs)
class _ReActAgentMeta(_AgentMeta):
"""The ReAct metaclass that adds pre- and post-hooks for the _reasoning
and _acting functions."""
def __new__(mcs, name: Any, bases: Any, attrs: Dict) -> Any:
"""Wrap the ReAct agent's _reasoning and _acting functions with
hooks."""
for func_name in [
"_reasoning",
"_acting",
]:
if func_name in attrs:
attrs[func_name] = _wrap_with_hooks(attrs[func_name])
return super().__new__(mcs, name, bases, attrs)
---- _react_agent.py ----
# -*- coding: utf-8 -*-
# pylint: disable=not-an-iterable
# mypy: disable-error-code="list-item"
"""ReAct agent class in agentscope."""
import asyncio
from typing import Type, Any, AsyncGenerator, Literal
import shortuuid
from pydantic import BaseModel, ValidationError, Field
from ._react_agent_base import ReActAgentBase
from .._logging import logger
from ..formatter import FormatterBase
from ..memory import MemoryBase, LongTermMemoryBase, InMemoryMemory
from ..message import Msg, ToolUseBlock, ToolResultBlock, TextBlock
from ..model import ChatModelBase
from ..rag import KnowledgeBase, Document
from ..plan import PlanNotebook
from ..tool import Toolkit, ToolResponse
from ..tracing import trace_reply
class _QueryRewriteModel(BaseModel):
"""The structured model used for query rewriting."""
rewritten_query: str = Field(
description=(
"The rewritten query, which should be specific and concise. "
),
)
def finish_function_pre_print_hook(
self: "ReActAgent",
kwargs: dict[str, Any],
) -> dict[str, Any] | None:
"""A pre-speak hook function that check if finish_function is called. If
so, it will wrap the response argument into a message and return it to
replace the original message. By this way, the calling of the finish
function will be displayed as a text reply instead of a tool call."""
msg = kwargs["msg"]
if isinstance(msg.content, str):
return None
if isinstance(msg.content, list):
for i, block in enumerate(msg.content):
if (
block["type"] == "tool_use"
and block["name"] == self.finish_function_name
):
# Convert the response argument into a text block for
# displaying
try:
msg.content[i] = TextBlock(
type="text",
text=block["input"].get("response", ""),
)
return kwargs
except Exception:
print("Error in block input", block["input"])
return None
class ReActAgent(ReActAgentBase):
"""A ReAct agent implementation in AgentScope, which supports
- Realtime steering
- API-based (parallel) tool calling
- Hooks around reasoning, acting, reply, observe and print functions
- Structured output generation
"""
finish_function_name: str = "generate_response"
"""The function name used to finish replying and return a response to
the user."""
def __init__(
self,
name: str,
sys_prompt: str,
model: ChatModelBase,
formatter: FormatterBase,
toolkit: Toolkit | None = None,
memory: MemoryBase | None = None,
long_term_memory: LongTermMemoryBase | None = None,
long_term_memory_mode: Literal[
"agent_control",
"static_control",
"both",
] = "both",
enable_meta_tool: bool = False,
parallel_tool_calls: bool = False,
knowledge: KnowledgeBase | list[KnowledgeBase] | None = None,
enable_rewrite_query: bool = True,
plan_notebook: PlanNotebook | None = None,
print_hint_msg: bool = False,
max_iters: int = 10,
) -> None:
"""Initialize the ReAct agent
Args:
name (`str`):
The name of the agent.
sys_prompt (`str`):
The system prompt of the agent.
model (`ChatModelBase`):
The chat model used by the agent.
formatter (`FormatterBase`):
The formatter used to format the messages into the required
format of the model API provider.
toolkit (`Toolkit | None`, optional):
A `Toolkit` object that contains the tool functions. If not
provided, a default empty `Toolkit` will be created.
memory (`MemoryBase | None`, optional):
The memory used to store the dialogue history. If not provided,
a default `InMemoryMemory` will be created, which stores
messages in a list in memory.
long_term_memory (`LongTermMemoryBase | None`, optional):
The optional long-term memory, which will provide two tool
functions: `retrieve_from_memory` and `record_to_memory`, and
will attach the retrieved information to the system prompt
before each reply.
enable_meta_tool (`bool`, defaults to `False`):
If `True`, a meta tool function `reset_equipped_tools` will be
added to the toolkit, which allows the agent to manage its
equipped tools dynamically.
long_term_memory_mode (`Literal['agent_control', 'static_control',\
'both']`, defaults to `both`):
The mode of the long-term memory. If `agent_control`, two
tool functions `retrieve_from_memory` and `record_to_memory`
will be registered in the toolkit to allow the agent to
manage the long-term memory. If `static_control`, retrieving
and recording will happen in the beginning and end of
each reply respectively.
parallel_tool_calls (`bool`, defaults to `False`):
When LLM generates multiple tool calls, whether to execute
them in parallel.
knowledge (`KnowledgeBase | list[KnowledgeBase] | None`, optional):
The knowledge object(s) used by the agent to retrieve
relevant documents at the beginning of each reply.
enable_rewrite_query (`bool`, defaults to `True`):
Whether ask the agent to rewrite the user input query before
retrieving from the knowledge base(s), e.g. rewrite "Who am I"
to "{user's name}" to get more relevant documents. Only works
when the knowledge base(s) is provided.
plan_notebook (`PlanNotebook | None`, optional):
The plan notebook instance, allow the agent to finish the
complex task by decomposing it into a sequence of subtasks.
print_hint_msg (`bool`, defaults to `False`):
Whether to print the hint messages, including the reasoning
hint from the plan notebook, the retrieved information from
the long-term memory and knowledge base(s).
max_iters (`int`, defaults to `10`):
The maximum number of iterations of the reasoning-acting loops.
"""
super().__init__()
assert long_term_memory_mode in [
"agent_control",
"static_control",
"both",
]
# Static variables in the agent
self.name = name
self._sys_prompt = sys_prompt
self.max_iters = max_iters
self.model = model
self.formatter = formatter
# -------------- Memory management --------------
# Record the dialogue history in the memory
self.memory = memory or InMemoryMemory()
# If provide the long-term memory, it will be used to retrieve info
# in the beginning of each reply, and the result will be added to the
# system prompt
self.long_term_memory = long_term_memory
# The long-term memory mode
self._static_control = long_term_memory and long_term_memory_mode in [
"static_control",
"both",
]
self._agent_control = long_term_memory and long_term_memory_mode in [
"agent_control",
"both",
]
# -------------- Tool management --------------
# If None, a default Toolkit will be created
self.toolkit = toolkit or Toolkit()
self.toolkit.register_tool_function(
getattr(self, self.finish_function_name),
)
if self._agent_control:
# Adding two tool functions into the toolkit to allow self-control
self.toolkit.register_tool_function(
long_term_memory.record_to_memory,
)
self.toolkit.register_tool_function(
long_term_memory.retrieve_from_memory,
)
# Add a meta tool function to allow agent-controlled tool management
if enable_meta_tool or plan_notebook:
self.toolkit.register_tool_function(
self.toolkit.reset_equipped_tools,
)
self.parallel_tool_calls = parallel_tool_calls
# -------------- RAG management --------------
# The knowledge base(s) used by the agent
if isinstance(knowledge, KnowledgeBase):
knowledge = [knowledge]
self.knowledge: list[KnowledgeBase] = knowledge or []
self.enable_rewrite_query = enable_rewrite_query
# -------------- Plan management --------------
# Equipped the plan-related tools provided by the plan notebook as
# a tool group named "plan_related". So that the agent can activate
# the plan tools by the meta tool function
self.plan_notebook = None
if plan_notebook:
self.plan_notebook = plan_notebook
self.toolkit.create_tool_group(
"plan_related",
description=self.plan_notebook.description,
)
for tool in plan_notebook.list_tools():
self.toolkit.register_tool_function(
tool,
group_name="plan_related",
)
# If print the reasoning hint messages
self.print_hint_msg = print_hint_msg
# The maximum number of iterations of the reasoning-acting loops
self.max_iters = max_iters
# The hint messages that will be attached to the prompt to guide the
# agent's behavior before each reasoning step, and cleared after
# each reasoning step, meaning the hint messages is one-time use only.
# We use an InMemoryMemory instance to store the hint messages
self._reasoning_hint_msgs = InMemoryMemory()
# Variables to record the intermediate state
# If required structured output model is provided
self._required_structured_model: Type[BaseModel] | None = None
# -------------- State registration and hooks --------------
# Register the status variables
self.register_state("name")
self.register_state("_sys_prompt")
self.register_instance_hook(
"pre_print",
"finish_function_pre_print_hook",
finish_function_pre_print_hook,
)
@property
def sys_prompt(self) -> str:
"""The dynamic system prompt of the agent."""
return self._sys_prompt
@trace_reply
async def reply(
self,
msg: Msg | list[Msg] | None = None,
structured_model: Type[BaseModel] | None = None,
) -> Msg:
"""Generate a reply based on the current state and input arguments.
Args:
msg (`Msg | list[Msg] | None`, optional):
The input message(s) to the agent.
structured_model (`Type[BaseModel] | None`, optional):
The required structured output model. If provided, the agent
is expected to generate structured output in the `metadata`
field of the output message.
Returns:
`Msg`:
The output message generated by the agent.
"""
# Record the input message(s) in the memory
await self.memory.add(msg)
# Retrieve relevant records from the long-term memory if activated
await self._retrieve_from_long_term_memory(msg)
# Retrieve relevant documents from the knowledge base(s) if any
await self._retrieve_from_knowledge(msg)
self._required_structured_model = structured_model
# Record structured output model if provided
if structured_model:
self.toolkit.set_extended_model(
self.finish_function_name,
structured_model,
)
# The reasoning-acting loop
reply_msg = None
for _ in range(self.max_iters):
msg_reasoning = await self._reasoning()
futures = [
self._acting(tool_call)
for tool_call in msg_reasoning.get_content_blocks(
"tool_use",
)
]
# Parallel tool calls or not
if self.parallel_tool_calls:
acting_responses = await asyncio.gather(*futures)
else:
# Sequential tool calls
acting_responses = [await _ for _ in futures]
# Find the first non-None replying message from the acting
for acting_msg in acting_responses:
reply_msg = reply_msg or acting_msg
if reply_msg:
break
# When the maximum iterations are reached
if reply_msg is None:
reply_msg = await self._summarizing()
# Post-process the memory, long-term memory
if self._static_control:
await self.long_term_memory.record(
[
*([*msg] if isinstance(msg, list) else [msg]),
*await self.memory.get_memory(),
reply_msg,
],
)
await self.memory.add(reply_msg)
return reply_msg
async def _reasoning(
self,
) -> Msg:
"""Perform the reasoning process."""
if self.plan_notebook:
# Insert the reasoning hint from the plan notebook
hint_msg = await self.plan_notebook.get_current_hint()
if self.print_hint_msg and hint_msg:
await self.print(hint_msg)
await self._reasoning_hint_msgs.add(hint_msg)
# Convert Msg objects into the required format of the model API
prompt = await self.formatter.format(
msgs=[
Msg("system", self.sys_prompt, "system"),
*await self.memory.get_memory(),
# The hint messages to guide the agent's behavior, maybe empty
*await self._reasoning_hint_msgs.get_memory(),
],
)
# Clear the hint messages after use
await self._reasoning_hint_msgs.clear()
res = await self.model(
prompt,
tools=self.toolkit.get_json_schemas(),
)
# handle output from the model
interrupted_by_user = False
msg = None
try:
if self.model.stream:
msg = Msg(self.name, [], "assistant")
async for content_chunk in res:
msg.content = content_chunk.content
await self.print(msg, False)
await self.print(msg, True)
# Add a tiny sleep to yield the last message object in the
# message queue
await asyncio.sleep(0.001)
else:
msg = Msg(self.name, list(res.content), "assistant")
await self.print(msg, True)
except asyncio.CancelledError as e:
interrupted_by_user = True
raise e from None
finally:
if msg and not msg.has_content_blocks("tool_use"):
# Turn plain text response into a tool call of the finish
# function
msg = Msg.from_dict(msg.to_dict())
msg.content = [
ToolUseBlock(
id=shortuuid.uuid(),
type="tool_use",
name=self.finish_function_name,
input={"response": msg.get_text_content()},
),
]
# None will be ignored by the memory
await self.memory.add(msg)
# Post-process for user interruption
if interrupted_by_user and msg:
# Fake tool results
tool_use_blocks: list = msg.get_content_blocks(
"tool_use",
)
for tool_call in tool_use_blocks:
msg_res = Msg(
"system",
[
ToolResultBlock(
type="tool_result",
id=tool_call["id"],
name=tool_call["name"],
output="The tool call has been interrupted "
"by the user.",
),
],
"system",
)
await self.memory.add(msg_res)
await self.print(msg_res, True)
return msg
async def _acting(self, tool_call: ToolUseBlock) -> Msg | None:
"""Perform the acting process.
Args:
tool_call (`ToolUseBlock`):
The tool use block to be executed.
Returns:
`Union[Msg, None]`:
Return a message to the user if the `finish_function` is
called, otherwise return `None`.
"""
tool_res_msg = Msg(
"system",
[
ToolResultBlock(
type="tool_result",
id=tool_call["id"],
name=tool_call["name"],
output=[],
),
],
"system",
)
try:
# Execute the tool call
tool_res = await self.toolkit.call_tool_function(tool_call)
response_msg = None
# Async generator handling
async for chunk in tool_res:
# Turn into a tool result block
tool_res_msg.content[0][ # type: ignore[index]
"output"
] = chunk.content
# Skip the printing of the finish function call
if (
tool_call["name"] != self.finish_function_name
or tool_call["name"] == self.finish_function_name
and (
chunk.metadata is None
or not chunk.metadata.get("success")
)
):
await self.print(tool_res_msg, chunk.is_last)
# Return message if generate_response is called successfully
if (
tool_call["name"] == self.finish_function_name
and chunk.metadata
and chunk.metadata.get(
"success",
True,
)
):
response_msg = chunk.metadata.get("response_msg")
return response_msg
finally:
# Record the tool result message in the memory
await self.memory.add(tool_res_msg)
async def observe(self, msg: Msg | list[Msg] | None) -> None:
"""Receive observing message(s) without generating a reply.
Args:
msg (`Msg | list[Msg] | None`):
The message or messages to be observed.
"""
await self.memory.add(msg)
async def _summarizing(self) -> Msg:
"""Generate a response when the agent fails to solve the problem in
the maximum iterations."""
hint_msg = Msg(
"user",
"You have failed to generate response within the maximum "
"iterations. Now respond directly by summarizing the current "
"situation.",
role="user",
)
# Generate a reply by summarizing the current situation
prompt = await self.formatter.format(
[
Msg("system", self.sys_prompt, "system"),
*await self.memory.get_memory(),
hint_msg,
],
)
# TODO: handle the structured output here, maybe force calling the
# finish_function here
res = await self.model(prompt)
res_msg = Msg(self.name, [], "assistant")
if isinstance(res, AsyncGenerator):
async for chunk in res:
res_msg.content = chunk.content
await self.print(res_msg, False)
await self.print(res_msg, True)
else:
res_msg.content = res.content
await self.print(res_msg, True)
return res_msg
async def handle_interrupt(
self,
_msg: Msg | list[Msg] | None = None,
) -> Msg:
"""The post-processing logic when the reply is interrupted by the
user or something else."""
response_msg = Msg(
self.name,
"I noticed that you have interrupted me. What can I "
"do for you?",
"assistant",
metadata={},
)
await self.print(response_msg, True)
await self.memory.add(response_msg)
return response_msg
def generate_response(
self,
response: str,
**kwargs: Any,
) -> ToolResponse:
"""Generate a response. Note only the input argument `response` is
visible to the others, you should include all the necessary
information in the `response` argument.
Args:
response (`str`):
Your response to the user.
"""
response_msg = Msg(
self.name,
response,
"assistant",
)
# Prepare structured output
if self._required_structured_model:
try:
# Use the metadata field of the message to store the
# structured output
response_msg.metadata = (
self._required_structured_model.model_validate(
kwargs,
).model_dump()
)
except ValidationError as e:
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Arguments Validation Error: {e}",
),
],
metadata={
"success": False,
"response_msg": None,
},
)
return ToolResponse(
content=[
TextBlock(
type="text",
text="Successfully generated response.",
),
],
metadata={
"success": True,
"response_msg": response_msg,
},
is_last=True,
)
async def _retrieve_from_long_term_memory(
self,
msg: Msg | list[Msg] | None,
) -> None:
"""Insert the retrieved information from the long-term memory into
the short-term memory as a Msg object.
Args:
msg (`Msg | list[Msg] | None`):
The input message to the agent.
"""
if self._static_control and msg:
# Retrieve information from the long-term memory if available
retrieved_info = await self.long_term_memory.retrieve(msg)
if retrieved_info:
retrieved_msg = Msg(
name="long_term_memory",
content="<long_term_memory>The content below are "
"retrieved from long-term memory, which maybe "
f"useful:\n{retrieved_info}</long_term_memory>",
role="user",
)
if self.print_hint_msg:
await self.print(retrieved_msg, True)
await self.memory.add(retrieved_msg)
async def _retrieve_from_knowledge(
self,
msg: Msg | list[Msg] | None,
) -> None:
"""Insert the retrieved documents from the RAG knowledge base(s) if
available.
Args:
msg (`Msg | list[Msg] | None`):
The input message to the agent.
"""
if self.knowledge and msg:
# Prepare the user input query
query = None
if isinstance(msg, Msg):
query = msg.get_text_content()
elif isinstance(msg, list):
query = "\n".join(_.get_text_content() for _ in msg)
# Skip if the query is empty
if not query:
return
# Rewrite the query by the LLM if enabled
if self.enable_rewrite_query:
try:
rewrite_prompt = await self.formatter.format(
msgs=[
Msg("system", self.sys_prompt, "system"),
*await self.memory.get_memory(),
Msg(
"user",
"<system-hint>Now you need to rewrite "
"the above user query to be more specific and "
"concise for knowledge retrieval. For "
"example, rewrite the query 'what happened "
"last day' to 'what happened on 2023-10-01' "
"(assuming today is 2023-10-02)."
"</system-hint>",
"user",
),
],
)
stream_tmp = self.model.stream
self.model.stream = False
res = await self.model(
rewrite_prompt,
structured_model=_QueryRewriteModel,
)
self.model.stream = stream_tmp
if res.metadata and res.metadata.get("rewritten_query"):
query = res.metadata["rewritten_query"]
except Exception as e:
logger.warning(
"Skipping the query rewriting due to error: %s",
str(e),
)
docs: list[Document] = []
for kb in self.knowledge:
# retrieve the user input query
docs.extend(
await kb.retrieve(query=query),
)
if docs:
# Rerank by the relevance score
docs = sorted(
docs,
key=lambda doc: doc.score or 0.0,
reverse=True,
)
# Prepare the retrieved knowledge string
retrieved_msg = Msg(
name="user",
content=[
TextBlock(
type="text",
text=(
"<retrieved_knowledge>Use the following "
"content from the knowledge base(s) if it's "
"helpful:\n"
),
),
*[_.metadata.content for _ in docs],
TextBlock(
type="text",
text="</retrieved_knowledge>",
),
],
role="user",
)
if self.print_hint_msg:
await self.print(retrieved_msg, True)
await self.memory.add(retrieved_msg)
---- _react_agent_base.py ----
# -*- coding: utf-8 -*-
"""The base class for ReAct agent in agentscope."""
from abc import abstractmethod
from collections import OrderedDict
from typing import Callable, Any
from ._agent_base import AgentBase
from ._agent_meta import _ReActAgentMeta
from ..message import Msg
class ReActAgentBase(AgentBase, metaclass=_ReActAgentMeta):
"""The ReAct agent base class.
To support ReAct algorithm, this class extends the AgentBase class by
adding two abstract interfaces: reasoning and acting, while supporting
hook functions at four positions: pre-reasoning, post-reasoning,
pre-acting, and post-acting by the `_ReActAgentMeta` metaclass.
"""
supported_hook_types: list[str] = [
"pre_reply",
"post_reply",
"pre_print",
"post_print",
"pre_observe",
"post_observe",
"pre_reasoning",
"post_reasoning",
"pre_acting",
"post_acting",
]
"""Supported hook types for the agent base class."""
_class_pre_reasoning_hooks: dict[
str,
Callable[
[
"ReActAgentBase", # self
dict[str, Any], # kwargs
],
dict[str, Any] | None, # The modified kwargs or None
],
] = OrderedDict()
"""The class-level pre-reasoning hooks, taking `self` object, the input
arguments as input"""
_class_post_reasoning_hooks: dict[
str,
Callable[
[
"ReActAgentBase", # self
dict[str, Any], # kwargs
Any, # output
],
Msg | None, # the modified output message or None
],
] = OrderedDict()
"""The class-level post-reasoning hooks, taking `self` object, the input
arguments and the output message as input, and return the modified output
message or None if no modification is needed."""
_class_pre_acting_hooks: dict[
str,
Callable[
[
"ReActAgentBase", # self
dict[str, Any], # kwargs
],
dict[str, Any] | None, # The modified kwargs or None
],
] = OrderedDict()
"""The class-level pre-acting hooks, taking `self` object, the input
arguments as input, and return the modified input arguments or None if no
modification is needed."""
_class_post_acting_hooks: dict[
str,
Callable[
[
"ReActAgentBase", # self
dict[str, Any], # kwargs
Any, # output
],
Msg | None, # the modified output message or None
],
] = OrderedDict()
"""The class-level post-acting hooks, taking `self` object, the input
arguments and the output message as input, and return the modified output
message or None if no modification is needed."""
def __init__(
self,
) -> None:
"""Initialize the ReAct agent base class."""
super().__init__()
# Init reasoning and acting hooks
self._instance_pre_reasoning_hooks = OrderedDict()
self._instance_post_reasoning_hooks = OrderedDict()
self._instance_pre_acting_hooks = OrderedDict()
self._instance_post_acting_hooks = OrderedDict()
@abstractmethod
async def _reasoning(
self,
*args: Any,
**kwargs: Any,
) -> Any:
"""The reasoning process of the ReAct agent, which will be wrapped
with pre- and post-hooks."""
@abstractmethod
async def _acting(self, *args: Any, **kwargs: Any) -> Any:
"""The acting process of the ReAct agent, which will be wrapped with
pre- and post-hooks."""
---- _user_agent.py ----
# -*- coding: utf-8 -*-
"""The user agent class."""
from typing import Type, Any
from pydantic import BaseModel
from ._agent_base import AgentBase
from ._user_input import UserInputBase, TerminalUserInput
from ..message import Msg
class UserAgent(AgentBase):
"""The class for user interaction, allowing developers to handle the user
input from different sources, such as web UI, cli, and other interfaces.
"""
_input_method: UserInputBase = TerminalUserInput()
"""The user input method, can be overridden by calling the
`register_instance/class_input_method` function."""
def __init__(
self,
name: str,
) -> None:
"""Initialize the user agent with a name."""
super().__init__()
self.name = name
async def reply(
self,
msg: Msg | list[Msg] | None = None,
structured_model: Type[BaseModel] | None = None,
) -> Msg:
"""Receive input message(s) and generate a reply message from the user.
Args:
msg (`Msg | list[Msg] | None`, defaults to `None`):
The message(s) to be replied. If `None`, the agent will wait
for user input.
structured_model (`Type[BaseModel] | None`, defaults to `None`):
A child class of `pydantic.BaseModel` that defines the
structured output format. If provided, the user will be
prompted to fill in the required fields.
Returns:
`Msg`:
The reply message generated by the user.
"""
# Get the input from the specified input method.
input_data = await self._input_method(
agent_id=self.id,
agent_name=self.name,
structured_model=structured_model,
)
blocks_input = input_data.blocks_input
if (
blocks_input
and len(blocks_input) == 1
and blocks_input[0].get("type") == "text"
):
# Turn blocks_input into a string if only one text block exists
blocks_input = blocks_input[0].get("text")
msg = Msg(
self.name,
content=blocks_input,
role="user",
metadata=input_data.structured_input,
)
await self.print(msg)
return msg
def override_instance_input_method(
self,
input_method: UserInputBase,
) -> None:
"""Override the input method of the current UserAgent instance.
Args:
input_method (`UserInputBase`):
The callable input method, which should be an object of a
class that inherits from `UserInputBase`.
"""
if not isinstance(input_method, UserInputBase):
raise ValueError(
f"The input method should be an instance of the child class "
f"of `UserInputBase`, but got {type(input_method)} instead.",
)
self._input_method = input_method
@classmethod
def override_class_input_method(
cls,
input_method: UserInputBase,
) -> None:
"""Override the input method of the current UserAgent class.
Args:
input_method (`UserInputBase`):
The callable input method, which should be an object of a
class that inherits from `UserInputBase`.
"""
if not isinstance(input_method, UserInputBase):
raise ValueError(
f"The input method should be an instance of the child class "
f"of `UserInputBase`, but got {type(input_method)} instead.",
)
cls._input_method = input_method
async def handle_interrupt(
self,
*args: Any,
**kwargs: Any,
) -> Msg:
"""The post-processing logic when the reply is interrupted by the
user or something else."""
raise NotImplementedError(
f"The handle_interrupt function is not implemented in "
f"{self.__class__.__name__}",
)
async def observe(self, msg: Msg | list[Msg] | None) -> None:
"""Observe the message(s) from the other agents or the environment."""
---- _user_input.py ----
# -*- coding: utf-8 -*-
"""The user input related classes."""
import json.decoder
import time
from abc import abstractmethod
from dataclasses import dataclass
from queue import Queue
from threading import Event
from typing import Any, Type, List
import jsonschema
import requests
import shortuuid
import socketio
from pydantic import BaseModel
import json5
from .. import _config
from .._logging import logger
from ..message import (
TextBlock,
VideoBlock,
AudioBlock,
ImageBlock,
)
@dataclass
class UserInputData:
"""The user input data."""
blocks_input: List[TextBlock | ImageBlock | AudioBlock | VideoBlock] = None
"""The text input from the user"""
structured_input: dict[str, Any] | None = None
"""The structured input from the user"""
class UserInputBase:
"""The base class used to handle the user input from different sources."""
@abstractmethod
async def __call__(
self,
agent_id: str,
agent_name: str,
*args: Any,
structured_model: Type[BaseModel] | None = None,
**kwargs: Any,
) -> UserInputData:
"""The user input method, which returns the user input and the
required structured data.
Args:
agent_id (`str`):
The agent identifier.
agent_name (`str`):
The agent name.
structured_model (`Type[BaseModel] | None`, optional):
A base model class that defines the structured input format.
Returns:
`UserInputData`:
The user input data.
"""
class TerminalUserInput(UserInputBase):
"""The terminal user input."""
def __init__(self, input_hint: str = "User Input: ") -> None:
"""Initialize the terminal user input with a hint."""
self.input_hint = input_hint
async def __call__(
self,
agent_id: str,
agent_name: str,
*args: Any,
structured_model: Type[BaseModel] | None = None,
**kwargs: Any,
) -> UserInputData:
"""Handle the user input from the terminal.
Args:
agent_id (`str`):
The agent identifier.
agent_name (`str`):
The agent name.
structured_model (`Type[BaseModel] | None`, optional):
A base model class that defines the structured input format.
Returns:
`UserInputData`:
The user input data.
"""
text_input = input(self.input_hint)
structured_input = None
if structured_model is not None:
structured_input = {}
json_schema = structured_model.model_json_schema()
required = json_schema.get("required", [])
print("Structured input (press Enter to skip for optional):)")
for key, item in json_schema.get("properties").items():
requirements = {**item}
requirements.pop("title")
while True:
res = input(f"\t{key} ({requirements}): ")
if res == "":
if key in required:
print(f"Key {key} is required.")
continue
res = item.get("default", None)
if item.get("type").lower() == "integer":
try:
res = json5.loads(res)
except json.decoder.JSONDecodeError as e:
print(
"\033[31mInvalid input with error:\n"
"```\n"
f"{e}\n"
"```\033[0m",
)
continue
try:
jsonschema.validate(res, item)
structured_input[key] = res
break
except jsonschema.ValidationError as e:
print(
f"\033[31mValidation error:\n```\n{e}\n```\033[0m",
)
time.sleep(0.5)
return UserInputData(
blocks_input=[TextBlock(type="text", text=text_input)],
structured_input=structured_input,
)
class StudioUserInput(UserInputBase):
"""The class that host the user input on the AgentScope Studio."""
_websocket_namespace: str = "/python"
def __init__(
self,
studio_url: str,
run_id: str,
max_retries: int = 3,
reconnect_attempts: int = 3,
reconnection_delay: int = 1,
reconnection_delay_max: int = 5,
) -> None:
"""Initialize the StudioUserInput object.
Args:
studio_url (`str`):
The URL of the AgentScope Studio.
run_id (`str`):
The current run identity.
max_retries (`int`, defaults to `3`):
The maximum number of retries to get user input.
"""
self._is_connected = False
self._is_reconnecting = False
self.studio_url = studio_url
self.run_id = run_id
self.max_retries = max_retries
# Init Websocket
self.sio = socketio.Client(
reconnection=True,
reconnection_attempts=reconnect_attempts,
reconnection_delay=reconnection_delay,
reconnection_delay_max=reconnection_delay_max,
)
self.input_queues = {}
self.input_events = {}
@self.sio.on("connect", namespace=self._websocket_namespace)
def on_connect() -> None:
self._is_connected = True
logger.info(
'Connected to AgentScope Studio at "%s" with '
'run name "%s".',
self.studio_url,
run_id,
)
logger.info(
"View the run at: %s/dashboard/projects/%s",
self.studio_url,
_config.project,
)
@self.sio.on("disconnect", namespace=self._websocket_namespace)
def on_disconnect() -> None:
self._is_connected = False
logger.info(
"Disconnected from AgentScope Studio at %s",
self.studio_url,
)
@self.sio.on("reconnect", namespace=self._websocket_namespace)
def on_reconnect(attempt_number: int) -> None:
self._is_connected = True
self._is_reconnecting = False
logger.info(
"Reconnected to AgentScope Studio at %s with run_id %s after "
"%d attempts",
self.studio_url,
self.run_id,
attempt_number,
)
@self.sio.on("reconnect_attempt", namespace=self._websocket_namespace)
def on_reconnect_attempt(attempt_number: int) -> None:
self._is_reconnecting = True
logger.info(
"Attempting to reconnect to AgentScope Studio at %s "
"(attempt %d)",
self.studio_url,
attempt_number,
)
@self.sio.on("reconnect_failed", namespace=self._websocket_namespace)
def on_reconnect_failed() -> None:
self._is_reconnecting = False
logger.error(
"Failed to reconnect to AgentScope Studio at %s",
self.studio_url,
)
@self.sio.on("reconnect_error", namespace=self._websocket_namespace)
def on_reconnect_error(error: Any) -> None:
logger.error(
"Error while reconnecting to AgentScope Studio at %s: %s",
self.studio_url,
str(error),
)
# The AgentScope Studio backend send the "sendUserInput" event to
# the current python run
@self.sio.on("forwardUserInput", namespace=self._websocket_namespace)
def receive_user_input(
request_id: str,
blocks_input: List[
TextBlock | ImageBlock | AudioBlock | VideoBlock
],
structured_input: dict[str, Any],
) -> None:
if request_id in self.input_queues:
self.input_queues[request_id].put(
UserInputData(
blocks_input=blocks_input,
structured_input=structured_input,
),
)
self.input_events[request_id].set()
try:
self.sio.connect(
f"{self.studio_url}",
namespaces=["/python"],
auth={"run_id": self.run_id},
)
except Exception as e:
raise RuntimeError(
f"Failed to connect to AgentScope Studio at {self.studio_url}",
) from e
def _ensure_connected(
self,
timeout: float = 30.0,
check_interval: float = 5.0,
) -> None:
"""Ensure the connection is established or wait for reconnection.
Args:
timeout (`float`):
Maximum time to wait for reconnection in seconds. Defaults
to 30.0.
check_interval (`float`):
Interval between connection checks in seconds. Defaults to 1.0.
Raises:
`RuntimeError`:
If connection cannot be established within timeout.
"""
if self._is_connected:
return
if self._is_reconnecting:
start_time = time.time()
while self._is_reconnecting:
# Check timeout
elapsed_time = time.time() - start_time
if elapsed_time > timeout:
raise RuntimeError(
f"Reconnection timeout after {elapsed_time} seconds",
)
# Log status
logger.info(
"Waiting for reconnection... (%.1fs / %.1fs)",
elapsed_time,
timeout,
)
# Wait for next check
time.sleep(check_interval)
# After reconnection attempt completed, check final status
if self._is_connected:
return
# Not connected and not reconnecting
raise RuntimeError(
f"Not connected to AgentScope Studio at {self.studio_url}.",
)
async def __call__( # type: ignore[override]
self,
agent_id: str,
agent_name: str,
*args: Any,
structured_model: Type[BaseModel] | None = None,
) -> UserInputData:
"""Get the user input from AgentScope Studio.
Args:
agent_id (`str`):
The identity of the agent.
agent_name (`str`):
The name of the agent.
structured_model (`Type[BaseModel] | None`, optional):
The base model class of the structured input.
Raises:
`RuntimeError`:
Failed to get user input from AgentScope Studio.
Returns:
`UserInputData`:
The user input.
"""
self._ensure_connected()
request_id = shortuuid.uuid()
self.input_queues[request_id] = Queue()
self.input_events[request_id] = Event()
if structured_model is None:
structured_input = None
else:
structured_input = structured_model.model_json_schema()
n_retry = 0
while True:
try:
response = requests.post(
f"{self.studio_url}/trpc/requestUserInput",
json={
"requestId": request_id,
"runId": self.run_id,
"agentId": agent_id,
"agentName": agent_name,
"structuredInput": structured_input,
},
)
response.raise_for_status()
break
except Exception as e:
if n_retry < self.max_retries:
n_retry += 1
continue
raise RuntimeError(
"Failed to get user input from AgentScope Studio",
) from e
try:
self.input_events[request_id].wait()
response_data = self.input_queues[request_id].get()
return response_data
finally:
self.input_queues.pop(request_id, None)
self.input_events.pop(request_id, None)
def __del__(self) -> None:
"""Cleanup socket connection when object it destroyed"""
try:
self.sio.disconnect()
except Exception as e:
logger.error(
"Failed to disconnect from AgentScope Studio at %s: %s",
self.studio_url,
str(e),
)
==== embedding ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The embedding module in agentscope."""
from ._embedding_base import EmbeddingModelBase
from ._embedding_usage import EmbeddingUsage
from ._embedding_response import EmbeddingResponse
from ._dashscope_embedding import DashScopeTextEmbedding
from ._dashscope_multimodal_embedding import DashScopeMultiModalEmbedding
from ._openai_embedding import OpenAITextEmbedding
from ._gemini_embedding import GeminiTextEmbedding
from ._ollama_embedding import OllamaTextEmbedding
from ._cache_base import EmbeddingCacheBase
from ._file_cache import FileEmbeddingCache
__all__ = [
"EmbeddingModelBase",
"EmbeddingUsage",
"EmbeddingResponse",
"DashScopeTextEmbedding",
"DashScopeMultiModalEmbedding",
"OpenAITextEmbedding",
"GeminiTextEmbedding",
"OllamaTextEmbedding",
"EmbeddingCacheBase",
"FileEmbeddingCache",
]
---- _cache_base.py ----
# -*- coding: utf-8 -*-
"""The embedding cache base class."""
from abc import abstractmethod
from typing import List, Any
from ..types import (
JSONSerializableObject,
Embedding,
)
class EmbeddingCacheBase:
"""Base class for embedding caches, which is responsible for storing and
retrieving embeddings."""
@abstractmethod
async def store(
self,
embeddings: List[Embedding],
identifier: JSONSerializableObject,
overwrite: bool = False,
**kwargs: Any,
) -> None:
"""Store the embeddings with the given identifier.
Args:
embeddings (`List[Embedding]`):
The embeddings to store.
identifier (`JSONSerializableObject`):
The identifier to distinguish the embeddings.
overwrite (`bool`, defaults to `False`):
Whether to overwrite existing embeddings with the same
identifier. If `True`, existing embeddings will be replaced.
"""
@abstractmethod
async def retrieve(
self,
identifier: JSONSerializableObject,
) -> List[Embedding] | None:
"""Retrieve the embeddings with the given identifier. If not
found, return `None`.
Args:
identifier (`JSONSerializableObject`):
The identifier to retrieve the embeddings.
"""
@abstractmethod
async def remove(
self,
identifier: JSONSerializableObject,
) -> None:
"""Remove the embeddings with the given identifier.
Args:
identifier (`JSONSerializableObject`):
The identifier to remove the embeddings.
"""
@abstractmethod
async def clear(self) -> None:
"""Clear all cached embeddings."""
---- _dashscope_embedding.py ----
# -*- coding: utf-8 -*-
"""The dashscope embedding module in agentscope."""
from datetime import datetime
from typing import Any, List, Literal
from ._cache_base import EmbeddingCacheBase
from ._embedding_response import EmbeddingResponse
from ._embedding_usage import EmbeddingUsage
from ._embedding_base import EmbeddingModelBase
from .._logging import logger
from ..message import TextBlock
class DashScopeTextEmbedding(EmbeddingModelBase):
"""DashScope text embedding API class.
.. note:: From the `official documentation
<https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2712515>`_:
- The max batch size that DashScope text embedding API
supports is 10 for `text-embedding-v4` and `text-embedding-v3` models, and
25 for `text-embedding-v2` and `text-embedding-v1` models.
- The max token limit for a single input is 8192 tokens for `v4` and `v3`
models, and 2048 tokens for `v2` and `v1` models.
"""
supported_modalities: list[str] = ["text"]
"""This class only supports text input."""
def __init__(
self,
api_key: str,
model_name: str,
dimensions: int = 1024,
embedding_cache: EmbeddingCacheBase | None = None,
) -> None:
"""Initialize the DashScope text embedding model class.
Args:
api_key (`str`):
The dashscope API key.
model_name (`str`):
The name of the embedding model.
dimensions (`int`, defaults to 1024):
The dimension of the embedding vector, refer to the
`official documentation
<https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2712515>`_
for more details.
embedding_cache (`EmbeddingCacheBase`):
The embedding cache class instance, used to cache the
embedding results to avoid repeated API calls.
"""
super().__init__(model_name, dimensions)
self.api_key = api_key
self.embedding_cache = embedding_cache
self.batch_size_limit = 10
async def _call_api(self, kwargs: dict[str, Any]) -> EmbeddingResponse:
"""Call the DashScope embedding API by the given keyword arguments."""
if self.embedding_cache:
cached_embeddings = await self.embedding_cache.retrieve(
identifier=kwargs,
)
if cached_embeddings:
return EmbeddingResponse(
embeddings=cached_embeddings,
usage=EmbeddingUsage(
tokens=0,
time=0,
),
source="cache",
)
import dashscope
start_time = datetime.now()
response = dashscope.embeddings.TextEmbedding.call(
api_key=self.api_key,
**kwargs,
)
time = (datetime.now() - start_time).total_seconds()
if response.status_code != 200:
raise RuntimeError(
f"Failed to get embedding from DashScope API: {response}",
)
if self.embedding_cache:
await self.embedding_cache.store(
identifier=kwargs,
embeddings=[
_["embedding"] for _ in response.output["embeddings"]
],
)
return EmbeddingResponse(
embeddings=[_["embedding"] for _ in response.output["embeddings"]],
usage=EmbeddingUsage(
tokens=response.usage["total_tokens"],
time=time,
),
)
async def __call__(
self,
text: List[str | TextBlock],
**kwargs: Any,
) -> EmbeddingResponse:
"""Call the DashScope embedding API.
Args:
text (`List[str | TextBlock]`):
The input text to be embedded. It can be a list of strings.
"""
gather_text = []
for _ in text:
if isinstance(_, dict) and "text" in _:
gather_text.append(_["text"])
elif isinstance(_, str):
gather_text.append(_)
else:
raise ValueError(
"Input text must be a list of strings or TextBlock dicts.",
)
if len(gather_text) > self.batch_size_limit:
logger.info(
"The input texts (%d) will be embedded with %d API calls due "
f"to the batch size limit of {self.batch_size_limit} for "
f"DashScope embedding API.",
len(gather_text),
(len(gather_text) + self.batch_size_limit - 1)
// self.batch_size_limit,
)
# Handle the batch size limit for DashScope embedding API
collected_embeddings = []
collected_time = 0.0
collected_tokens = 0
collected_source: Literal["cache", "api"] = "cache"
for _ in range(0, len(gather_text), self.batch_size_limit):
batch_texts = gather_text[_ : _ + self.batch_size_limit]
batch_kwargs = {
"input": batch_texts,
"model": self.model_name,
"dimension": self.dimensions,
**kwargs,
}
res = await self._call_api(batch_kwargs)
collected_embeddings.extend(res.embeddings)
collected_time += res.usage.time
if res.usage.tokens:
collected_tokens += res.usage.tokens
if res.source == "api":
collected_source = "api"
return EmbeddingResponse(
embeddings=collected_embeddings,
usage=EmbeddingUsage(
tokens=collected_tokens,
time=collected_time,
),
source=collected_source,
)
---- _dashscope_multimodal_embedding.py ----
# -*- coding: utf-8 -*-
"""The dashscope multimodal embedding model in agentscope."""
from datetime import datetime
from typing import Any, Literal
from ._cache_base import EmbeddingCacheBase
from ._embedding_response import EmbeddingResponse
from ._embedding_usage import EmbeddingUsage
from ._embedding_base import EmbeddingModelBase
from ..message import (
VideoBlock,
ImageBlock,
TextBlock,
)
class DashScopeMultiModalEmbedding(EmbeddingModelBase):
"""The DashScope multimodal embedding API, supporting text, image and
video embedding."""
supported_modalities: list[str] = ["text", "image", "video"]
"""This class supports text, image and video input."""
def __init__(
self,
api_key: str,
model_name: str,
dimensions: int | None = None,
embedding_cache: EmbeddingCacheBase | None = None,
) -> None:
"""Initialize the DashScope multimodal embedding model class.
Args:
api_key (`str`):
The dashscope API key.
model_name (`str`):
The name of the embedding model, e.g. "multimodal-embedding-
v1", "tongyi-embedding-vision-plus".
dimensions (`int`, defaults to 1024):
The dimension of the embedding vector, refer to the
`official documentation
<https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2712517>`_
for more details.
embedding_cache (`EmbeddingCacheBase`):
The embedding cache class instance, used to cache the
embedding results to avoid repeated API calls.
"""
path_doc = (
"https://bailian.console.aliyun.com/?tab=api#/api/?type=model&"
"url=2712517"
)
self.batch_size_limit = 1
if model_name.startswith("tongyi-embedding-vision-plus"):
self.batch_size_limit = 8
if dimensions is None:
dimensions = 1152
elif dimensions != 1152:
raise ValueError(
f"The dimension of model {model_name} must be 1152, "
"refer to the official documentation for more details: "
f"{path_doc}",
)
if model_name.startswith("tongyi-embedding-vision-flash"):
self.batch_size_limit = 8
if dimensions is None:
dimensions = 768
elif dimensions != 768:
raise ValueError(
f"The dimension of model {model_name} must be 768, "
"refer to the official documentation for more details: "
f"{path_doc}",
)
if model_name.startswith("multimodal-embedding-v"):
if dimensions is None:
dimensions = 1024
elif dimensions != 1024:
raise ValueError(
f"The dimension of model {model_name} must be 1024, "
"refer to the official documentation for more details: "
f"{path_doc}",
)
refined_dimensions: int = 1024
if dimensions is not None:
refined_dimensions = dimensions
super().__init__(model_name, refined_dimensions)
self.api_key = api_key
self.embedding_cache = embedding_cache
async def __call__(
self,
inputs: list[TextBlock | ImageBlock | VideoBlock],
**kwargs: Any,
) -> EmbeddingResponse:
"""Call the DashScope multimodal embedding API, which accepts text,
image, and video data.
Args:
inputs (`list[TextBlock | ImageBlock | VideoBlock]`):
The input data to be embedded. It can be a list of text,
image, and video blocks.
Returns:
`EmbeddingResponse`:
The embedding response object, which contains the embeddings
and usage information.
"""
# check data type
formatted_data = []
for _ in inputs:
if (
not isinstance(_, dict)
or "type" not in _
or _["type"]
not in [
"text",
"image",
"video",
]
):
raise ValueError(
f"Invalid data : {_}. It should be a list of "
"TextBlock, ImageBlock, or VideoBlock.",
)
if (
_["type"] == "video"
and _.get("source", {}).get("type") != "url"
):
raise ValueError(
f"The multimodal embedding API only supports URL input "
f"for video data, but got {_}.",
)
if _["type"] == "text":
assert "text" in _, (
f"Invalid text block: {_}. It should contain a "
f"'text' field.",
)
formatted_data.append({"text": _["text"]})
elif _["type"] == "video":
formatted_data.append({"video": _["source"]["url"]})
elif (
_["type"] == "image"
and "source" in _
and _["source"].get("type") in ["base64", "url"]
):
typ = _["source"]["type"]
if typ == "base64":
formatted_data.append(
{
"image": f'data:{_["source"]["media_type"]};'
f'base64,{_["source"]["data"]}',
},
)
elif typ == "url":
formatted_data.append(
{"image": _["source"]["url"]},
)
else:
raise ValueError(
f"Invalid block {_}. It should be a valid TextBlock, "
f"ImageBlock, or VideoBlock.",
)
# Handle the batch size limit of the DashScope multimodal embedding API
collected_embeddings = []
collected_time = 0.0
collected_tokens = 0
collected_source: Literal["cache", "api"] = "cache"
for _ in range(0, len(formatted_data), self.batch_size_limit):
batch_data = formatted_data[_ : _ + self.batch_size_limit]
batch_kwargs = {
"input": batch_data,
"model": self.model_name,
**kwargs,
}
res = await self._call_api(batch_kwargs)
collected_embeddings.extend(res.embeddings)
collected_time += res.usage.time
if res.usage.tokens:
collected_tokens += res.usage.tokens
if res.source == "api":
collected_source = "api"
return EmbeddingResponse(
embeddings=collected_embeddings,
usage=EmbeddingUsage(
tokens=collected_tokens,
time=collected_time,
),
source=collected_source,
)
async def _call_api(self, kwargs: dict[str, Any]) -> EmbeddingResponse:
"""
Call the DashScope multimodal embedding API by the given arguments.
"""
# Search in cache first
if self.embedding_cache:
cached_embeddings = await self.embedding_cache.retrieve(
identifier=kwargs,
)
if cached_embeddings:
return EmbeddingResponse(
embeddings=cached_embeddings,
usage=EmbeddingUsage(
tokens=0,
time=0,
),
source="cache",
)
import dashscope
kwargs["api_key"] = self.api_key
start_time = datetime.now()
res = dashscope.MultiModalEmbedding.call(**kwargs)
time = (datetime.now() - start_time).total_seconds()
if res.status_code != 200:
raise RuntimeError(
f"Failed to get embedding from DashScope API: {res}",
)
return EmbeddingResponse(
embeddings=[_["embedding"] for _ in res.output["embeddings"]],
usage=EmbeddingUsage(
tokens=res.usage.get(
"image_tokens",
0,
)
+ res.usage.get(
"input_tokens",
0,
),
time=time,
),
source="api",
)
---- _embedding_base.py ----
# -*- coding: utf-8 -*-
"""The embedding model base class."""
from typing import Any
from ._embedding_response import EmbeddingResponse
class EmbeddingModelBase:
"""Base class for embedding models."""
model_name: str
"""The embedding model name"""
supported_modalities: list[str]
"""The supported data modalities, e.g. "text", "image", "video"."""
dimensions: int
"""The dimensions of the embedding vector."""
def __init__(
self,
model_name: str,
dimensions: int,
) -> None:
"""Initialize the embedding model base class.
Args:
model_name (`str`):
The name of the embedding model.
dimensions (`int`):
The dimension of the embedding vector.
"""
self.model_name = model_name
self.dimensions = dimensions
async def __call__(
self,
*args: Any,
**kwargs: Any,
) -> EmbeddingResponse:
"""Call the embedding API with the given arguments."""
raise NotImplementedError(
f"The {self.__class__.__name__} class does not implement "
f"the __call__ method.",
)
---- _embedding_response.py ----
# -*- coding: utf-8 -*-
"""The embedding response class."""
from dataclasses import dataclass, field
from typing import Literal, List
from ._embedding_usage import EmbeddingUsage
from .._utils._common import _get_timestamp
from .._utils._mixin import DictMixin
from ..types import Embedding
@dataclass
class EmbeddingResponse(DictMixin):
"""The embedding response class."""
embeddings: List[Embedding]
"""The embedding data"""
id: str = field(default_factory=lambda: _get_timestamp(True))
"""The identity of the embedding response"""
created_at: str = field(default_factory=_get_timestamp)
"""The timestamp of the embedding response creation"""
type: Literal["embedding"] = field(default_factory=lambda: "embedding")
"""The type of the response, must be `embedding`."""
usage: EmbeddingUsage | None = field(default_factory=lambda: None)
"""The usage of the embedding model API invocation, if available."""
source: Literal["cache", "api"] = field(default_factory=lambda: "api")
"""If the response comes from the cache or the API."""
---- _embedding_usage.py ----
# -*- coding: utf-8 -*-
"""The embedding usage class in agentscope."""
from dataclasses import dataclass, field
from typing import Literal
from .._utils._mixin import DictMixin
@dataclass
class EmbeddingUsage(DictMixin):
"""The usage of an embedding model API invocation."""
time: float
"""The time used in seconds."""
tokens: int | None = field(default_factory=lambda: None)
"""The number of tokens used, if available."""
type: Literal["embedding"] = field(default_factory=lambda: "embedding")
"""The type of the usage, must be `embedding`."""
---- _file_cache.py ----
# -*- coding: utf-8 -*-
"""A file embedding cache implementation for storing and retrieving
embeddings in binary files."""
import hashlib
import json
import os
from typing import Any, List
import numpy as np
from ._cache_base import EmbeddingCacheBase
from .._logging import logger
from ..types import (
Embedding,
JSONSerializableObject,
)
class FileEmbeddingCache(EmbeddingCacheBase):
"""The embedding cache class that stores each embeddings vector in
binary files."""
def __init__(
self,
cache_dir: str = "./.cache/embeddings",
max_file_number: int | None = None,
max_cache_size: int | None = None,
) -> None:
"""Initialize the file embedding cache class.
Args:
cache_dir (`str`, defaults to `"./.cache/embeddings"`):
The directory to store the embedding files.
max_file_number (`int | None`, defaults to `None`):
The maximum number of files to keep in the cache directory. If
exceeded, the oldest files will be removed.
max_cache_size (`int | None`, defaults to `None`):
The maximum size of the cache directory in MB. If exceeded,
the oldest files will be removed until the size is within the
limit.
"""
self._cache_dir = os.path.abspath(cache_dir)
self.max_file_number = max_file_number
self.max_cache_size = max_cache_size
@property
def cache_dir(self) -> str:
"""The cache directory where the embedding files are stored."""
if not os.path.exists(self._cache_dir):
os.makedirs(self._cache_dir, exist_ok=True)
return self._cache_dir
async def store(
self,
embeddings: List[Embedding],
identifier: JSONSerializableObject,
overwrite: bool = False,
**kwargs: Any,
) -> None:
"""Store the embeddings with the given identifier.
Args:
embeddings (`List[Embedding]`):
The embeddings to store.
identifier (`JSONSerializableObject`):
The identifier to distinguish the embeddings, which will be
used to generate a hashable filename, so it should be
JSON serializable (e.g. a string, number, list, dict).
overwrite (`bool`, defaults to `False`):
Whether to overwrite existing embeddings with the same
identifier. If `True`, existing embeddings will be replaced.
"""
filename = self._get_filename(identifier)
path_file = os.path.join(self.cache_dir, filename)
if os.path.exists(path_file):
if not os.path.isfile(path_file):
raise RuntimeError(
f"Path {path_file} exists but is not a file.",
)
if overwrite:
np.save(path_file, embeddings)
await self._maintain_cache_dir()
else:
np.save(path_file, embeddings)
await self._maintain_cache_dir()
async def retrieve(
self,
identifier: JSONSerializableObject,
) -> List[Embedding] | None:
"""Retrieve the embeddings with the given identifier. If not found,
return `None`.
Args:
identifier (`JSONSerializableObject`):
The identifier to retrieve the embeddings, which will be
used to generate a hashable filename, so it should be
JSON serializable (e.g. a string, number, list, dict).
"""
filename = self._get_filename(identifier)
path_file = os.path.join(self.cache_dir, filename)
if os.path.exists(path_file):
return np.load(os.path.join(self.cache_dir, filename)).tolist()
return None
async def remove(self, identifier: JSONSerializableObject) -> None:
"""Remove the embeddings with the given identifier.
Args:
identifier (`JSONSerializableObject`):
The identifiers to remove the embeddings, which will be
used to generate a hashable filename, so it should be
JSON serializable (e.g. a string, number, list, dict).
"""
filename = self._get_filename(identifier)
path_file = os.path.join(self.cache_dir, filename)
if os.path.exists(path_file):
os.remove(path_file)
else:
raise FileNotFoundError(f"File {path_file} does not exist.")
async def clear(self) -> None:
"""Clear the cache directory by removing all files."""
for filename in os.listdir(self.cache_dir):
if filename.endswith(".npy"):
os.remove(os.path.join(self.cache_dir, filename))
def _get_cache_size(self) -> float:
"""Get the current size of the cache directory in MB."""
total_size = 0
for filename in os.listdir(self.cache_dir):
if filename.endswith(".npy"):
path_file = os.path.join(self.cache_dir, filename)
if os.path.isfile(path_file):
total_size += os.path.getsize(path_file)
return total_size / (1024.0 * 1024.0)
@staticmethod
def _get_filename(identifier: JSONSerializableObject) -> str:
"""Generate a filename based on the identifier."""
json_str = json.dumps(identifier, ensure_ascii=False)
return hashlib.sha256(json_str.encode("utf-8")).hexdigest() + ".npy"
async def _maintain_cache_dir(self) -> None:
"""Maintain the cache directory by removing old files if the number of
files exceeds the maximum limit or if the cache size exceeds the
maximum size."""
files = [
(_.name, _.stat().st_mtime)
for _ in os.scandir(self.cache_dir)
if _.is_file() and _.name.endswith(".npy")
]
files.sort(key=lambda x: x[1])
if self.max_file_number and len(files) > self.max_file_number:
for file_name, _ in files[: 0 - self.max_file_number]:
os.remove(os.path.join(self.cache_dir, file_name))
logger.info(
"Remove cached embedding file %s for limited number "
"of files (%d).",
file_name,
self.max_file_number,
)
files = files[0 - self.max_file_number :]
if (
self.max_cache_size is not None
and self._get_cache_size() > self.max_cache_size
):
removed_files = []
for filename, _ in files:
os.remove(os.path.join(self.cache_dir, filename))
removed_files.append(filename)
if self._get_cache_size() <= self.max_cache_size:
break
if removed_files:
logger.info(
"Remove %d cached embedding file(s) for limited "
"cache size (%d MB).",
len(removed_files),
self.max_cache_size,
)
---- _gemini_embedding.py ----
# -*- coding: utf-8 -*-
"""The gemini text embedding model class."""
from datetime import datetime
from typing import Any, List
from ._embedding_response import EmbeddingResponse
from ._embedding_usage import EmbeddingUsage
from ._cache_base import EmbeddingCacheBase
from ._embedding_base import EmbeddingModelBase
from ..message import TextBlock
class GeminiTextEmbedding(EmbeddingModelBase):
"""The Gemini text embedding model."""
supported_modalities: list[str] = ["text"]
"""This class only supports text input."""
def __init__(
self,
api_key: str,
model_name: str,
dimensions: int = 3072,
embedding_cache: EmbeddingCacheBase | None = None,
**kwargs: Any,
) -> None:
"""Initialize the Gemini text embedding model class.
Args:
api_key (`str`):
The Gemini API key.
model_name (`str`):
The name of the embedding model.
dimensions (`int`, defaults to 3072):
The dimension of the embedding vector, refer to the
`official documentation
<https://ai.google.dev/gemini-api/docs/embeddings?hl=zh-cn#control-embedding-size>`_
for more details.
embedding_cache (`EmbeddingCacheBase | None`, defaults to `None`):
The embedding cache class instance, used to cache the
embedding results to avoid repeated API calls.
"""
from google import genai
super().__init__(model_name, dimensions)
self.client = genai.Client(api_key=api_key, **kwargs)
self.embedding_cache = embedding_cache
async def __call__(
self,
text: List[str | TextBlock],
**kwargs: Any,
) -> EmbeddingResponse:
"""The Gemini embedding API call.
Args:
text (`List[str | TextBlock]`):
The input text to be embedded. It can be a list of strings.
# TODO: handle the batch size limit
"""
gather_text = []
for _ in text:
if isinstance(_, dict) and "text" in _:
gather_text.append(_["text"])
elif isinstance(_, str):
gather_text.append(_)
else:
raise ValueError(
"Input text must be a list of strings or TextBlock dicts.",
)
kwargs = {
"model": self.model_name,
"contents": gather_text,
"config": kwargs,
}
if self.embedding_cache:
cached_embeddings = await self.embedding_cache.retrieve(
identifier=kwargs,
)
if cached_embeddings:
return EmbeddingResponse(
embeddings=cached_embeddings,
usage=EmbeddingUsage(
tokens=0,
time=0,
),
source="cache",
)
start_time = datetime.now()
response = self.client.models.embed_content(**kwargs)
time = (datetime.now() - start_time).total_seconds()
if self.embedding_cache:
await self.embedding_cache.store(
identifier=kwargs,
embeddings=[_.values for _ in response.embeddings],
)
return EmbeddingResponse(
embeddings=[_.values for _ in response.embeddings],
usage=EmbeddingUsage(
time=time,
),
)
---- _ollama_embedding.py ----
# -*- coding: utf-8 -*-
"""The ollama text embedding model class."""
from datetime import datetime
from typing import List, Any
from ._embedding_response import EmbeddingResponse
from ._embedding_usage import EmbeddingUsage
from ._cache_base import EmbeddingCacheBase
from ..embedding import EmbeddingModelBase
from ..message import TextBlock
class OllamaTextEmbedding(EmbeddingModelBase):
"""The Ollama embedding model."""
supported_modalities: list[str] = ["text"]
"""This class only supports text input."""
def __init__(
self,
model_name: str,
dimensions: int,
host: str | None = None,
embedding_cache: EmbeddingCacheBase | None = None,
**kwargs: Any,
) -> None:
"""Initialize the Ollama text embedding model class.
Args:
model_name (`str`):
The name of the embedding model.
dimensions (`int`):
The dimension of the embedding vector, the parameter should be
provided according to the model used.
host (`str | None`, defaults to `None`):
The host URL for the Ollama API.
embedding_cache (`EmbeddingCacheBase | None`, defaults to `None`):
The embedding cache class instance, used to cache the
embedding results to avoid repeated API calls.
"""
import ollama
super().__init__(model_name, dimensions)
self.client = ollama.AsyncClient(host=host, **kwargs)
self.embedding_cache = embedding_cache
async def __call__(
self,
text: List[str | TextBlock],
**kwargs: Any,
) -> EmbeddingResponse:
"""Call the Ollama embedding API.
Args:
text (`List[str | TextBlock]`):
The input text to be embedded. It can be a list of strings.
"""
gather_text = []
for _ in text:
if isinstance(_, dict) and "text" in _:
gather_text.append(_["text"])
elif isinstance(_, str):
gather_text.append(_)
else:
raise ValueError(
"Input text must be a list of strings or TextBlock dicts.",
)
kwargs = {
"input": gather_text,
"model": self.model_name,
"dimensions": self.dimensions,
**kwargs,
}
if self.embedding_cache:
cached_embeddings = await self.embedding_cache.retrieve(
identifier=kwargs,
)
if cached_embeddings:
return EmbeddingResponse(
embeddings=cached_embeddings,
usage=EmbeddingUsage(
tokens=0,
time=0,
),
source="cache",
)
start_time = datetime.now()
response = await self.client.embed(**kwargs)
time = (datetime.now() - start_time).total_seconds()
if self.embedding_cache:
await self.embedding_cache.store(
identifier=kwargs,
embeddings=response.embeddings,
)
return EmbeddingResponse(
embeddings=response.embeddings,
usage=EmbeddingUsage(
time=time,
),
)
---- _openai_embedding.py ----
# -*- coding: utf-8 -*-
"""The OpenAI text embedding model class."""
from datetime import datetime
from typing import Any, List
from ._embedding_response import EmbeddingResponse
from ._embedding_usage import EmbeddingUsage
from ._cache_base import EmbeddingCacheBase
from ._embedding_base import EmbeddingModelBase
from ..message import TextBlock
class OpenAITextEmbedding(EmbeddingModelBase):
"""OpenAI text embedding model class."""
supported_modalities: list[str] = ["text"]
"""This class only supports text input."""
def __init__(
self,
api_key: str,
model_name: str,
dimensions: int = 1024,
embedding_cache: EmbeddingCacheBase | None = None,
**kwargs: Any,
) -> None:
"""Initialize the OpenAI text embedding model class.
Args:
api_key (`str`):
The OpenAI API key.
model_name (`str`):
The name of the embedding model.
dimensions (`int`, defaults to 1024):
The dimension of the embedding vector.
embedding_cache (`EmbeddingCacheBase | None`, defaults to `None`):
The embedding cache class instance, used to cache the
embedding results to avoid repeated API calls.
# TODO: handle batch size limit and token limit
"""
import openai
super().__init__(model_name, dimensions)
self.client = openai.AsyncClient(api_key=api_key, **kwargs)
self.embedding_cache = embedding_cache
async def __call__(
self,
text: List[str | TextBlock],
**kwargs: Any,
) -> EmbeddingResponse:
"""Call the OpenAI embedding API.
Args:
text (`List[str | TextBlock]`):
The input text to be embedded. It can be a list of strings.
"""
gather_text = []
for _ in text:
if isinstance(_, dict) and "text" in _:
gather_text.append(_["text"])
elif isinstance(_, str):
gather_text.append(_)
else:
raise ValueError(
"Input text must be a list of strings or TextBlock dicts.",
)
kwargs = {
"input": gather_text,
"model": self.model_name,
"dimensions": self.dimensions,
"encoding_format": "float",
**kwargs,
}
if self.embedding_cache:
cached_embeddings = await self.embedding_cache.retrieve(
identifier=kwargs,
)
if cached_embeddings:
return EmbeddingResponse(
embeddings=cached_embeddings,
usage=EmbeddingUsage(
tokens=0,
time=0,
),
source="cache",
)
start_time = datetime.now()
response = await self.client.embeddings.create(**kwargs)
time = (datetime.now() - start_time).total_seconds()
if self.embedding_cache:
await self.embedding_cache.store(
identifier=kwargs,
embeddings=[_.embedding for _ in response.data],
)
return EmbeddingResponse(
embeddings=[_.embedding for _ in response.data],
usage=EmbeddingUsage(
tokens=response.usage.total_tokens,
time=time,
),
)
==== evaluate ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The evaluation module in AgentScope."""
from ._evaluator import (
EvaluatorBase,
RayEvaluator,
GeneralEvaluator,
)
from ._metric_base import (
MetricBase,
MetricResult,
MetricType,
)
from ._task import Task
from ._solution import SolutionOutput
from ._benchmark_base import BenchmarkBase
from ._evaluator_storage import (
EvaluatorStorageBase,
FileEvaluatorStorage,
)
from ._ace_benchmark import (
ACEBenchmark,
ACEAccuracy,
ACEProcessAccuracy,
ACEPhone,
)
__all__ = [
"BenchmarkBase",
"EvaluatorBase",
"RayEvaluator",
"GeneralEvaluator",
"MetricBase",
"MetricResult",
"MetricType",
"EvaluatorStorageBase",
"FileEvaluatorStorage",
"Task",
"SolutionOutput",
"ACEBenchmark",
"ACEAccuracy",
"ACEProcessAccuracy",
"ACEPhone",
]
---- _benchmark_base.py ----
# -*- coding: utf-8 -*-
"""The base class for benchmark evaluation."""
from abc import ABC, abstractmethod
from typing import Generator
from ._task import Task
class BenchmarkBase(ABC):
"""The base class for benchmark evaluation."""
name: str
"""The name of the benchmark."""
description: str
"""The description of the benchmark."""
def __init__(self, name: str, description: str) -> None:
"""Initialize the benchmark.
Args:
name (`str`):
The name of the benchmark.
description (`str`):
A brief description of the benchmark.
"""
self.name = name
self.description = description
@abstractmethod
def __iter__(self) -> Generator[Task, None, None]:
"""Iterate over the benchmark."""
raise NotImplementedError("Subclasses must implement this method.")
@abstractmethod
def __len__(self) -> int:
"""Get the length of the benchmark."""
raise NotImplementedError("Subclasses must implement this method.")
@abstractmethod
def __getitem__(self, index: int) -> Task:
"""Get the task at the given index."""
raise NotImplementedError("Subclasses must implement this method.")
---- _metric_base.py ----
# -*- coding: utf-8 -*-
"""The base class for _metric in evaluation."""
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Any
from .._utils._common import _get_timestamp
from .._utils._mixin import DictMixin
from ..types import JSONSerializableObject
@dataclass
class MetricResult(DictMixin):
"""The result of a _metric."""
name: str
"""The metric name."""
result: str | float | int
"""The metric result."""
created_at: str = field(default_factory=_get_timestamp)
"""The timestamp when the metric result was created."""
message: str | None = field(default_factory=lambda: None)
"""An optional message for the metric result, can be used to provide
additional information or context about the result."""
metadata: dict[str, JSONSerializableObject] | None = field(default=None)
"""Optional metadata for the metric result, can be used to store
additional information related to the metric result."""
class MetricType(str, Enum):
"""The metric type enum."""
CATEGORY = "category"
"""The metric result is a category, e.g. "pass" or "fail"."""
NUMERICAL = "numerical"
"""The metric result is a numerical value, e.g. 0.95 or 100."""
class MetricBase(ABC):
"""The base class for _metric in evaluation."""
def __init__(
self,
name: str,
metric_type: MetricType,
description: str | None = None,
categories: list[str] | None = None,
) -> None:
"""Initialize the _metric object.
Args:
name (`str`):
The name of the metric.
metric_type (`MetricType`):
The type of the metric, can be either "category" or
"numerical", which will determine how to display the result.
description (`str`):
The description of the metric.
categories (`list[str] | None`, optional):
The candidate categories. If `metric_type` is "category", the
categories must be provided, otherwise it should be `None`.
"""
self.name = name
self.metric_type = metric_type
self.description = description
if metric_type == MetricType.CATEGORY and categories is None:
raise ValueError(
"Categories must be provided for category metrics.",
)
self.categories = categories
@abstractmethod
async def __call__(
self,
*args: Any,
**kwargs: Any,
) -> MetricResult:
"""The call function to calculate the _metric result"""
---- _solution.py ----
# -*- coding: utf-8 -*-
"""Solution class for evaluation tasks."""
from dataclasses import dataclass, field
from typing import Any
from ..message import (
ToolResultBlock,
ToolUseBlock,
TextBlock,
)
from ..types._json import JSONSerializableObject
from .._utils._mixin import DictMixin
@dataclass
class SolutionOutput(DictMixin):
"""The output of a solution in evaluation task"""
success: bool
"""Indicates whether the solution is executed successfully. When the
solution raise exception, this should be set to False."""
output: JSONSerializableObject
"""The final output of the solution."""
trajectory: list[ToolUseBlock | ToolResultBlock | TextBlock]
"""The tool calls and results trajectory"""
meta: dict[str, Any] | None = field(default_factory=lambda: None)
"""Additional metadata for the solution"""
def __getstate__(self) -> dict[str, Any]:
"""Custom pickling to handle dataclass + DictMixin inheritance."""
return self.__dict__.copy()
def __setstate__(self, state: dict[str, Any]) -> None:
"""Custom unpickling to handle dataclass + DictMixin inheritance."""
self.__dict__.update(state)
---- _task.py ----
# -*- coding: utf-8 -*-
"""The base class for task in evaluation."""
from dataclasses import dataclass, field
from typing import Any
from ._solution import SolutionOutput
from ._metric_base import MetricBase, MetricResult
from ..types._json import JSONSerializableObject
@dataclass
class Task:
"""The base class for task in evaluation."""
id: str
"""The unique identifier for the task."""
input: JSONSerializableObject
"""The task input, which should be a JSON serializable object."""
ground_truth: JSONSerializableObject
"""The task ground truth if exists, which should be a JSON serializable
object."""
metrics: list[MetricBase]
"""The metrics to evaluate the task, which should be a list of
`MetricBase` objects."""
tags: dict[str, str] | None = field(default_factory=lambda: None)
"""Tags to categorize the task, e.g. `{"difficulty": "easy",
"cate": "math"}`."""
metadata: dict[str, Any] | None = field(
default_factory=lambda: None,
)
"""Additional metadata for the task."""
async def evaluate(self, solution: SolutionOutput) -> list[MetricResult]:
"""Evaluate the task with the given solution.
Args:
solution (`SolutionOutput`):
The solution to evaluate the task with.
Returns:
`MetricResult`:
The result of the evaluation.
"""
evaluations = []
for metric in self.metrics:
result = await metric(solution)
evaluations.append(result)
return evaluations
==== _ace_benchmark ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The ACE benchmark related implementations in AgentScope."""
from ._ace_benchmark import ACEBenchmark
from ._ace_metric import (
ACEAccuracy,
ACEProcessAccuracy,
)
from ._ace_tools_zh import ACEPhone
__all__ = [
"ACEBenchmark",
"ACEPhone",
"ACEAccuracy",
"ACEProcessAccuracy",
]
---- _ace_benchmark.py ----
# -*- coding: utf-8 -*-
"""The ACE benchmark class in agentscope. The code is implemented with
reference to the `ACEBench <https://github.com/ACEBench/ACEBench>`_
under the MIT license."""
import json
import os
from typing import Generator
import json5
import requests
from tqdm import tqdm
from ._ace_metric import ACEAccuracy, ACEProcessAccuracy
from ._ace_tools_zh import ACEPhone
from .._benchmark_base import BenchmarkBase
from .._task import Task
class ACEBenchmark(BenchmarkBase):
"""The ACE benchmark for evaluating AI agents."""
data_dir_url: str = (
"https://raw.githubusercontent.com/ACEBench/ACEBench/main/data_all"
)
"""The URL to the data dir"""
data_subdir: list[str] = [
# "data_en", # TODO: enable English version
"data_zh",
]
ground_truth_dir: str = "possible_answer"
data_files: list[str] = [
"data_agent_multi_step.json",
"data_agent_multi_turn.json",
# "data_normal_atom_bool.json",
# "data_normal_atom_enum.json",
# "data_normal_atom_list.json",
# "data_normal_atom_number.json",
# "data_normal_atom_object_deep.json",
# "data_normal_atom_object_short.json",
#
# "data_normal_multi_turn_user_adjust.json",
# "data_normal_multi_turn_user_switch.json",
#
# "data_normal_preference.json",
# "data_normal_similar_api.json",
# "data_normal_single_turn_parallel_function.json",
# "data_normal_single_turn_single_function.json",
#
# "data_special_error_param.json",
# "data_special_incomplete.json",
# "data_special_irrelevant.json",
]
"""The data filenames"""
def __init__(
self,
data_dir: str,
) -> None:
"""Initialize the ACEBenchmark
Args:
data_dir (`str`):
The directory where the dataset is downloaded and saved.
"""
super().__init__(
name="ACEBench",
description="The ACE benchmark for evaluating AI agents.",
)
self.data_dir = os.path.abspath(data_dir)
if os.path.exists(data_dir) and not os.path.isdir(data_dir):
raise RuntimeError(
f"The data_dir `{data_dir}` is not a valid directory path.",
)
os.makedirs(data_dir, exist_ok=True)
if not self._verify_data():
self._download_data()
self.dataset = self._load_data()
def _load_data(self) -> list[dict]:
"""Load the dataset from the data directory."""
dataset = []
for subdir in self.data_subdir:
for filename in self.data_files:
file_path = os.path.join(self.data_dir, subdir, filename)
gt_path = os.path.join(
self.data_dir,
subdir,
self.ground_truth_dir,
filename,
)
gt_dataset = {}
with open(gt_path, "r", encoding="utf-8") as gt_file:
for line in gt_file:
gt_data = json5.loads(line)
gt_dataset[gt_data["id"]] = gt_data
with open(file_path, "r", encoding="utf-8") as f:
for line in f:
data = json5.loads(line)
gt = gt_dataset[data["id"]]
gt.pop("id", None)
data["ground_truth"] = gt["ground_truth"]
data["mile_stone"] = gt["mile_stone"]
data["language"] = subdir.rsplit(
"_",
maxsplit=1,
)[-1]
data["tags"] = {
"language": data["language"],
"category": filename.split(
".",
maxsplit=1,
)[0].removeprefix(
"data_",
),
}
dataset.append(data)
return dataset
def _verify_data(self) -> bool:
"""Verify the data completeness and integrity."""
for subdir in self.data_subdir:
for filename in self.data_files:
file_path = os.path.join(self.data_dir, subdir, filename)
if not os.path.exists(file_path):
return False
gt_path = os.path.join(
self.data_dir,
subdir,
self.ground_truth_dir,
filename,
)
if not os.path.exists(gt_path):
return False
return True
def _download_data(self) -> None:
"""Download the data from the URL"""
for subdir in self.data_subdir:
subdir_path = os.path.join(self.data_dir, subdir)
subdir_gt_path = os.path.join(subdir_path, self.ground_truth_dir)
os.makedirs(subdir_path, exist_ok=True)
os.makedirs(subdir_gt_path, exist_ok=True)
for filename in tqdm(
self.data_files,
desc=f"Downloading {subdir}",
):
response = requests.get(
f"{self.data_dir_url}/{subdir}/{filename}",
)
response.raise_for_status()
with open(os.path.join(subdir_path, filename), "wb") as f:
f.write(response.content)
gt_response = requests.get(
f"{self.data_dir_url}/{subdir}/"
f"{self.ground_truth_dir}/{filename}",
)
gt_response.raise_for_status()
with open(os.path.join(subdir_gt_path, filename), "wb") as f:
f.write(gt_response.content)
@staticmethod
def _data_to_task(item: dict) -> Task:
"""Convert a dataset item to a Task object."""
# Start the simulated phone and load initial configuration
ace_phone = ACEPhone()
ace_phone.load_initial_config(item["initial_config"])
# Obtain tool functions
tools: list[tuple] = []
for function_schema in item["function"]:
name = function_schema["name"]
# Handle the schema differences
formatted_schema = json.loads(
json.dumps(
function_schema,
).replace(
'"type": "dict"',
'"type": "object"',
),
)
tool_function = ace_phone.get_tool_function(name)
tools.append(
(
tool_function,
{
"type": "function",
"function": formatted_schema,
},
),
)
return Task(
id=item["id"],
input=item["question"],
ground_truth={
"state": item["ground_truth"],
"mile_stone": item.get("mile_stone", []),
},
tags=item.get("tags", {}),
metrics=[
ACEAccuracy(item["ground_truth"]),
ACEProcessAccuracy(item["mile_stone"]),
],
metadata={
# The phone is used to extract the final state after finishing
# the task.
"phone": ace_phone,
# The provided tools for this task, used to equip the agent
"tools": tools,
},
)
def __iter__(self) -> Generator[Task, None, None]:
"""Iterate over the benchmark."""
for item in self.dataset:
yield self._data_to_task(item)
def __getitem__(self, index: int) -> Task:
"""Get a task by index."""
return self._data_to_task(self.dataset[index])
def __len__(self) -> int:
"""Get the length of the benchmark."""
return len(self.dataset)
---- _ace_metric.py ----
# -*- coding: utf-8 -*-
"""The ACE benchmark metric implementations in AgentScope."""
from .._solution import SolutionOutput
from .._metric_base import MetricBase, MetricResult, MetricType
class ACEProcessAccuracy(MetricBase):
"""The ace benchmark process accuracy metric."""
def __init__(
self,
mile_stone: list[str],
) -> None:
"""Initialize the AceBench process accuracy metric."""
super().__init__(
name="process_accuracy",
metric_type=MetricType.NUMERICAL,
description="The AceBench Agent eval process accuracy metric.",
)
self.mile_stone = mile_stone
async def __call__(
self,
solution: SolutionOutput,
) -> MetricResult:
"""Calculate the metric result."""
# Turn the tool use block sequence into ACEBench format
# e.g. func(arg1='dfd', arg2=44)
gathered_trajectory = []
for tool_call in solution.trajectory:
if tool_call.get("type") == "tool_use":
function_name = tool_call.get("name")
kwargs = tool_call.get("input")
gathered_kwargs = []
for key, value in kwargs.items():
if isinstance(value, str):
gathered_kwargs.append(
f"{key}='{value}'",
)
else:
gathered_kwargs.append(
f"{key}={value}",
)
kwargs_str = ", ".join(gathered_kwargs)
gathered_trajectory.append(
f"[{function_name}({kwargs_str})]",
)
for stone in self.mile_stone:
if stone not in gathered_trajectory:
return MetricResult(
name=self.name,
result=0,
message=f"Error: Missing milestone '{stone}' in "
"the given trajectory.",
)
return MetricResult(
name=self.name,
result=1,
message="Success",
)
class ACEAccuracy(MetricBase):
"""The ace benchmark metric"""
def __init__(
self,
state: list[dict],
) -> None:
"""Initialize the _metric object."""
super().__init__(
"accuracy",
MetricType.NUMERICAL,
"The AceBench Agent eval accuracy metric.",
)
self.state = state
async def __call__(
self,
solution: SolutionOutput,
) -> MetricResult:
"""Calculate the metric result."""
# Check if the solution matches the ground truth
if not isinstance(solution.output, list):
raise ValueError("Ground truth state must be a list.")
# Handle the typos in ACEBench dataset
gathered_state = {}
for item in self.state:
for key, value in item.items():
if key.endswith("API"):
key = key.replace("API", "Api")
elif key.endswith("rpi"):
key = key.replace("pi", "Api")
gathered_state[key] = value
gathered_output = {}
for item in solution.output:
for key, value in item.items():
gathered_output[key] = value
if not set(gathered_state.keys()).issubset(gathered_output.keys()):
raise ValueError(
"Missing keys in solution output compared to state, "
f"ground truth keys: {gathered_state.keys()}, "
f"solution keys: {gathered_output.keys()}",
)
for key, value in gathered_state.items():
if value != gathered_output.get(key):
return MetricResult(
name=self.name,
result=0,
message=(
f"Error: Mismatch in key '{key}':"
f"\n{value}\n{gathered_output.get(key)}"
),
)
return MetricResult(
name=self.name,
result=1,
message="Success: All keys match",
)
---- _ace_tools_zh.py ----
# -*- coding: utf-8 -*-
"""The Chinese tools for ACEBench evaluation."""
from functools import wraps
from typing import Callable, Any
from ._ace_tools_api import (
ReminderApi,
FoodPlatformApi,
TravelApi,
MessageApi,
)
from ...message import TextBlock
from ...tool import ToolResponse
def _tool_function_wrapper(get_tool_function: Callable) -> Callable:
"""Wrap the tool function result to be ToolResponse."""
@wraps(get_tool_function)
def wrapper(self: "ACEPhone", name: str) -> Callable:
"""Wrap the tool function to return ToolResponse."""
tool_function = get_tool_function(self, name)
@wraps(tool_function)
def wrapper_tool_function(*args: Any, **kwargs: Any) -> ToolResponse:
"""The wrapped tool function"""
res = tool_function(*args, **kwargs)
return ToolResponse(
content=[
TextBlock(
type="text",
text=str(res),
),
],
)
return wrapper_tool_function
return wrapper
class ACEPhone:
"""Simulate a user phone with various apps and functionalities in
ACEBench. The code is implemented with reference to the
`ACEBench <https://github.com/ACEBench/ACEBench>`_.
"""
def __init__(self) -> None:
"""Initialize the shared state and apps for the ACEPhone."""
self._state = {
"wifi": False,
"logged_in": False,
}
self._message_app = MessageApi(self._state)
self._reminder_app = ReminderApi(self._state)
self._food_platform_app = FoodPlatformApi(self._state)
self._travel = TravelApi()
def turn_on_wifi(self) -> dict[str, bool | str]:
"""开启WiFi连接。"""
self._state["wifi"] = True
return {"status": True, "message": "wifi已经打开"}
def login_device(self) -> dict[str, bool | str]:
"""登录设备。"""
self._state["logged_in"] = True
return {"status": True, "message": "设备已经登录"}
def load_initial_config(self, initial_config: dict) -> None:
"""Load the initial config from the application configuration."""
# Empty initial config
if len(initial_config) == 0:
return
# Fix the typo in ACEBench by renaming "Baspi" to "BaseApi"
if "Baspi" in initial_config:
initial_config["BaseApi"] = initial_config.pop("Baspi")
# Verify state
assert (
"BaseApi" in initial_config
and "wifi" in initial_config["BaseApi"]
and "logged_in" in initial_config["BaseApi"]
), f"Invalid initial config: {initial_config}"
self._state["wifi"] = initial_config["BaseApi"]["wifi"]
self._state["logged_in"] = initial_config["BaseApi"]["logged_in"]
def get_current_state(self) -> list[dict]:
"""Follow ACEBench to get the current state of the ACEPhone."""
return [
{"BaseApi": self._state},
self._message_app.get_state_dict(),
self._reminder_app.get_state_dict(),
self._food_platform_app.get_state_dict(),
self._travel.get_state_dict(),
]
@_tool_function_wrapper
def get_tool_function(self, name: str) -> Callable:
"""Get a tool function by name."""
if name in [
"turn_on_wifi",
"login_device",
]:
return getattr(self, name)
if name in self._message_app.tool_functions:
return getattr(self._message_app, name)
if name in self._food_platform_app.tool_functions:
return getattr(self._food_platform_app, name)
if name in self._reminder_app.tool_functions:
return getattr(self._reminder_app, name)
if name in self._travel.tool_functions:
return getattr(self._travel, name)
raise ValueError(
f"Tool function '{name}' not found in ACEPhone.",
)
==== _ace_tools_api ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The ACEBench simulation tools in AgentScope."""
from ._message_api import MessageApi
from ._travel_api import TravelApi
from ._reminder_api import ReminderApi
from ._food_platform_api import FoodPlatformApi
__all__ = [
"MessageApi",
"TravelApi",
"ReminderApi",
"FoodPlatformApi",
]
---- _food_platform_api.py ----
# -*- coding: utf-8 -*-
"""The food platform API in the ACEBench evaluation."""
from ._shared_state import SharedState
class FoodPlatformApi(SharedState):
"""The food platform Api in the ACEBench evaluation."""
tool_functions: list[str] = [
"login_food_platform",
"view_logged_in_users",
"check_balance",
"add_food_delivery_order",
"get_products",
"view_orders",
"search_orders",
]
def __init__(self, shared_state: dict) -> None:
super().__init__(shared_state)
# 设置用户和初始金额
self.users: dict = {
"Eve": {
"user_id": "U100",
"password": "password123",
"balance": 500.0,
},
"Frank": {
"user_id": "U101",
"password": "password456",
"balance": 300.0,
},
"Grace": {
"user_id": "U102",
"password": "password789",
"balance": 150.0,
},
"Helen": {
"user_id": "U103",
"password": "password321",
"balance": 800.0,
},
"Isaac": {
"user_id": "U104",
"password": "password654",
"balance": 400.0,
},
"Jack": {
"user_id": "U105",
"password": "password654",
"balance": 120.0,
},
}
# 设置六个商家及其菜单
self.merchant_list: dict[str, dict] = {
"达美乐": {
"merchant_id": "M100",
"service_type": "Pizza",
"menu": [
{"product": "玛格丽特披萨", "price": 68.0},
{"product": "超级至尊披萨", "price": 88.0},
],
},
"米村拌饭": {
"merchant_id": "M101",
"service_type": "Bibimbap",
"menu": [
{"product": "石锅拌饭", "price": 35.0},
{"product": "韩式牛肉拌饭", "price": 45.0},
],
},
"海底捞": {
"merchant_id": "M102",
"service_type": "Hotpot",
"menu": [
{"product": "牛肉卷", "price": 68.0},
{"product": "海鲜拼盘", "price": 88.0},
],
},
"喜茶": {
"merchant_id": "M103",
"service_type": "Milk Tea",
"menu": [
{"product": "芝士奶茶", "price": 25.0},
{"product": "四季春奶茶", "price": 22.0},
],
},
"盒马生鲜": {
"merchant_id": "M104",
"service_type": "Fresh Grocery",
"menu": [
{"product": "有机蔬菜包", "price": 15.0},
{"product": "生鲜大礼包", "price": 99.0},
],
},
"九田家烤肉": {
"merchant_id": "M105",
"service_type": "BBQ",
"menu": [
{"product": "韩式烤牛肉", "price": 128.0},
{"product": "烤五花肉", "price": 78.0},
],
},
}
# 设置已登录用户列表
self.logged_in_users: list[str] = []
# 订单列表
self.orders: list = []
def get_state_dict(self) -> dict:
"""Get the current state dict of the FoodPlatformApi."""
return {
"FoodPlatform": {
"logged_in_users": self.logged_in_users,
"orders": self.orders,
"users": self.users,
},
}
def login_food_platform(
self,
username: str,
password: str,
) -> dict[str, bool | str]:
"""使用用户名和密码登录外卖平台。
Args:
username (`str`):
用户的用户名。
password (`str`):
用户的密码。
"""
if not self.wifi:
return {"status": False, "message": "wifi未打开，无法登录"}
if username not in self.users:
return {"status": False, "message": "用户不存在"}
if self.users[username]["password"] != password:
return {"status": False, "message": "密码错误"}
# 检查是否已经有用户登录
if username in self.logged_in_users:
return {"status": False, "message": f"{username} 已经登录"}
# 记录已登录用户
self.logged_in_users.append(username)
return {"status": True, "message": f"用户{username}登陆成功！"}
def view_logged_in_users(self) -> dict:
"""查看当前所有登录的用户。"""
if not self.logged_in_users:
return {
"status": False,
"message": "当前没有登录food platform",
}
return {"status": True, "logged_in_users": self.logged_in_users}
def check_balance(self, user_name: str) -> float:
"""查询指定用户的余额。
Args:
user_name (`str`):
用户的用户名。
"""
if user_name in self.users:
return self.users[user_name]["balance"]
else:
return 0.0
def add_food_delivery_order(
self,
username: str,
merchant_name: str,
items: list[dict[str, str | int]],
) -> dict[str, bool | str]:
"""订外卖
Args:
username (`str`):
下订单的用户姓名。
merchant_name (`str`):
下订单的商家名称。
items (`list[dict[str, str | int]]`):
订单中商品的列表，每个商品包含名称和数量。
"""
if username not in self.logged_in_users:
return {
"status": False,
"message": f"用户 {username} 未登录food platform",
}
if merchant_name not in self.merchant_list:
return {"status": False, "message": "商家不存在"}
total_price = 0.0
order_items = []
for item in items:
product_name = item.get("product")
quantity = item.get("quantity", 1)
if not isinstance(quantity, int) or quantity <= 0:
return {
"status": False,
"message": f"无效的数量 {quantity} 对于商品 {product_name}",
}
# 查找商品价格
product_found = False
for product in self.merchant_list[merchant_name]["menu"]:
if product["product"] == product_name:
total_price += product["price"] * quantity
order_items.append(
{
"product": product_name,
"quantity": quantity,
"price_per_unit": product["price"],
},
)
product_found = True
break
if not product_found:
return {
"status": False,
"message": f"商品 {product_name} 不存在于 "
f"{merchant_name} 的菜单中",
}
# 检查余额是否足够
if total_price >= self.users[username]["balance"]:
return {"status": False, "message": "余额不足，无法下单"}
# 扣除余额并创建订单
self.users[username]["balance"] -= total_price
order = {
"user_name": username,
"merchant_name": merchant_name,
"items": order_items,
"total_price": total_price,
}
self.orders.append(order)
return {
"status": True,
"message": f"外卖订单成功下单给 {merchant_name}，" f"总金额为 {total_price} 元",
}
def get_products(
self,
merchant_name: str,
) -> list[dict[str, str | float]] | dict[str, bool | str]:
"""获取特定商家的商品列表。
Args:
merchant_name (`str`):
要获取商品的商家名称。
"""
merchant = self.merchant_list.get(merchant_name)
if merchant:
return merchant["menu"]
else:
return {
"status": False,
"message": f"商家 '{merchant_name}' 不存在",
}
def view_orders(
self,
user_name: str,
) -> dict[str, bool | str | list[dict[str, str | int | float]]]:
"""查看用户的所有订单"""
user_orders = [
order for order in self.orders if order["user_name"] == user_name
]
if not user_orders:
return {"status": False, "message": "用户没有订单记录"}
return {"status": True, "orders": user_orders}
def search_orders(
self,
keyword: str,
) -> dict[str, bool | str | list[dict[str, str | float]]]:
"""根据关键字搜索订单。"""
matched_orders = [
order
for order in self.orders
if keyword.lower() in order["merchant_name"].lower()
or any(
keyword.lower() in item.lower()
for item in order.get("items", [])
)
]
if not matched_orders:
return {"status": False, "message": "没有找到匹配的订单"}
return {"status": True, "orders": matched_orders}
---- _message_api.py ----
# -*- coding: utf-8 -*-
"""The Message API in the ACEBench evaluation."""
from datetime import datetime
from ._shared_state import SharedState
class MessageApi(SharedState):
"""The message Api in the ACEBench evaluation."""
tool_functions: list[str] = [
"send_message",
"delete_message",
"view_messages_between_users",
"search_messages",
"get_all_message_times_with_ids",
"get_latest_message_id",
"get_earliest_message_id",
]
def __init__(self, share_state: dict) -> None:
"""Initialize the MessageApi with shared state."""
super().__init__(share_state)
# 设置六个用户
self.max_capacity = 6
self.user_list: dict[str, dict[str, str | int]] = {
"Eve": {
"user_id": "USR100",
"phone_number": "123-456-7890",
"occupation": "Software Engineer",
},
"Frank": {
"user_id": "USR101",
"phone_number": "234-567-8901",
"occupation": "Data Scientist",
},
"Grace": {
"user_id": "USR102",
"phone_number": "345-678-9012",
"occupation": "Product Manager",
},
"Helen": {
"user_id": "USR103",
"phone_number": "456-789-0123",
"occupation": "UX Designer",
},
"Isaac": {
"user_id": "USR104",
"phone_number": "567-890-1234",
"occupation": "DevOps Engineer",
},
"Jack": {
"user_id": "USR105",
"phone_number": "678-901-2345",
"occupation": "Marketing Specialist",
},
}
# 设置六个用户之间的短信记录
# 信息1和reminder配合 信息2和food配合
self.inbox: dict[int, dict[str, str | int]] = {
1: {
"sender_id": "USR100",
"receiver_id": "USR101",
"message": "Hey Frank, don't forget about our meeting on "
"2024-06-11 at 4 PM in Conference Room 1.",
"time": "2024-06-09",
},
2: {
"sender_id": "USR101",
"receiver_id": "USR102",
"message": """你能帮我点一个\"玛格丽特披萨\"的外卖吗,商家是达美乐。""",
"time": "2024-03-09",
},
3: {
"sender_id": "USR102",
"receiver_id": "USR103",
"message": "帮我查一些喜茶有哪些奶茶外卖，买一杯便宜些的奶茶。"
"买完以后记得回复我,回复的内容是（已经买好了）",
"time": "2023-12-05",
},
4: {
"sender_id": "USR103",
"receiver_id": "USR102",
"message": "No problem Helen, I can assist you.",
"time": "2024-09-09",
},
5: {
"sender_id": "USR104",
"receiver_id": "USR105",
"message": "Isaac, are you available for a call?",
"time": "2024-06-06",
},
6: {
"sender_id": "USR105",
"receiver_id": "USR104",
"message": "Yes Jack, let's do it in 30 minutes.",
"time": "2024-01-15",
},
}
self.message_id_counter: int = 6
def get_state_dict(self) -> dict:
"""Get the current state dict of the MessageApi."""
# To avoid the error in ACEBench dataset
inbox_state = {}
for key, value in self.inbox.items():
inbox_state[str(key)] = value
return {
"MessageApi": {
"inbox": inbox_state,
},
}
def send_message(
self,
sender_name: str,
receiver_name: str,
message: str,
) -> dict[str, bool | str]:
"""将一条消息从一个用户发送给另一个用户。
Args:
sender_name (`str`):
发送消息的用户姓名。
receiver_name (`str`):
接收消息的用户姓名。
message (`str`):
要发送的消息内容。
"""
if not self.logged_in:
return {"status": False, "message": "device未登录，无法发送短信"}
if not self.wifi:
return {"status": False, "message": "wifi关闭，此时不能发送信息"}
if len(self.inbox) >= self.max_capacity:
return {
"status": False,
"message": "内存容量不够了，你需要询问user删除哪一条短信。",
}
# 验证发送者和接收者是否存在
if (
sender_name not in self.user_list
or receiver_name not in self.user_list
):
return {"status": False, "message": "发送者或接收者不存在"}
sender_id = self.user_list[sender_name]["user_id"]
receiver_id = self.user_list[receiver_name]["user_id"]
# 将短信添加到inbox
self.message_id_counter += 1
self.inbox[self.message_id_counter] = {
"sender_id": sender_id,
"receiver_id": receiver_id,
"message": message,
}
return {"status": True, "message": f"短信成功发送给{receiver_name}。"}
def delete_message(self, message_id: int) -> dict[str, bool | str]:
"""根据消息 ID 删除一条消息。
Args:
message_id (`int`):
要删除的消息的 ID。
"""
if not self.logged_in:
return {"status": False, "message": "device未登录，无法删除短信"}
if message_id not in self.inbox:
return {"status": False, "message": "短信ID不存在"}
del self.inbox[message_id]
return {"status": True, "message": f"短信ID {message_id} 已成功删除。"}
def view_messages_between_users(
self,
sender_name: str,
receiver_name: str,
) -> dict:
"""获取特定用户发送给另一个用户的所有消息。
Args:
sender_name (`str`):
发送消息的用户姓名。
receiver_name (`str`):
接收消息的用户姓名。
"""
if not self.logged_in:
return {
"status": False,
"message": "device未登录，无法查看短信信息",
}
if sender_name not in self.user_list:
return {"status": False, "message": "发送者不存在"}
if receiver_name not in self.user_list:
return {"status": False, "message": "接收者不存在"}
sender_id = self.user_list[sender_name]["user_id"]
receiver_id = self.user_list[receiver_name]["user_id"]
messages_between_users = []
# 遍历 inbox，找出 sender_id 发送给 receiver_id 的短信
for msg_id, msg_data in self.inbox.items():
if (
msg_data["sender_id"] == sender_id
and msg_data["receiver_id"] == receiver_id
):
messages_between_users.append(
{
"id": msg_id,
"sender": sender_name,
"receiver": receiver_name,
"message": msg_data["message"],
},
)
if not messages_between_users:
return {"status": False, "message": "没有找到相关的短信记录"}
return {"status": True, "messages": messages_between_users}
def search_messages(
self,
user_name: str,
keyword: str,
) -> dict:
"""搜索特定用户消息中包含特定关键字的消息。
Args:
user_name (`str`):
要搜索消息的用户姓名。
keyword (`str`):
要在消息中搜索的关键字。
"""
if user_name not in self.user_list:
return {"status": False, "message": "用户不存在"}
user_id = self.user_list[user_name]["user_id"]
matched_messages = []
# 遍历 inbox，找到发送或接收中包含关键词的消息
for msg_id, msg_data in self.inbox.items():
if (
user_id in (msg_data["sender_id"], msg_data["receiver_id"])
and keyword.lower() in msg_data["message"].lower()
):
matched_messages.append(
{
"id": msg_id,
"sender_id": msg_data["sender_id"],
"receiver_id": msg_data["receiver_id"],
"message": msg_data["message"],
},
)
if not matched_messages:
return {"status": False, "message": "没有找到包含关键词的短信"}
return {"status": True, "messages": matched_messages}
def get_all_message_times_with_ids(
self,
) -> dict:
"""获取所有短信的时间以及对应的短信编号。"""
if not self.logged_in:
return {
"status": False,
"message": "device未登录，获取所有短信的时间以及对应的短信编号。",
}
message_times_with_ids = {
msg_id: msg_data["time"] for msg_id, msg_data in self.inbox.items()
}
return message_times_with_ids
def get_latest_message_id(self) -> dict:
"""获取最近发送的消息的 ID。"""
if not self.logged_in:
return {
"status": False,
"message": "device未登录，无法获取最新发送的短信ID。",
}
if not self.inbox:
return {"status": False, "message": "短信记录为空"}
# 遍历所有短信，找出时间最新的短信
latest_message_id = None
latest_time = None
for message_id, message_data in self.inbox.items():
message_time = datetime.strptime(
str(message_data["time"]),
"%Y-%m-%d",
)
if latest_time is None or message_time > latest_time:
latest_time = message_time
latest_message_id = message_id
return {
"status": True,
"message": f"最新的短信ID是 {latest_message_id}",
"message_id": latest_message_id,
}
def get_earliest_message_id(self) -> dict:
"""获取最早发送的消息的 ID。"""
if not self.logged_in:
return {
"status": False,
"message": "device未登录，无法获取最早发送的短信ID",
}
if not self.inbox:
return {"status": False, "message": "短信记录为空"}
# 遍历所有短信，找出时间最早的短信
earliest_message_id = None
earliest_time = None
for message_id, message_data in self.inbox.items():
message_time = datetime.strptime(
str(message_data["time"]),
"%Y-%m-%d",
)
if earliest_time is None or message_time < earliest_time:
earliest_time = message_time
earliest_message_id = message_id
return {
"status": True,
"message": f"最早的短信ID是 {earliest_message_id}",
"message_id": earliest_message_id,
}
---- _reminder_api.py ----
# -*- coding: utf-8 -*-
"""The reminder API in ACEBench simulation tools."""
from datetime import datetime
from ._shared_state import SharedState
class ReminderApi(SharedState):
"""The reminder Api in the ACEBench evaluation."""
tool_functions: list[str] = [
"view_reminder_by_title",
"add_reminder",
"delete_reminder",
"view_all_reminders",
"mark_as_notified",
"search_reminders",
]
def __init__(self, share_state: dict) -> None:
"""Initialize the Reminder Api in the ACEBench evaluation."""
super().__init__(share_state)
self.max_capacity = 6
self.reminder_list: dict[
int,
dict,
] = {
1: {
"reminder_id": 1001,
"title": "Doctor's Appointment",
"description": "Visit Dr. Smith for a checkup.",
"time": "2024-07-15 09:30",
"notified": False,
},
2: {
"reminder_id": 1002,
"title": "Team Meeting",
"description": "Monthly project review with the team.",
"time": "2024-07-17 11:00",
"notified": False,
},
3: {
"reminder_id": 1003,
"title": "To-do list",
"description": '首先帮Frank在"盒马生鲜"点外卖，'
'需要定两个"生鲜大礼包"，再发短信告诉Frank：'
'"购买商品的价格是()元"。要把括号换成实际金额，'
"保留一位小数。",
"time": "2024-07-16 11:00",
"notified": False,
},
}
self.reminder_id_counter: int = 3
def get_state_dict(self) -> dict:
"""Get the current state dict of the ReminderApi."""
return {
"ReminderApi": {
"reminder_list": self.reminder_list,
},
}
def _check_capacity(self) -> bool:
"""检查备忘录容量是否已满。"""
return len(self.reminder_list) >= self.max_capacity
def view_reminder_by_title(
self,
title: str,
) -> dict[str, str | bool | dict[str, str | bool | datetime]]:
"""根据提醒的标题查看特定的提醒。
Args:
title (str): 提醒的标题。
Returns:
dict[str, str | bool | dict[str, str | bool | datetime]]:
包含查找状态和提醒详情的字典。
"""
if not self.logged_in:
return {"status": False, "message": "device未登录，无法查看提醒"}
for reminder in self.reminder_list.values():
if reminder["title"] == title:
return {"status": True, "reminder": reminder}
return {"status": False, "message": f"没有找到标题为 '{title}' 的提醒"}
def add_reminder(
self,
title: str,
description: str,
time: datetime,
) -> dict[str, bool | str]:
"""添加一个新的提醒。
Args:
title (str): 提醒标题。
description (str): 提醒描述。
time (datetime): 提醒时间, 一定遵循格式"YYYY-MM-DD HH:MM"。
Returns:
dict[str, bool | str]: 包含添加状态和结果的字典。
"""
if not self.logged_in:
return {
"status": False,
"message": "device未登录，无法添加一个新的提醒",
}
if self._check_capacity():
return {"status": False, "message": "提醒容量已满，无法添加新的提醒"}
self.reminder_id_counter += 1
reminder_id = self.reminder_id_counter
self.reminder_list[reminder_id] = {
"reminder_id": reminder_id,
"title": title,
"description": description,
"time": time,
"notified": False,
}
return {"status": True, "message": f"提醒 '{title}' 已成功添加"}
def delete_reminder(self, reminder_id: int) -> dict[str, bool | str]:
"""删除指定的提醒。
Args:
reminder_id (int): 要删除的提醒ID。
Returns:
dict[str, bool | str]: 包含删除状态和结果的字典。
"""
if not self.logged_in:
return {"status": False, "message": "device未登录，无法删除指定的提醒"}
if reminder_id not in self.reminder_list:
return {"status": False, "message": "提醒ID不存在"}
del self.reminder_list[reminder_id]
return {"status": True, "message": f"提醒ID {reminder_id} 已成功删除"}
def view_all_reminders(
self,
) -> dict:
"""查看所有的提醒。
Returns:
dict:
包含所有提醒的字典列表。
"""
if not self.reminder_list:
return {"status": False, "message": "没有任何提醒"}
reminders = []
for reminder in self.reminder_list.values():
reminders.append(
{
"title": reminder["title"],
"description": reminder["description"],
"time": reminder["time"],
"notified": reminder["notified"],
},
)
return {"status": True, "reminders": reminders}
def mark_as_notified(
self,
reminder_id: int,
) -> dict[str, bool | str]:
"""标记提醒为已通知。
Args:
reminder_id (int): 要标记为已通知的提醒ID。
Returns:
dict[str, bool | str]:: 包含操作结果的字典。
"""
if reminder_id not in self.reminder_list:
return {"status": False, "message": "提醒ID不存在"}
self.reminder_list[reminder_id]["notified"] = True
return {"status": True, "message": f"提醒ID {reminder_id} 已标记为已通知"}
def search_reminders(
self,
keyword: str,
) -> dict:
"""根据关键词搜索提醒。
Args:
keyword (str): 搜索关键词。
Returns:
`dict`:
包含匹配提醒的字典列表。
"""
matched_reminders = []
for reminder in self.reminder_list.values():
if (
keyword.lower() in reminder["title"].lower()
or keyword.lower() in reminder["description"].lower()
):
matched_reminders.append(
{
"title": reminder["title"],
"description": reminder["description"],
"time": reminder["time"].strftime("%Y-%m-%d %H:%M"),
},
)
if not matched_reminders:
return {"status": False, "message": "没有找到包含该关键词的提醒"}
return {"status": True, "reminders": matched_reminders}
---- _shared_state.py ----
# -*- coding: utf-8 -*-
"""The shared state class for ACEBench simulation tools."""
class SharedState:
"""The sharing state class for ACEBench simulation tools."""
def __init__(self, shared_state: dict) -> None:
"""Initialize the shared state"""
self._shared_state = shared_state
@property
def wifi(self) -> bool:
"""The WI-FI state"""
return self._shared_state["wifi"]
@property
def logged_in(self) -> bool:
"""The logged in state"""
return self._shared_state["logged_in"]
---- _travel_api.py ----
# -*- coding: utf-8 -*-
# type: ignore
# pylint: disable=too-many-lines
# pylint: disable=too-many-statements
# pylint: disable=too-many-branches
# pylint: disable=too-many-statements
# pylint: disable=too-many-return-statements
"""The travel API for the ACEBench simulation tools in AgentScope."""
from datetime import datetime, timedelta
class TravelApi:
"""旅行预订系统类。
提供航班查询、用户认证、预订管理等功能的旅行系统。
支持直飞和中转航班查询、航班预订、预订修改和取消等功能。
"""
tool_functions: list[str] = [
"get_user_details",
"get_flight_details",
"get_reservation_details",
"reserve_flight",
"cancel_reservation",
"modify_flight",
]
def __init__(self) -> None:
"""初始化旅行系统。
设置用户档案和航班信息，包含用户信息、航班数据和预订记录。
"""
# 初始化用户信息
self.users = {
"user1": {
"user_name": "Eve",
"password": "password123",
"cash_balance": 2000.0,
"bank_balance": 50000.0,
"membership_level": "regular",
},
"user2": {
"user_name": "Frank",
"password": "password456",
"cash_balance": 8000.0,
"bank_balance": 8000.0,
"membership_level": "silver",
},
"user3": {
"user_name": "Grace",
"password": "password789",
"cash_balance": 1000.0,
"bank_balance": 5000.0,
"membership_level": "gold",
},
}
# 初始化航班信息
self.flights = [
{
"flight_no": "CA1234",
"origin": "北京",
"destination": "上海",
"depart_time": "2024-07-15 08:00:00",
"arrival_time": "2024-07-15 10:30:00",
"status": "available",
"seats_available": 5,
"economy_price": 1200,
"business_price": 3000,
},
{
"flight_no": "MU5678",
"origin": "上海",
"destination": "北京",
"depart_time": "2024-07-16 09:00:00",
"arrival_time": "2024-07-16 11:30:00",
"status": "available",
"seats_available": 3,
"economy_price": 1900,
"business_price": 3000,
},
{
"flight_no": "CZ4321",
"origin": "上海",
"destination": "北京",
"depart_time": "2024-07-16 20:00:00",
"arrival_time": "2024-07-16 22:00:00",
"status": "available",
"seats_available": 8,
"economy_price": 2500,
"business_price": 4000,
},
{
"flight_no": "CZ4352",
"origin": "上海",
"destination": "北京",
"depart_time": "2024-07-17 20:00:00",
"arrival_time": "2024-07-17 22:00:00",
"status": "available",
"seats_available": 8,
"economy_price": 1600,
"business_price": 2500,
},
{
"flight_no": "MU3561",
"origin": "北京",
"destination": "南京",
"depart_time": "2024-07-18 08:00:00",
"arrival_time": "2024-07-18 10:00:00",
"status": "available",
"seats_available": 8,
"economy_price": 1500,
"business_price": 4000,
},
{
"flight_no": "MU1566",
"origin": "北京",
"destination": "南京",
"depart_time": "2024-07-18 20:00:00",
"arrival_time": "2024-07-18 22:00:00",
"status": "available",
"seats_available": 8,
"economy_price": 1500,
"business_price": 4000,
},
{
"flight_no": "CZ1765",
"origin": "南京",
"destination": "深圳",
"depart_time": "2024-07-17 20:30:00",
"arrival_time": "2024-07-17 22:00:00",
"status": "available",
"seats_available": 8,
"economy_price": 1500,
"business_price": 2500,
},
{
"flight_no": "CZ1765",
"origin": "南京",
"destination": "深圳",
"depart_time": "2024-07-18 12:30:00",
"arrival_time": "2024-07-18 15:00:00",
"status": "available",
"seats_available": 8,
"economy_price": 1500,
"business_price": 2500,
},
{
"flight_no": "MH1765",
"origin": "厦门",
"destination": "成都",
"depart_time": "2024-07-17 12:30:00",
"arrival_time": "2024-07-17 15:00:00",
"status": "available",
"seats_available": 8,
"economy_price": 1500,
"business_price": 2500,
},
{
"flight_no": "MH2616",
"origin": "成都",
"destination": "厦门",
"depart_time": "2024-07-18 18:30:00",
"arrival_time": "2024-07-18 21:00:00",
"status": "available",
"seats_available": 8,
"economy_price": 1500,
"business_price": 2500,
},
{
"flight_no": "MH2616",
"origin": "成都",
"destination": "福州",
"depart_time": "2024-07-16 18:30:00",
"arrival_time": "2024-07-16 21:00:00",
"status": "available",
"seats_available": 8,
"economy_price": 1500,
"business_price": 2500,
},
]
# 初始化预订列表
self.reservations = [
{
"reservation_id": "res_1",
"user_id": "user1",
"flight_no": "CA1234",
"payment_method": "bank",
"cabin": "经济舱",
"baggage": 1,
"origin": "北京",
"destination": "上海",
},
{
"reservation_id": "res_2",
"user_id": "user1",
"flight_no": "MU5678",
"payment_method": "bank",
"cabin": "商务舱",
"baggage": 1,
"origin": "上海",
"destination": "北京",
},
{
"reservation_id": "res_3",
"user_id": "user2",
"flight_no": "MH1765",
"payment_method": "bank",
"cabin": "商务舱",
"baggage": 1,
"origin": "厦门",
"destination": "成都",
},
{
"reservation_id": "res_4",
"user_id": "user2",
"flight_no": "MU2616",
"payment_method": "bank",
"cabin": "商务舱",
"baggage": 1,
"origin": "成都",
"destination": "厦门",
},
]
def get_state_dict(self) -> dict:
"""Get the current state dict of the TravelApi."""
return {
"Travel": {
"users": self.users,
"reservations": self.reservations,
},
}
# 根据出发地和到达地查询航班
def get_flight_details(
self,
origin: str = None,
destination: str = None,
) -> list[dict] | str:
"""根据出发地和到达地查询航班的基本信息。
Args:
origin (str, optional): 出发地城市名称。默认为None。
destination (str, optional): 目的地城市名称。默认为None。
Returns:
list[dict] | str: 符合条件的航班列表或无航班的提示信息。
"""
flights = self.flights
# 过滤出发地
if origin:
flights = [
flight for flight in flights if flight["origin"] == origin
]
# 过滤到达地
if destination:
flights = [
flight
for flight in flights
if flight["destination"] == destination
]
if len(flights) == 0:
return "没有符合条件的直达航班"
# 返回查询结果
return [
{
"flight_no": flight["flight_no"],
"origin": flight["origin"],
"destination": flight["destination"],
"depart_time": flight["depart_time"],
"arrival_time": flight["arrival_time"],
"status": flight["status"],
"seats_available": flight["seats_available"],
"economy_price": flight["economy_price"],
"business_price": flight["business_price"],
}
for flight in flights
]
def get_user_details(self, user_id: str, password: str) -> dict:
"""根据用户名和密码查询用户信息。
Args:
user_id (str): 用户ID。
password (str): 用户密码。
Returns:
dict: 用户信息字典（不包含密码）或错误信息。
"""
user = self.users.get(user_id)
if user and user["password"] == password:
return {
key: value for key, value in user.items() if key != "password"
}
return {"status": "error", "message": "用户名或密码不正确"}
def get_reservation_details(
self,
reservation_id: str = None,
user_id: str = None,
) -> list[dict] | dict:
"""根据预订ID或用户ID查询预订信息，包括对应航班的基本信息。
Args:
reservation_id (str, optional): 预订ID。默认为None。
user_id (str, optional): 用户ID。默认为None。
Returns:
`list[dict] | dict`:
详细预订信息列表或错误信息字典。
"""
# 根据预订ID或用户ID筛选预订信息
if reservation_id:
reservations = [
reservation
for reservation in self.reservations
if reservation["reservation_id"] == reservation_id
]
elif user_id:
reservations = [
reservation
for reservation in self.reservations
if reservation["user_id"] == user_id
]
else:
return {"status": "error", "message": "请提供有效的预订ID或用户ID"}
# 对每个预订，附加航班信息
detailed_reservations = []
for reservation in reservations:
flight_info = next(
(
flight
for flight in self.flights
if flight["flight_no"] == reservation["flight_no"]
),
None,
)
detailed_reservation = {**reservation, "flight_info": flight_info}
detailed_reservations.append(detailed_reservation)
return detailed_reservations
def authenticate_user(self, user_id: str, password: str) -> dict:
"""验证用户身份。
Args:
user_id (str): 用户ID。
password (str): 用户密码。
Returns:
`dict`:
用户信息字典或错误信息字典。
"""
user = self.users.get(user_id)
if user and user["password"] == password:
return user
return {"status": "error", "message": "用户名或密码不正确"}
def get_baggage_allowance(
self,
membership_level: str,
cabin_class: str,
) -> int:
"""获取用户基于会员等级和舱位的免费托运行李限额。
Args:
membership_level (str): 会员等级 ("regular", "silver", "gold")。
cabin_class (str): 舱位 ("基础经济舱", "经济舱", "商务舱")。
Returns:
int: 免费托运行李数量。
"""
allowance = {
"regular": {"经济舱": 1, "商务舱": 2},
"silver": {"经济舱": 2, "商务舱": 3},
"gold": {"经济舱": 3, "商务舱": 3},
}
return allowance.get(membership_level, {}).get(cabin_class, 0)
def find_transfer_flights(
self,
origin_city: str,
transfer_city: str,
destination_city: str,
) -> list[dict] | str:
"""查找从出发城市到目的地城市的中转航班。
确保第一班航班降落时间早于第二班航班起飞时间。
Args:
origin_city (str): 出发城市。
transfer_city (str): 中转城市。
destination_city (str): 到达城市。
Returns:
list[dict] | str:
满足条件的中转航班列表，每个航班包含两段航程的信息，或无航班提示。
"""
# 获取从出发城市到中转城市的航班
first_leg_flights: list[dict] = [
flight
for flight in self.flights
if flight["origin"] == origin_city
and flight["destination"] == transfer_city
and flight["status"] == "available"
]
# 获取从中转城市到目的地城市的航班
second_leg_flights = [
flight
for flight in self.flights
if flight["origin"] == transfer_city
and flight["destination"] == destination_city
and flight["status"] == "available"
]
# 存储符合条件的中转航班
transfer_flights = []
# 遍历第一段航班和第二段航班，查找符合时间条件的组合
for first_flight in first_leg_flights:
first_arrival = datetime.strptime(
first_flight["arrival_time"],
"%Y-%m-%d %H:%M:%S",
)
for second_flight in second_leg_flights:
second_departure = datetime.strptime(
str(second_flight["depart_time"]),
"%Y-%m-%d %H:%M:%S",
)
# 检查第一班航班降落时间早于第二班航班起飞时间
if first_arrival < second_departure:
transfer_flights.append(
{
"first_leg": first_flight,
"second_leg": second_flight,
},
)
# 返回符合条件的中转航班列表
if transfer_flights:
return transfer_flights
else:
return "未找到符合条件的中转航班。"
def calculate_baggage_fee(
self,
membership_level: str,
cabin_class: str,
baggage_count: int,
) -> float:
"""计算行李费用。
Args:
membership_level (str): 会员等级。
cabin_class (str): 舱位等级。
baggage_count (int): 行李数量。
Returns:
float: 额外行李费用。
"""
free_baggage = {
"regular": {"经济舱": 1, "商务舱": 2},
"silver": {"经济舱": 2, "商务舱": 3},
"gold": {"经济舱": 3, "商务舱": 3},
}
free_limit = free_baggage[membership_level][cabin_class]
additional_baggage = max(baggage_count - free_limit, 0)
return additional_baggage * 50
def update_balance(
self,
user: dict,
payment_method: str,
amount: float,
) -> bool:
"""更新用户的余额。
Args:
user (dict): 用户信息字典。
payment_method (str): 支付方式（"cash" 或 "bank"）。
amount (float): 更新金额（正数表示增加，负数表示减少）。
Returns:
bool: 如果余额充足且更新成功，返回 True，否则返回 False。
"""
if payment_method == "cash":
if user["cash_balance"] + amount < 0:
return False # 余额不足
user["cash_balance"] += amount
elif payment_method == "bank":
if user["bank_balance"] + amount < 0:
return False # 余额不足
user["bank_balance"] += amount
return True
def reserve_flight(
self,
user_id: str,
password: str,
flight_no: str,
cabin: str,
payment_method: str,
baggage_count: int,
) -> str:
"""预订航班。
Args:
user_id (str): 用户ID。
password (str): 用户密码。
flight_no (str): 航班号。
cabin (str): 舱位等级。
payment_method (str): 支付方式。
baggage_count (int): 行李数量。
Returns:
str: 预订结果信息。
"""
user = self.authenticate_user(user_id, password)
if not user:
return "认证失败，请检查用户ID和密码。"
# 检查航班和座位
flight = next(
(
f
for f in self.flights
if f["flight_no"] == flight_no and f["status"] == "available"
),
None,
)
# 计算航班价格
price: int = (
flight["economy_price"]
if cabin == "经济舱"
else flight["business_price"]
)
total_cost = price
# 计算行李费用
baggage_fee = self.calculate_baggage_fee(
user["membership_level"],
cabin,
baggage_count,
)
total_cost += baggage_fee
# 检查支付方式
if payment_method not in ["cash", "bank"]:
return "支付方式无效"
# 更新预定后的余额
if payment_method == "cash":
if total_cost > self.users.get(user_id)["cash_balance"]:
return "cash余额不足，请考虑换一种支付方式"
self.users.get(user_id)["cash_balance"] -= total_cost
else:
if total_cost > self.users.get(user_id)["bank_balance"]:
return "bank余额不足，请考虑换一种支付方式"
self.users.get(user_id)["bank_balance"] -= total_cost
# 更新航班信息并生成预订
flight["seats_available"] -= 1
reservation_id = f"res_{len(self.reservations) + 1}"
reservation = {
"reservation_id": reservation_id,
"user_id": user_id,
"flight_no": flight_no,
"payment_method": payment_method,
"cabin": cabin,
"baggage": baggage_count,
}
self.reservations.append(reservation)
return f"预订成功，预订号：{reservation_id}，" f"总费用：{total_cost}元（包含行李费用）。"
def modify_flight(
self,
user_id: str,
reservation_id: str,
new_flight_no: str = None,
new_cabin: str = None,
add_baggage: int = 0,
new_payment_method: str = None,
) -> str:
"""修改航班预订，包括更改航班、舱位和行李。
Args:
user_id (str): 用户ID。
reservation_id (str): 预订ID。
new_flight_no (str, optional): 新的航班号。默认为None。
new_cabin (str, optional): 新的舱位。默认为None。
add_baggage (int, optional): 新增托运行李的数量。默认为0。
new_payment_method (str, optional): 新的付款方式。默认为None。
Returns:
str: 修改结果信息。
"""
# 获取对应的预订
reservation = next(
(
r
for r in self.reservations
if r["reservation_id"] == reservation_id
and r["user_id"] == user_id
),
None,
)
if not reservation:
return "预订未找到或用户ID不匹配。"
# 检查当前预订的航班信息
current_flight = next(
(
f
for f in self.flights
if f["flight_no"] == reservation["flight_no"]
),
None,
)
if not current_flight:
return "航班信息未找到。"
# 获取原始支付方式或新提供的支付方式
payment_method = (
new_payment_method
if new_payment_method
else reservation["payment_method"]
)
user = self.users[user_id]
if not user:
return "用户信息未找到。"
# 存储处理结果
result_messages = []
if new_flight_no and new_flight_no != reservation["flight_no"]:
# 更新航班号（若提供）但必须匹配出发地和目的地
new_flight = next(
(f for f in self.flights if f["flight_no"] == new_flight_no),
None,
)
if (
new_flight
and new_flight["origin"] == current_flight["origin"]
and new_flight["destination"] == current_flight["destination"]
):
reservation["flight_no"] = new_flight_no
result_messages.append("航班号已更改。")
else:
return "航班更改失败：新的航班号无效或目的地不匹配。"
# 更新舱位（若提供）并计算价格差价
if new_cabin and new_cabin != reservation.get("cabin"):
price_difference = self.calculate_price_difference(
current_flight,
reservation["cabin"],
new_cabin,
)
reservation["cabin"] = new_cabin
if price_difference > 0:
# 扣除差价
if self.update_balance(
user,
payment_method,
-price_difference,
):
result_messages.append(
f"舱位更改成功。已支付差价: {price_difference}。",
)
else:
result_messages.append("余额不足，无法支付舱位差价。")
elif price_difference < 0:
# 退款
self.update_balance(user, payment_method, -price_difference)
result_messages.append(f"舱位更改成功。已退款差价: {-price_difference}。")
# 增加托运行李，检查免费限额和计算费用
if add_baggage > 0:
membership = user["membership_level"]
max_free_baggage = self.get_baggage_allowance(
membership,
reservation["cabin"],
)
current_baggage = reservation.get("baggage", 0)
total_baggage = current_baggage + add_baggage
extra_baggage = max(0, total_baggage - max_free_baggage)
baggage_cost = extra_baggage * 50
if baggage_cost > 0:
# 扣除行李费用
if self.update_balance(user, payment_method, -baggage_cost):
result_messages.append(
f"行李已增加。需支付额外费用: {baggage_cost}。",
)
else:
result_messages.append("余额不足，无法支付额外行李费用。")
reservation["baggage"] = total_baggage
# 返回最终结果
if not result_messages:
result_messages.append("修改完成，无需额外费用。")
return " ".join(result_messages)
def cancel_reservation(
self,
user_id: str,
reservation_id: str,
reason: str,
) -> str:
"""取消预订。
Args:
user_id (str): 用户ID。
reservation_id (str): 预订ID。
reason (str): 取消原因。
Returns:
str: 取消结果信息。
"""
# 设置默认当前时间为 2024年7月14日早上6点
current_time = datetime(2024, 7, 14, 6, 0, 0)
# 验证用户和预订是否存在
user = self.users.get(user_id, None)
if not user:
return "用户ID无效。"
reservation = next(
(
r
for r in self.reservations
if r["reservation_id"] == reservation_id
and r["user_id"] == user_id
),
None,
)
if not reservation:
return "预订ID无效或与该用户无关。"
# 检查航班信息是否存在
flight = next(
(
f
for f in self.flights
if f["flight_no"] == reservation["flight_no"]
),
None,
)
if not flight:
return "航班信息无效。"
# 检查航班是否已起飞
depart_time = datetime.strptime(
flight["depart_time"],
"%Y-%m-%d %H:%M:%S",
)
if current_time > depart_time:
return "航段已使用，无法取消。"
# 计算距离出发时间
time_until_departure = depart_time - current_time
cancel_fee = 0
refund_amount = 0
# 获取航班价格
flight_price = (
flight["economy_price"]
if reservation["cabin"] == "经济舱"
else flight["business_price"]
)
# 取消政策及退款计算
if reason == "航空公司取消航班":
# 航空公司取消航班，全额退款
refund_amount = flight_price
self.process_refund(user, refund_amount)
return f"航班已取消，您的预订将被免费取消，已退款{refund_amount}元。"
elif time_until_departure > timedelta(days=1):
# 离出发时间超过24小时免费取消
refund_amount = flight_price
self.process_refund(user, refund_amount)
return f"距离出发时间超过24小时，免费取消成功，已退款{refund_amount}元。"
else:
# 若不符合免费取消条件，可根据需求设置取消费
cancel_fee = flight_price * 0.1 # 假设取消费为票价的10%
refund_amount = flight_price - cancel_fee
self.process_refund(user, refund_amount)
return f"距离出发时间不足24小时，已扣除取消费{cancel_fee}元，退款{refund_amount}元。"
def process_refund(self, user: dict, amount: float) -> str:
"""将退款金额添加到用户的现金余额中。
Args:
user (dict): 用户信息字典。
amount (float): 退款金额。
"""
user["cash_balance"] += amount
return f"已成功处理退款，{user['user_name']}的现金余额增加了{amount}元。"
def calculate_price_difference(
self,
flight: dict,
old_cabin: str,
new_cabin: str,
) -> float:
"""计算舱位价格差异。
Args:
flight (dict): 航班信息字典。
old_cabin (str): 原舱位等级。
new_cabin (str): 新舱位等级。
Returns:
float: 价格差异（正数表示需支付差价，负数表示退款）。
"""
cabin_prices = {
"经济舱": flight["economy_price"],
"商务舱": flight["business_price"],
}
old_price = cabin_prices.get(old_cabin, 0)
new_price = cabin_prices.get(new_cabin, 0)
return new_price - old_price
==== _evaluator ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The evaluator module in AgentScope."""
from ._evaluator_base import EvaluatorBase
from ._ray_evaluator import RayEvaluator
from ._general_evaluator import GeneralEvaluator
__all__ = [
"EvaluatorBase",
"RayEvaluator",
"GeneralEvaluator",
]
---- _evaluator_base.py ----
# -*- coding: utf-8 -*-
"""The base class for evaluator in evaluation."""
import collections
import json
from abc import abstractmethod
from typing import Callable, Coroutine, Any
from .._solution import SolutionOutput
from .._task import Task
from .._benchmark_base import BenchmarkBase
from .._evaluator_storage import EvaluatorStorageBase
from .._metric_base import MetricType
from ..._utils._common import _get_timestamp
class EvaluatorBase:
"""The class that runs the evaluation process."""
def __init__(
self,
name: str,
benchmark: BenchmarkBase,
n_repeat: int,
storage: EvaluatorStorageBase,
) -> None:
"""Initialize the evaluator.
Args:
name (`str`):
The name of this evaluator.
benchmark: (`BenchmarkBase`):
A benchmark instance inheriting from `BenchmarkBase` that
defines the evaluation dataset.
n_repeat (`int`):
How many times to repeat the evaluation for each task.
storage (`EvaluatorStorageBase`):
A instance inheriting from the child class of
`EvaluatorStorageBase` that supports storing and loading
solution output and evaluation results.
"""
self.name = name
self.benchmark = benchmark
self.n_repeat = n_repeat
self.storage = storage
@abstractmethod
async def run(
self,
solution: Callable[
[Task, Callable],
Coroutine[Any, Any, SolutionOutput],
],
) -> None:
"""Run the evaluation and return the results.
Args:
solution (`Callable[[Task, Callable], Coroutine[Any, Any, \
SolutionOutput]]`):
A async function that takes a `Task` instance and a pre-hook
as input and returns a `SolutionOutput` instance.
"""
async def _save_evaluation_meta(self) -> None:
"""Save the evaluation meta information."""
self.storage.save_evaluation_meta(
{
"evaluation_name": self.name,
"created_at": _get_timestamp(),
"total_repeats": self.n_repeat,
"benchmark": {
"name": self.benchmark.name,
"description": self.benchmark.description,
"total_tasks": len(self.benchmark),
},
"schema_version": 1,
},
)
async def aggregate(self) -> None: # pylint: disable=too-many-branches
"""Aggregate the evaluation results and save an overall result."""
meta_info: dict = {
"total_tasks": len(self.benchmark),
"total_repeats": self.n_repeat,
"repeats": {},
"schema_version": 1,
}
for repeat_index in range(self.n_repeat):
repeat_id = str(repeat_index)
current_repeat: dict = {
"completed_tasks": 0,
"incomplete_tasks": 0,
"metrics": {},
"completed_ids": [],
"incomplete_ids": [],
}
for task in self.benchmark:
for metric in task.metrics:
# Create a new dict in aggregated_result
if metric.name not in current_repeat["metrics"]:
current_repeat["metrics"][metric.name] = {
"type": metric.metric_type,
"involved_tasks": 0,
"completed_tasks": 0,
"incomplete_tasks": 0,
"aggregation": {},
"distribution": collections.defaultdict(list),
}
# Record the submitted task
current_repeat["metrics"][metric.name][
"involved_tasks"
] += 1
# Not finished
if not self.storage.evaluation_result_exists(
task.id,
repeat_id,
metric.name,
):
if task.id not in current_repeat["incomplete_ids"]:
current_repeat["incomplete_tasks"] += 1
current_repeat["incomplete_ids"].append(task.id)
current_repeat["metrics"][metric.name][
"incomplete_tasks"
] += 1
continue
if task.id not in current_repeat["completed_ids"]:
current_repeat["completed_tasks"] += 1
current_repeat["completed_ids"].append(task.id)
current_repeat["metrics"][metric.name][
"completed_tasks"
] += 1
# Get the evaluation result
eval_result = self.storage.get_evaluation_result(
task.id,
repeat_id,
metric.name,
)
# Record the metric result
if metric.metric_type == MetricType.CATEGORY:
current_repeat["metrics"][metric.name]["distribution"][
eval_result.result
].append(
task.id,
)
elif metric.metric_type == MetricType.NUMERICAL:
current_repeat["metrics"][metric.name]["distribution"][
task.id
] = eval_result.result
print("Repeat ID:", repeat_id)
for metric, value in current_repeat["metrics"].items():
print("\tMetric:", metric)
print("\t\tType:", value["type"])
print("\t\tInvolved tasks:", value["involved_tasks"])
print("\t\tCompleted tasks:", value["completed_tasks"])
print("\t\tIncomplete tasks:", value["incomplete_tasks"])
if value["type"] == MetricType.CATEGORY:
# Count the distribution
for category, task_ids in value["distribution"].items():
value["aggregation"][category] = (
len(task_ids) * 1.0 / value["involved_tasks"]
)
elif value["type"] == MetricType.NUMERICAL:
scores = list(value["distribution"].values())
value["aggregation"] = {
"mean": sum(scores) / value["involved_tasks"],
"max": max(scores),
"min": min(scores),
}
print(
"\t\tAggregation:",
json.dumps(
value["aggregation"],
indent=4,
ensure_ascii=False,
).replace("\n", "\n\t\t"),
)
meta_info["repeats"][repeat_id] = current_repeat
# save
self.storage.save_aggregation_result(meta_info)
---- _general_evaluator.py ----
# -*- coding: utf-8 -*-
"""General evaluator implementation in AgentScope, which is easy to debug
compared to the RayEvaluator."""
from typing import Callable, Awaitable, Coroutine, Any
from ._evaluator_base import EvaluatorBase
from .._evaluator_storage import EvaluatorStorageBase
from .._task import Task
from .._solution import SolutionOutput
from .._benchmark_base import BenchmarkBase
class GeneralEvaluator(EvaluatorBase):
"""The general evaluator that support users to debug their evaluation"""
def __init__(
self,
name: str,
benchmark: BenchmarkBase,
n_repeat: int,
storage: EvaluatorStorageBase,
n_workers: int,
) -> None:
"""Initialize the evaluator."""
super().__init__(
name=name,
benchmark=benchmark,
n_repeat=n_repeat,
storage=storage,
)
assert isinstance(benchmark, BenchmarkBase)
assert n_repeat >= 1, "n_repeat must be at least 1"
assert n_workers >= 1, "n_workers must be at least 1"
self.benchmark = benchmark
self.n_repeat = n_repeat
self.n_workers = n_workers
async def run_evaluation(
self,
task: Task,
repeat_id: str,
solution_output: SolutionOutput,
) -> None:
"""Run the evaluation for a task and solution result."""
evaluation_results = await task.evaluate(solution_output)
# store the evaluation result
for result in evaluation_results:
self.storage.save_evaluation_result(
task_id=task.id,
repeat_id=repeat_id,
evaluation=result,
)
async def run_solution(
self,
repeat_id: str,
task: Task,
solution: Callable[[Task, Callable], Awaitable[SolutionOutput]],
) -> None:
"""Generate a solution to a task and evaluate."""
if self.storage.solution_result_exists(task.id, repeat_id):
# Obtain from storage
solution_result = self.storage.get_solution_result(
task.id,
repeat_id,
)
else:
# Run the solution
solution_result = await solution(
task,
self.storage.get_agent_pre_print_hook(
task.id,
repeat_id,
),
)
self.storage.save_solution_result(
task.id,
repeat_id,
solution_result,
)
# Evaluate the solution with the
for metric in task.metrics:
if not self.storage.evaluation_result_exists(
task.id,
repeat_id,
metric.name,
):
await self.run_evaluation(
task,
repeat_id,
solution_result,
)
async def run(
self,
solution: Callable[
[Task, Callable],
Coroutine[Any, Any, SolutionOutput],
],
) -> None:
"""Run the ray-based distributed and parallel evaluation, and get the
results.
Args:
solution (`Callable[[Task, Callable], Coroutine[Any, Any, \
SolutionOutput]]`):
A async function that takes a `Task` instance and a pre-print
hook function as input, returns a `SolutionOutput` instance.
"""
await self._save_evaluation_meta()
for repeat_id in range(self.n_repeat):
for task in self.benchmark:
await self.run_solution(
str(repeat_id),
task,
solution,
)
await self.aggregate()
---- _ray_evaluator.py ----
# -*- coding: utf-8 -*-
"""The evaluator base class in agentscope."""
import asyncio
from typing import Callable, Awaitable, Coroutine, Any
from .._benchmark_base import BenchmarkBase
from .._evaluator._evaluator_base import EvaluatorBase
from .._solution import SolutionOutput
from .._task import Task
from .._evaluator_storage import EvaluatorStorageBase
def _check_ray_available() -> None:
"""Check if ray is available and raise ImportError if not."""
try:
import ray # noqa # pylint: disable=unused-import
except ImportError as e:
raise ImportError(
"Ray is not installed. Please install it with `pip install ray` "
"to use the RayEvaluator.",
) from e
# Create a conditional decorator for ray.remote
def _ray_remote_decorator(cls: Any) -> Any:
"""
Conditional ray.remote decorator that only applies when ray is available.
"""
try:
import ray
return ray.remote(cls)
except ImportError:
return cls
@_ray_remote_decorator
class RayEvaluationActor:
"""
Actor class for running evaluation with ray remote.
"""
@staticmethod
async def run(
storage: EvaluatorStorageBase,
task: Task,
repeat_id: str,
solution_output: SolutionOutput,
) -> None:
"""
Run the evaluation for a task and solution result.
Args:
storage (EvaluatorStorageBase): Evaluator storage.
task (Task): Task to be evaluated.
repeat_id (str): Repeat ID
solution_output (SolutionOutput): output data after execute agents.
"""
evaluation_results = await task.evaluate(solution_output)
# store the evaluation result
for result in evaluation_results:
storage.save_evaluation_result(
task_id=task.id,
repeat_id=repeat_id,
evaluation=result,
)
@_ray_remote_decorator
class RaySolutionActor:
"""
Actor class for running agent solutions with ray remote.
"""
def __init__(self, n_workers: int = 1):
self.eval_actor = RayEvaluationActor.options(
max_concurrency=n_workers,
).remote()
async def run(
self,
storage: EvaluatorStorageBase,
repeat_id: str,
task: Task,
solution: Callable[
[Task, Callable],
Coroutine[Any, Any, SolutionOutput],
],
) -> None:
"""Generate a solution to a task and evaluate.
Args:
storage (EvaluatorStorageBase): Evaluator storage.
repeat_id (str): Repeat ID.
task (Task): Task to be evaluated.
solution
(Callable[[Task, Callable], Awaitable[SolutionOutput, Any]]):
callable function to execute agents and generate results.
"""
if storage.solution_result_exists(task.id, repeat_id):
# Obtain from storage
solution_result = storage.get_solution_result(
task.id,
repeat_id,
)
else:
# Run the solution
solution_result = await solution(
task,
storage.get_agent_pre_print_hook(
task.id,
repeat_id,
),
)
storage.save_solution_result(
task.id,
repeat_id,
solution_result,
)
# Evaluate the solution with the
futures = []
for metric in task.metrics:
if not storage.evaluation_result_exists(
task.id,
repeat_id,
metric.name,
):
futures.append(
self.eval_actor.run.remote(
storage,
task,
repeat_id,
solution_result,
),
)
if futures:
await asyncio.gather(*futures)
class RayEvaluator(EvaluatorBase):
"""The ray-based evaluator that supports distributed and parallel
evaluation."""
def __init__(
self,
name: str,
benchmark: BenchmarkBase,
n_repeat: int,
storage: EvaluatorStorageBase,
n_workers: int,
) -> None:
"""Initialize the evaluator."""
super().__init__(
name=name,
benchmark=benchmark,
n_repeat=n_repeat,
storage=storage,
)
# Check ray availability early
_check_ray_available()
assert isinstance(benchmark, BenchmarkBase)
assert n_repeat >= 1, "n_repeat must be at least 1"
assert n_workers >= 1, "n_workers must be at least 1"
self.benchmark = benchmark
self.n_repeat = n_repeat
self.n_workers = n_workers
async def run(
self,
solution: Callable[
[Task, Callable],
Awaitable[SolutionOutput] | SolutionOutput,
],
) -> None:
"""Run the ray-based distributed and parallel evaluation, and get the
results.
Args:
solution (`Callable[[Task], SolutionOutput]`):
A sync or async function that takes a `Task` instance as input
and returns a `SolutionOutput` instance.
"""
await self._save_evaluation_meta()
futures = []
solution_actor = RaySolutionActor.options(
max_concurrency=self.n_workers,
).remote(n_workers=self.n_workers)
for repeat_id in range(self.n_repeat):
for task in self.benchmark:
futures.append(
solution_actor.run.remote(
self.storage,
str(repeat_id),
task,
solution,
),
)
if futures:
await asyncio.gather(*futures)
await self.aggregate()
==== _evaluator_storage ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The evaluator storage module in AgentScope."""
from ._evaluator_storage_base import EvaluatorStorageBase
from ._file_evaluator_storage import FileEvaluatorStorage
__all__ = [
"EvaluatorStorageBase",
"FileEvaluatorStorage",
]
---- _evaluator_storage_base.py ----
# -*- coding: utf-8 -*-
"""The evaluator storage base class for storing solution and evaluation
results."""
from abc import abstractmethod
from typing import Any, Callable
from .._metric_base import MetricResult
from .._solution import SolutionOutput
from ...agent import AgentBase
class EvaluatorStorageBase:
"""Used to store the solution results and evaluation results to support
resuming the evaluation process"""
@abstractmethod
def save_solution_result(
self,
task_id: str,
repeat_id: str,
output: SolutionOutput,
**kwargs: Any,
) -> None:
"""Save the solution result.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
output (`SolutionOutput`):
The solution output to be saved.
"""
@abstractmethod
def get_evaluation_result(
self,
task_id: str,
repeat_id: str,
metric_name: str,
) -> MetricResult:
"""Get the evaluation result by the given task id and repeat id
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
metric_name (`str`):
The metric name.
Returns:
`MetricResult`:
The evaluation result for the given task and repeat ID.
"""
@abstractmethod
def save_evaluation_result(
self,
task_id: str,
repeat_id: str,
evaluation: MetricResult,
**kwargs: Any,
) -> None:
"""Save the evaluation result.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
evaluation (`MetricResult`):
The evaluation result to be saved.
"""
@abstractmethod
def get_solution_result(
self,
task_id: str,
repeat_id: str,
**kwargs: Any,
) -> SolutionOutput:
"""Get the solution result for the given task and repeat id.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
Returns:
`SolutionOutput`:
The solution output for the given task and repeat ID.
"""
@abstractmethod
def solution_result_exists(self, task_id: str, repeat_id: str) -> bool:
"""Check if the solution for the given task and repeat is finished.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
Returns:
`bool`:
True if the solution result file exists, False otherwise.
"""
@abstractmethod
def evaluation_result_exists(
self,
task_id: str,
repeat_id: str,
metric_name: str,
) -> bool:
"""Check if the evaluation result for the given solution and metric
is finished.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
metric_name (`str`):
The name of the metric.
Returns:
`bool`:
True if the evaluation result file exists, False otherwise.
"""
@abstractmethod
def save_aggregation_result(
self,
aggregation_result: dict,
**kwargs: Any,
) -> None:
"""Save the aggregation result.
Args:
aggregation_result (`dict`):
A dictionary containing the aggregation result.
"""
@abstractmethod
def aggregation_result_exists(
self,
**kwargs: Any,
) -> bool:
"""Check if the aggregation result exists
Returns:
`bool`:
`True` if the aggregation result file exists.
"""
@abstractmethod
def save_evaluation_meta(self, meta_info: dict) -> None:
"""Save the evaluation meta information.
Args:
meta_info (`dict`):
A dictionary containing the meta information.
"""
@abstractmethod
def get_agent_pre_print_hook(
self,
task_id: str,
repeat_id: str,
) -> Callable[[AgentBase, dict], None]:
"""Get a pre-print hook function for the agent to save the agent
printing in the evaluation storage.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
Returns:
`Callable[[AgentBase, dict], None]`:
A hook function that takes an `AgentBase` instance and a
keyword arguments dictionary as input, saving the agent's
printing Msg into the evaluation storage.
"""
---- _file_evaluator_storage.py ----
# -*- coding: utf-8 -*-
"""A file system based evaluator storage."""
import json
import os
from json import JSONDecodeError
from typing import Any, Callable
from ._evaluator_storage_base import EvaluatorStorageBase
from .._solution import SolutionOutput
from .._metric_base import MetricResult
from ...agent import AgentBase
from ...message import Msg
class FileEvaluatorStorage(EvaluatorStorageBase):
"""File system based evaluator storage, providing methods to save and
retrieve evaluation results. So that the evaluation process can be resumed
from the last saved state.
The files are organized in a directory structure:
- save_dir/
- evaluation_result.json
- evaluation_meta.json
- {task_id}/
- {repeat_id}/
- solution.json
- evaluation/
- {metric_name}.json
"""
SOLUTION_FILE_NAME = "solution.json"
EVALUATION_DIR_NAME = "evaluation"
EVALUATION_RESULT_FILE = "evaluation_result.json"
EVALUATION_META_FILE = "evaluation_meta.json"
AGENT_PRINTING_LOG = "logging.txt"
def __init__(self, save_dir: str) -> None:
"""Initialize the file evaluator storage."""
self.save_dir = save_dir
def _get_save_path(self, task_id: str, repeat_id: str, *args: str) -> str:
"""Get the save path for a given task and repeat ID."""
return os.path.join(self.save_dir, repeat_id, task_id, *args)
def save_solution_result(
self,
task_id: str,
repeat_id: str,
output: SolutionOutput,
**kwargs: Any,
) -> None:
"""Save the solution result.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
output (`SolutionOutput`):
The solution output to be saved.
"""
path_file = self._get_save_path(
task_id,
repeat_id,
self.SOLUTION_FILE_NAME,
)
os.makedirs(os.path.dirname(path_file), exist_ok=True)
with open(path_file, "w", encoding="utf-8") as f:
json.dump(output, f, ensure_ascii=False, indent=4)
def save_evaluation_result(
self,
task_id: str,
repeat_id: str,
evaluation: MetricResult,
**kwargs: Any,
) -> None:
"""Save the evaluation result.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
evaluation (`MetricResult`):
The evaluation result to be saved.
"""
path_file = self._get_save_path(
task_id,
repeat_id,
self.EVALUATION_DIR_NAME,
f"{evaluation.name}.json",
)
os.makedirs(os.path.dirname(path_file), exist_ok=True)
with open(path_file, "w", encoding="utf-8") as f:
json.dump(evaluation, f, ensure_ascii=False, indent=4)
def get_evaluation_result(
self,
task_id: str,
repeat_id: str,
metric_name: str,
) -> MetricResult:
"""Get the evaluation result by the given task id and repeat id
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
metric_name (`str`):
The metric name.
Returns:
`MetricResult`:
The evaluation result for the given task and repeat ID.
"""
path_file = self._get_save_path(
task_id,
repeat_id,
self.EVALUATION_DIR_NAME,
f"{metric_name}.json",
)
if not os.path.exists(path_file):
raise FileNotFoundError(path_file)
with open(path_file, "r", encoding="utf-8") as f:
evaluation = json.load(f)
return MetricResult(**evaluation)
def get_solution_result(
self,
task_id: str,
repeat_id: str,
**kwargs: Any,
) -> SolutionOutput:
"""Get the solution result for the given task and repeat id from the
file system.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
Raises:
`FileNotFoundError`:
If the solution result file does not exist for the given task
and repeat ID.
Returns:
`SolutionOutput`:
The solution output for the given task and repeat ID.
"""
path_file = self._get_save_path(
task_id,
repeat_id,
self.SOLUTION_FILE_NAME,
)
if not os.path.exists(path_file):
raise FileNotFoundError(
f"Solution result for task {task_id} and repeat {repeat_id} "
"not found.",
)
try:
with open(path_file, "r", encoding="utf-8") as f:
solution_data = json.load(f)
except JSONDecodeError as e:
raise JSONDecodeError(
f"Failed to load JSON from {path_file}: {e.msg}",
e.doc,
e.pos,
) from e
return SolutionOutput(**solution_data)
def solution_result_exists(self, task_id: str, repeat_id: str) -> bool:
"""Check if the solution for the given task and repeat is finished.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
Returns:
`bool`:
True if the solution result file exists, False otherwise.
"""
path_file = self._get_save_path(
task_id,
repeat_id,
self.SOLUTION_FILE_NAME,
)
return os.path.exists(path_file) and os.path.getsize(path_file) > 0
def evaluation_result_exists(
self,
task_id: str,
repeat_id: str,
metric_name: str,
) -> bool:
"""Check if the evaluation result for the given solution and metric
is finished.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
metric_name (`str`):
The name of the metric.
Returns:
`bool`:
True if the evaluation result file exists, False otherwise.
"""
path_file = self._get_save_path(
task_id,
repeat_id,
self.EVALUATION_DIR_NAME,
f"{metric_name}.json",
)
return os.path.exists(path_file) and os.path.getsize(path_file) > 0
def save_aggregation_result(
self,
aggregation_result: dict,
**kwargs: Any,
) -> None:
"""Save the aggregation result.
Args:
aggregation_result (`dict`):
A dictionary containing the aggregation result.
"""
path_file = os.path.join(
self.save_dir,
self.EVALUATION_RESULT_FILE,
)
os.makedirs(os.path.dirname(path_file), exist_ok=True)
with open(path_file, "w", encoding="utf-8") as f:
json.dump(aggregation_result, f, ensure_ascii=False, indent=4)
def aggregation_result_exists(
self,
**kwargs: Any,
) -> bool:
"""Check if the aggregation result exists
Returns:
`bool`:
`True` if the aggregation result file exists.
"""
path_file = os.path.join(
self.save_dir,
self.EVALUATION_RESULT_FILE,
)
return os.path.exists(path_file) and os.path.getsize(path_file) > 0
def save_evaluation_meta(self, meta_info: dict) -> None:
"""Save the evaluation meta information.
Args:
meta_info (`dict`):
A dictionary containing the meta information.
"""
path_file = os.path.join(
self.save_dir,
self.EVALUATION_META_FILE,
)
os.makedirs(os.path.dirname(path_file), exist_ok=True)
with open(path_file, "w", encoding="utf-8") as f:
json.dump(meta_info, f, ensure_ascii=False, indent=4)
def get_agent_pre_print_hook(
self,
task_id: str,
repeat_id: str,
) -> Callable[[AgentBase, dict], None]:
"""Get a pre-print hook function for the agent to save the agent
printing in the evaluation storage.
Args:
task_id (`str`):
The task ID.
repeat_id (`str`):
The repeat ID for the task, usually the index of the repeat
evaluation.
Returns:
`Callable[[AgentBase, dict], None]`:
A hook function that takes an `AgentBase` instance and a
keyword arguments dictionary as input, saving the agent's
printing Msg into the evaluation storage.
"""
def pre_print_hook(_agent: AgentBase, kwargs: dict) -> None:
"""Hook function to save agent's printing."""
msg: Msg | None = kwargs.get("msg", None)
last: bool = kwargs.get("last", False)
if msg is None or not last:
return
# Only save the last message
printing_str = []
for block in msg.get_content_blocks():
match block["type"]:
case "text":
printing_str.append(
f"{msg.name}: {block['text']}",
)
case "thinking":
printing_str.append(
f"{msg.name} (thinking): {block['text']}",
)
case _:
block_str = json.dumps(
block,
ensure_ascii=False,
indent=4,
)
if printing_str:
printing_str.append(block_str)
else:
printing_str.append(f"{msg.name}: {block_str}")
path_file = self._get_save_path(
task_id,
repeat_id,
self.AGENT_PRINTING_LOG,
)
os.makedirs(os.path.dirname(path_file), exist_ok=True)
with open(path_file, "a", encoding="utf-8") as f:
f.write("\n".join(printing_str) + "\n")
return pre_print_hook
==== exception ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The exception module in agentscope."""
from ._exception_base import AgentOrientedExceptionBase
from ._tool import (
ToolInterruptedError,
ToolNotFoundError,
ToolInvalidArgumentsError,
)
__all__ = [
"AgentOrientedExceptionBase",
"ToolInterruptedError",
"ToolNotFoundError",
"ToolInvalidArgumentsError",
]
---- _exception_base.py ----
# -*- coding: utf-8 -*-
"""The base exception class in agentscope."""
class AgentOrientedExceptionBase(Exception):
"""The base class for all agent-oriented exceptions. These exceptions are
expect to the captured and exposed to the agent during runtime, so that
agents can handle the error appropriately during the runtime.
"""
def __init__(self, message: str):
"""Initialize the exception with a message."""
super().__init__(message)
self.message = message
def __str__(self) -> str:
"""Return the string representation of the exception."""
return f"{self.__class__.__name__}: {self.message}"
---- _tool.py ----
# -*- coding: utf-8 -*-
"""The tool-related exceptions in agentscope."""
from ._exception_base import AgentOrientedExceptionBase
class ToolNotFoundError(AgentOrientedExceptionBase):
"""Exception raised when a tool was not found."""
class ToolInterruptedError(AgentOrientedExceptionBase):
"""Exception raised when a tool calling was interrupted by the user."""
class ToolInvalidArgumentsError(AgentOrientedExceptionBase):
"""Exception raised when the arguments passed to a tool are invalid."""
==== formatter ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The formatter module in agentscope."""
from ._formatter_base import FormatterBase
from ._truncated_formatter_base import TruncatedFormatterBase
from ._dashscope_formatter import (
DashScopeChatFormatter,
DashScopeMultiAgentFormatter,
)
from ._anthropic_formatter import (
AnthropicChatFormatter,
AnthropicMultiAgentFormatter,
)
from ._openai_formatter import (
OpenAIChatFormatter,
OpenAIMultiAgentFormatter,
)
from ._gemini_formatter import (
GeminiChatFormatter,
GeminiMultiAgentFormatter,
)
from ._ollama_formatter import (
OllamaChatFormatter,
OllamaMultiAgentFormatter,
)
from ._deepseek_formatter import (
DeepSeekChatFormatter,
DeepSeekMultiAgentFormatter,
)
__all__ = [
"FormatterBase",
"TruncatedFormatterBase",
"DashScopeChatFormatter",
"DashScopeMultiAgentFormatter",
"OpenAIChatFormatter",
"OpenAIMultiAgentFormatter",
"AnthropicChatFormatter",
"AnthropicMultiAgentFormatter",
"GeminiChatFormatter",
"GeminiMultiAgentFormatter",
"OllamaChatFormatter",
"OllamaMultiAgentFormatter",
"DeepSeekChatFormatter",
"DeepSeekMultiAgentFormatter",
]
---- _anthropic_formatter.py ----
# -*- coding: utf-8 -*-
# pylint: disable=too-many-branches
"""The Anthropic formatter module."""
from typing import Any
from ._truncated_formatter_base import TruncatedFormatterBase
from .._logging import logger
from ..message import Msg, TextBlock, ImageBlock, ToolUseBlock, ToolResultBlock
from ..token import TokenCounterBase
class AnthropicChatFormatter(TruncatedFormatterBase):
"""Formatter for Anthropic messages."""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = False
"""Whether support multi-agent conversations"""
support_vision: bool = True
"""Whether support vision data"""
supported_blocks: list[type] = [
TextBlock,
# Multimodal
ImageBlock,
# Tool use
ToolUseBlock,
ToolResultBlock,
]
"""The list of supported message blocks"""
async def _format(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Format message objects into Anthropic API format.
Args:
msgs (`list[Msg]`):
The list of message objects to format.
Returns:
`list[dict[str, Any]]`:
The formatted messages as a list of dictionaries.
.. note:: Anthropic suggests always passing all previous thinking
blocks back to the API in subsequent calls to maintain reasoning
continuity. For more details, please refer to
`Anthropic's documentation
<https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#preserving-thinking-blocks>`_.
"""
self.assert_list_of_msgs(msgs)
messages: list[dict] = []
for index, msg in enumerate(msgs):
content_blocks = []
for block in msg.get_content_blocks():
typ = block.get("type")
if typ in ["thinking", "text", "image"]:
content_blocks.append({**block})
elif typ == "tool_use":
content_blocks.append(
{
"id": block.get("id"),
"type": "tool_use",
"name": block.get("name"),
"input": block.get("input", {}),
},
)
elif typ == "tool_result":
output = block.get("output")
if output is None:
content_value = [{"type": "text", "text": None}]
elif isinstance(output, list):
content_value = output
else:
content_value = [{"type": "text", "text": str(output)}]
messages.append(
{
"role": "user",
"content": [
{
"type": "tool_result",
"tool_use_id": block.get("id"),
"content": content_value,
},
],
},
)
else:
logger.warning(
"Unsupported block type %s in the message, skipped.",
typ,
)
# Claude only allow the first message to be system message
if msg.role == "system" and index != 0:
role = "user"
else:
role = msg.role
msg_anthropic = {
"role": role,
"content": content_blocks or None,
}
# When both content and tool_calls are None, skipped
if msg_anthropic["content"] or msg_anthropic.get("tool_calls"):
messages.append(msg_anthropic)
return messages
class AnthropicMultiAgentFormatter(TruncatedFormatterBase):
"""
Anthropic formatter for multi-agent conversations, where more than
a user and an agent are involved.
"""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = True
"""Whether support multi-agent conversations"""
support_vision: bool = True
"""Whether support vision data"""
supported_blocks: list[type] = [
TextBlock,
# Multimodal
ImageBlock,
# Tool use
ToolUseBlock,
ToolResultBlock,
]
"""The list of supported message blocks"""
def __init__(
self,
conversation_history_prompt: str = (
"# Conversation History\n"
"The content between <history></history> tags contains "
"your conversation history\n"
),
token_counter: TokenCounterBase | None = None,
max_tokens: int | None = None,
) -> None:
"""Initialize the DashScope multi-agent formatter.
Args:
conversation_history_prompt (`str`):
The prompt to use for the conversation history section.
"""
super().__init__(token_counter=token_counter, max_tokens=max_tokens)
self.conversation_history_prompt = conversation_history_prompt
async def _format_tool_sequence(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Given a sequence of tool call/result messages, format them into
the required format for the Anthropic API."""
return await AnthropicChatFormatter().format(msgs)
async def _format_agent_message(
self,
msgs: list[Msg],
is_first: bool = True,
) -> list[dict[str, Any]]:
"""Given a sequence of messages without tool calls/results, format
them into the required format for the Anthropic API."""
if is_first:
conversation_history_prompt = self.conversation_history_prompt
else:
conversation_history_prompt = ""
# Format into required Anthropic format
formatted_msgs: list[dict] = []
# Collect the multimodal files
conversation_blocks: list = []
accumulated_text = []
for msg in msgs:
for block in msg.get_content_blocks():
if block["type"] == "text":
accumulated_text.append(f"{msg.name}: {block['text']}")
elif block["type"] == "image":
# Handle the accumulated text as a single block
if accumulated_text:
conversation_blocks.append(
{
"text": "\n".join(accumulated_text),
"type": "text",
},
)
accumulated_text.clear()
conversation_blocks.append({**block})
if accumulated_text:
conversation_blocks.append(
{
"text": "\n".join(accumulated_text),
"type": "text",
},
)
if conversation_blocks:
if conversation_blocks[0].get("text"):
conversation_blocks[0]["text"] = (
conversation_history_prompt
+ "<history>\n"
+ conversation_blocks[0]["text"]
)
else:
conversation_blocks.insert(
0,
{
"type": "text",
"text": conversation_history_prompt + "<history>\n",
},
)
if conversation_blocks[-1].get("text"):
conversation_blocks[-1]["text"] += "\n</history>"
else:
conversation_blocks.append(
{"type": "text", "text": "</history>"},
)
if conversation_blocks:
formatted_msgs.append(
{
"role": "user",
"content": conversation_blocks,
},
)
return formatted_msgs
---- _dashscope_formatter.py ----
# -*- coding: utf-8 -*-
# pylint: disable=too-many-branches
"""The dashscope formatter module."""
import json
import os.path
from typing import Any
from ._truncated_formatter_base import TruncatedFormatterBase
from .._logging import logger
from .._utils._common import _is_accessible_local_file
from ..message import (
Msg,
TextBlock,
ImageBlock,
AudioBlock,
ToolUseBlock,
ToolResultBlock,
)
from ..token import TokenCounterBase
def _reformat_messages(
messages: list[dict[str, Any]],
) -> list[dict[str, Any]]:
"""Reformat the content to be compatible with HuggingFaceTokenCounter.
This function processes a list of messages and converts multi-part
text content into single string content when all parts are plain text.
This is necessary for compatibility with HuggingFaceTokenCounter which
expects simple string content rather than structured content with
multiple parts.
Args:
messages (list[dict[str, Any]]):
A list of message dictionaries where each message may contain a
"content" field. The content can be either:
- A string (unchanged)
- A list of content items, where each item is a dict that may
contain "text", "type", and other fields
Returns:
list[dict[str, Any]]:
A list of reformatted messages. For messages where all content
items are plain text (have "text" field and either no "type"
field or "type" == "text"), the content list is converted to a
single newline-joined string. Other messages remain unchanged.
Example:
.. code-block:: python
# Case 1: All text content - will be converted
messages = [
{
"role": "user",
"content": [
{"text": "Hello", "type": "text"},
{"text": "World", "type": "text"}
]
}
]
result = _reformat_messages(messages)
print(result[0]["content"])
# Output: "Hello\nWorld"
# Case 2: Mixed content - will remain unchanged
messages = [
{
"role": "user",
"content": [
{"text": "Hello", "type": "text"},
{"image_url": "...", "type": "image"}
]
}
]
result = _reformat_messages(messages) # remain unchanged
print(type(result[0]["content"]))
# Output: <class 'list'>
"""
for message in messages:
content = message.get("content", [])
is_all_text = True
texts = []
for item in content:
if not isinstance(item, dict) or "text" not in item:
is_all_text = False
break
if "type" in item and item["type"] != "text":
is_all_text = False
break
if item["text"]:
texts.append(item["text"])
if is_all_text and texts:
message["content"] = "\n".join(texts)
return messages
class DashScopeChatFormatter(TruncatedFormatterBase):
"""Formatter for DashScope messages."""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = False
"""Whether support multi-agent conversations"""
support_vision: bool = True
"""Whether support vision data"""
supported_blocks: list[type] = [
TextBlock,
ImageBlock,
AudioBlock,
ToolUseBlock,
ToolResultBlock,
]
async def _format(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Format message objects into DashScope API format.
Args:
msgs (`list[Msg]`):
The list of message objects to format.
Returns:
`list[dict[str, Any]]`:
The formatted messages as a list of dictionaries.
"""
self.assert_list_of_msgs(msgs)
formatted_msgs: list[dict] = []
for msg in msgs:
content_blocks = []
tool_calls = []
for block in msg.get_content_blocks():
typ = block.get("type")
if typ == "text":
content_blocks.append(
{
"text": block.get("text"),
},
)
elif typ in ["image", "audio"]:
source = block["source"]
if source["type"] == "url":
url = source["url"]
if _is_accessible_local_file(url):
content_blocks.append(
{typ: "file://" + os.path.abspath(url)},
)
else:
# treat as web url
content_blocks.append({typ: url})
elif source["type"] == "base64":
media_type = source["media_type"]
base64_data = source["data"]
content_blocks.append(
{typ: f"data:{media_type};base64,{base64_data}"},
)
else:
raise NotImplementedError(
f"Unsupported source type '{source.get('type')}' "
f"for {typ} block.",
)
elif typ == "tool_use":
tool_calls.append(
{
"id": block.get("id"),
"type": "function",
"function": {
"name": block.get("name"),
"arguments": json.dumps(
block.get("input", {}),
ensure_ascii=False,
),
},
},
)
elif typ == "tool_result":
formatted_msgs.append(
{
"role": "tool",
"tool_call_id": block.get("id"),
"content": self.convert_tool_result_to_string(
block.get("output"), # type: ignore[arg-type]
),
"name": block.get("name"),
},
)
else:
logger.warning(
"Unsupported block type %s in the message, skipped.",
typ,
)
msg_dashscope = {
"role": msg.role,
"content": content_blocks or [{"text": None}],
}
if tool_calls:
msg_dashscope["tool_calls"] = tool_calls
if msg_dashscope["content"] != [
{"text": None},
] or msg_dashscope.get(
"tool_calls",
):
formatted_msgs.append(msg_dashscope)
return _reformat_messages(formatted_msgs)
class DashScopeMultiAgentFormatter(TruncatedFormatterBase):
"""DashScope formatter for multi-agent conversations, where more than
a user and an agent are involved.
.. note:: This formatter will combine previous messages (except tool
calls/results) into a history section in the first system message with
the conversation history prompt.
.. note:: For tool calls/results, they will be presented as separate
messages as required by the DashScope API. Therefore, the tool calls/
results messages are expected to be placed at the end of the input
messages.
.. tip:: Telling the assistant's name in the system prompt is very
important in multi-agent conversations. So that LLM can know who it
is playing as.
"""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = True
"""Whether support multi-agent conversations"""
support_vision: bool = True
"""Whether support vision data"""
supported_blocks: list[type] = [
TextBlock,
# Multimodal
ImageBlock,
AudioBlock,
# Tool use
ToolUseBlock,
ToolResultBlock,
]
"""The list of supported message blocks"""
def __init__(
self,
conversation_history_prompt: str = (
"# Conversation History\n"
"The content between <history></history> tags contains "
"your conversation history\n"
),
token_counter: TokenCounterBase | None = None,
max_tokens: int | None = None,
) -> None:
"""Initialize the DashScope multi-agent formatter.
Args:
conversation_history_prompt (`str`):
The prompt to use for the conversation history section.
token_counter (`TokenCounterBase | None`, optional):
The token counter used for truncation.
max_tokens (`int | None`, optional):
The maximum number of tokens allowed in the formatted
messages. If `None`, no truncation will be applied.
"""
super().__init__(token_counter=token_counter, max_tokens=max_tokens)
self.conversation_history_prompt = conversation_history_prompt
async def _format_tool_sequence(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Given a sequence of tool call/result messages, format them into
the required format for the DashScope API.
Args:
msgs (`list[Msg]`):
The list of messages containing tool calls/results to format.
Returns:
`list[dict[str, Any]]`:
A list of dictionaries formatted for the DashScope API.
"""
return await DashScopeChatFormatter().format(msgs)
async def _format_agent_message(
self,
msgs: list[Msg],
is_first: bool = True,
) -> list[dict[str, Any]]:
"""Given a sequence of messages without tool calls/results, format
them into a user message with conversation history tags. For the
first agent message, it will include the conversation history prompt.
Args:
msgs (`list[Msg]`):
A list of Msg objects to be formatted.
is_first (`bool`, defaults to `True`):
Whether this is the first agent message in the conversation.
If `True`, the conversation history prompt will be included.
Returns:
`list[dict[str, Any]]`:
A list of dictionaries formatted for the DashScope API.
"""
if is_first:
conversation_history_prompt = self.conversation_history_prompt
else:
conversation_history_prompt = ""
# Format into required DashScope format
formatted_msgs: list[dict] = []
# Collect the multimodal files
conversation_blocks = []
accumulated_text = []
for msg in msgs:
for block in msg.get_content_blocks():
if block["type"] == "text":
accumulated_text.append(f"{msg.name}: {block['text']}")
elif block["type"] in ["image", "audio"]:
# Handle the accumulated text as a single block
if accumulated_text:
conversation_blocks.append(
{"text": "\n".join(accumulated_text)},
)
accumulated_text.clear()
if block["source"]["type"] == "url":
url = block["source"]["url"]
if _is_accessible_local_file(url):
conversation_blocks.append(
{
block["type"]: "file://"
+ os.path.abspath(url),
},
)
else:
conversation_blocks.append({block["type"]: url})
elif block["source"]["type"] == "base64":
media_type = block["source"]["media_type"]
base64_data = block["source"]["data"]
conversation_blocks.append(
{
block[
"type"
]: f"data:{media_type};base64,{base64_data}",
},
)
else:
logger.warning(
"Unsupported block type %s in the message, "
"skipped.",
block["type"],
)
if accumulated_text:
conversation_blocks.append({"text": "\n".join(accumulated_text)})
if conversation_blocks:
if conversation_blocks[0].get("text"):
conversation_blocks[0]["text"] = (
conversation_history_prompt
+ "<history>\n"
+ conversation_blocks[0]["text"]
)
else:
conversation_blocks.insert(
0,
{
"text": conversation_history_prompt + "<history>\n",
},
)
if conversation_blocks[-1].get("text"):
conversation_blocks[-1]["text"] += "\n</history>"
else:
conversation_blocks.append({"text": "</history>"})
formatted_msgs.append(
{
"role": "user",
"content": conversation_blocks,
},
)
return _reformat_messages(formatted_msgs)
async def _format_system_message(
self,
msg: Msg,
) -> dict[str, Any]:
"""Format system message for DashScope API."""
return {
"role": "system",
"content": msg.get_text_content(),
}
---- _deepseek_formatter.py ----
# -*- coding: utf-8 -*-
# pylint: disable=too-many-branches
"""The DeepSeek formatter module."""
import json
from typing import Any
from ._truncated_formatter_base import TruncatedFormatterBase
from .._logging import logger
from ..message import Msg, TextBlock, ToolUseBlock, ToolResultBlock
from ..token import TokenCounterBase
class DeepSeekChatFormatter(TruncatedFormatterBase):
"""Formatter for DeepSeek messages."""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = False
"""Whether support multi-agent conversations"""
support_vision: bool = False
"""Whether support vision data"""
supported_blocks: list[type] = [
TextBlock,
# Tool use
ToolUseBlock,
ToolResultBlock,
]
"""The list of supported message blocks"""
async def _format(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Format message objects into DeepSeek API format.
Args:
msgs (`list[Msg]`):
The list of message objects to format.
Returns:
`list[dict[str, Any]]`:
The formatted messages as a list of dictionaries.
"""
self.assert_list_of_msgs(msgs)
messages: list[dict] = []
for msg in msgs:
content_blocks: list = []
tool_calls = []
for block in msg.get_content_blocks():
typ = block.get("type")
if typ == "text":
content_blocks.append({**block})
elif typ == "tool_use":
tool_calls.append(
{
"id": block.get("id"),
"type": "function",
"function": {
"name": block.get("name"),
"arguments": json.dumps(
block.get("input", {}),
ensure_ascii=False,
),
},
},
)
elif typ == "tool_result":
messages.append(
{
"role": "tool",
"tool_call_id": block.get("id"),
"content": self.convert_tool_result_to_string(
block.get("output"), # type: ignore[arg-type]
),
"name": block.get("name"),
},
)
else:
logger.warning(
"Unsupported block type %s in the message, skipped.",
typ,
)
content_msg = "\n".join(
content.get("text", "") for content in content_blocks
)
msg_deepseek = {
"role": msg.role,
"content": content_msg or None,
}
if tool_calls:
msg_deepseek["tool_calls"] = tool_calls
if msg_deepseek["content"] or msg_deepseek.get("tool_calls"):
messages.append(msg_deepseek)
return messages
class DeepSeekMultiAgentFormatter(TruncatedFormatterBase):
"""
DeepSeek formatter for multi-agent conversations, where more than
a user and an agent are involved.
"""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = True
"""Whether support multi-agent conversations"""
support_vision: bool = False
"""Whether support vision data"""
supported_blocks: list[type] = [
TextBlock,
# Tool use
ToolUseBlock,
ToolResultBlock,
]
"""The list of supported message blocks"""
def __init__(
self,
conversation_history_prompt: str = (
"# Conversation History\n"
"The content between <history></history> tags contains "
"your conversation history\n"
),
token_counter: TokenCounterBase | None = None,
max_tokens: int | None = None,
) -> None:
"""Initialize the DeepSeek multi-agent formatter.
Args:
conversation_history_prompt (`str`):
The prompt to use for the conversation history section.
token_counter (`TokenCounterBase | None`, optional):
A token counter instance used to count tokens in the messages.
If not provided, the formatter will format the messages
without considering token limits.
max_tokens (`int | None`, optional):
The maximum number of tokens allowed in the formatted
messages. If not provided, the formatter will not truncate
the messages.
"""
super().__init__(token_counter=token_counter, max_tokens=max_tokens)
self.conversation_history_prompt = conversation_history_prompt
async def _format_tool_sequence(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Given a sequence of tool call/result messages, format them into
the required format for the DeepSeek API.
Args:
msgs (`list[Msg]`):
The list of messages containing tool calls/results to format.
Returns:
`list[dict[str, Any]]`:
A list of dictionaries formatted for the DeepSeek API.
"""
return await DeepSeekChatFormatter().format(msgs)
async def _format_agent_message(
self,
msgs: list[Msg],
is_first: bool = True,
) -> list[dict[str, Any]]:
"""Given a sequence of messages without tool calls/results, format
them into the required format for the DeepSeek API.
Args:
msgs (`list[Msg]`):
A list of Msg objects to be formatted.
is_first (`bool`, defaults to `True`):
Whether this is the first agent message in the conversation.
If `True`, the conversation history prompt will be included.
Returns:
`list[dict[str, Any]]`:
A list of dictionaries formatted for the DeepSeek API.
"""
if is_first:
conversation_history_prompt = self.conversation_history_prompt
else:
conversation_history_prompt = ""
# Format into required DeepSeek format
formatted_msgs: list[dict] = []
conversation_blocks: list = []
accumulated_text = []
for msg in msgs:
for block in msg.get_content_blocks():
if block["type"] == "text":
accumulated_text.append(f"{msg.name}: {block['text']}")
if accumulated_text:
conversation_blocks.append(
{"text": "\n".join(accumulated_text)},
)
if conversation_blocks:
if conversation_blocks[0].get("text"):
conversation_blocks[0]["text"] = (
conversation_history_prompt
+ "<history>\n"
+ conversation_blocks[0]["text"]
)
else:
conversation_blocks.insert(
0,
{
"text": conversation_history_prompt + "<history>\n",
},
)
if conversation_blocks[-1].get("text"):
conversation_blocks[-1]["text"] += "\n</history>"
else:
conversation_blocks.append({"text": "</history>"})
conversation_blocks_text = "\n".join(
conversation_block.get("text", "")
for conversation_block in conversation_blocks
)
user_message = {
"role": "user",
"content": conversation_blocks_text,
}
if conversation_blocks:
formatted_msgs.append(user_message)
return formatted_msgs
---- _formatter_base.py ----
# -*- coding: utf-8 -*-
"""The formatter module."""
from abc import abstractmethod
from typing import Any, List
from .._utils._common import _save_base64_data
from ..message import Msg, AudioBlock, ImageBlock, TextBlock
class FormatterBase:
"""The base class for formatters."""
@abstractmethod
async def format(self, *args: Any, **kwargs: Any) -> list[dict[str, Any]]:
"""Format the Msg objects to a list of dictionaries that satisfy the
API requirements."""
@staticmethod
def assert_list_of_msgs(msgs: list[Msg]) -> None:
"""Assert that the input is a list of Msg objects.
Args:
msgs (`list[Msg]`):
A list of Msg objects to be validated.
"""
if not isinstance(msgs, list):
raise TypeError("Input must be a list of Msg objects.")
for msg in msgs:
if not isinstance(msg, Msg):
raise TypeError(
f"Expected Msg object, got {type(msg)} instead.",
)
@staticmethod
def convert_tool_result_to_string(
output: str | List[TextBlock | ImageBlock | AudioBlock],
) -> str:
"""Turn the tool result list into a textual output to be compatible
with the LLM API that doesn't support multimodal data.
Args:
output (`str | List[TextBlock | ImageBlock | AudioBlock]`):
The output of the tool response, including text and multimodal
data like images and audio.
Returns:
`str`:
A string representation of the tool result, with text blocks
concatenated and multimodal data represented by file paths
or URLs.
"""
if isinstance(output, str):
return output
textual_output = []
for block in output:
assert isinstance(block, dict) and "type" in block, (
f"Invalid block: {block}, a TextBlock, ImageBlock, or "
f"AudioBlock is expected."
)
if block["type"] == "text":
textual_output.append(block["text"])
elif block["type"] in ["image", "audio", "video"]:
assert "source" in block, (
f"Invalid {block['type']} block: {block}, 'source' key "
"is required."
)
source = block["source"]
# Save the image locally and return the file path
if source["type"] == "url":
textual_output.append(
f"The returned {block['type']} can be found "
f"at: {source['url']}",
)
elif source["type"] == "base64":
path_temp_file = _save_base64_data(
source["media_type"],
source["data"],
)
textual_output.append(
f"The returned {block['type']} can be found "
f"at: {path_temp_file}",
)
else:
raise ValueError(
f"Invalid image source: {block['source']}, "
"expected 'url' or 'base64'.",
)
else:
raise ValueError(
f"Unsupported block type: {block['type']}, "
"expected 'text', 'image', 'audio', or 'video'.",
)
if len(textual_output) == 1:
return textual_output[0]
else:
return "\n".join("- " + _ for _ in textual_output)
---- _gemini_formatter.py ----
# -*- coding: utf-8 -*-
# pylint: disable=too-many-branches
"""Google gemini API formatter in agentscope."""
import base64
import os
from typing import Any
from urllib.parse import urlparse
from ._truncated_formatter_base import TruncatedFormatterBase
from .._utils._common import _get_bytes_from_web_url
from ..message import (
Msg,
TextBlock,
ImageBlock,
AudioBlock,
ToolUseBlock,
ToolResultBlock,
VideoBlock,
)
from .._logging import logger
from ..token import TokenCounterBase
def _to_gemini_inline_data(url: str) -> dict:
"""Convert url into the Gemini API required format."""
parsed_url = urlparse(url)
extension = url.split(".")[-1].lower()
# Pre-calculate media type from extension (image/audio/video).
typ = None
for k, v in GeminiChatFormatter.supported_extensions.items():
if extension in v:
typ = k
break
if not os.path.exists(url) and parsed_url.scheme != "":
# Web url
if typ is None:
raise TypeError(
f"Unsupported file extension: {extension}, expected "
f"{GeminiChatFormatter.supported_extensions}",
)
data = _get_bytes_from_web_url(url)
return {
"data": data,
"mime_type": f"{typ}/{extension}",
}
elif os.path.exists(url):
# Local file
if typ is None:
raise TypeError(
f"Unsupported file extension: {extension}, expected "
f"{GeminiChatFormatter.supported_extensions}",
)
with open(url, "rb") as f:
data = base64.b64encode(f.read()).decode("utf-8")
return {
"data": data,
"mime_type": f"{typ}/{extension}",
}
raise ValueError(
f"The URL `{url}` is not a valid image URL or local file.",
)
class GeminiChatFormatter(TruncatedFormatterBase):
"""The formatter for Google Gemini API."""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = False
"""Whether support multi-agent conversations"""
support_vision: bool = True
"""Whether support vision data"""
supported_blocks: list[type] = [
TextBlock,
# Multimodal
ImageBlock,
VideoBlock,
AudioBlock,
# Tool use
ToolUseBlock,
ToolResultBlock,
]
"""The list of supported message blocks"""
supported_extensions: dict[str, list[str]] = {
"image": ["png", "jpeg", "webp", "heic", "heif"],
"video": [
"mp4",
"mpeg",
"mov",
"avi",
"x-flv",
"mpg",
"webm",
"wmv",
"3gpp",
],
"audio": ["mp3", "wav", "aiff", "aac", "ogg", "flac"],
}
async def _format(
self,
msgs: list[Msg],
) -> list[dict]:
"""Format message objects into Gemini API required format."""
self.assert_list_of_msgs(msgs)
messages: list = []
for msg in msgs:
parts = []
for block in msg.get_content_blocks():
typ = block.get("type")
if typ == "text":
parts.append(
{
"text": block.get("text"),
},
)
elif typ == "tool_use":
parts.append(
{
"function_call": {
"id": block["id"],
"name": block["name"],
"args": block["input"],
},
},
)
elif typ == "tool_result":
text_output = self.convert_tool_result_to_string(
block["output"], # type: ignore[arg-type]
)
messages.append(
{
"role": "user",
"parts": [
{
"function_response": {
"id": block["id"],
"name": block["name"],
"response": {
"output": text_output,
},
},
},
],
},
)
elif typ in ["image", "audio", "video"]:
if block["source"]["type"] == "base64":
media_type = block["source"]["media_type"]
base64_data = block["source"]["data"]
parts.append(
{
"inline_data": {
"data": base64_data,
"mime_type": media_type,
},
},
)
elif block["source"]["type"] == "url":
parts.append(
{
"inline_data": _to_gemini_inline_data(
block["source"]["url"],
),
},
)
else:
logger.warning(
"Unsupported block type: %s in the message, skipped. ",
typ,
)
role = "model" if msg.role == "assistant" else "user"
if parts:
messages.append(
{
"role": role,
"parts": parts,
},
)
return messages
class GeminiMultiAgentFormatter(TruncatedFormatterBase):
"""The multi-agent formatter for Google Gemini API, where more than a
user and an agent are involved.
.. note:: This formatter will combine previous messages (except tool
calls/results) into a history section in the first system message with
the conversation history prompt.
.. note:: For tool calls/results, they will be presented as separate
messages as required by the Gemini API. Therefore, the tool calls/
results messages are expected to be placed at the end of the input
messages.
.. tip:: Telling the assistant's name in the system prompt is very
important in multi-agent conversations. So that LLM can know who it
is playing as.
"""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = True
"""Whether support multi-agent conversations"""
support_vision: bool = True
"""Whether support vision data"""
supported_blocks: list[type] = [
TextBlock,
# Multimodal
ImageBlock,
VideoBlock,
AudioBlock,
# Tool use
ToolUseBlock,
ToolResultBlock,
]
"""The list of supported message blocks"""
def __init__(
self,
conversation_history_prompt: str = (
"# Conversation History\n"
"The content between <history></history> tags contains "
"your conversation history\n"
),
token_counter: TokenCounterBase | None = None,
max_tokens: int | None = None,
) -> None:
"""Initialize the Gemini multi-agent formatter.
Args:
conversation_history_prompt (`str`):
The prompt to be used for the conversation history section.
token_counter (`TokenCounterBase | None`, optional):
The token counter used for truncation.
max_tokens (`int | None`, optional):
The maximum number of tokens allowed in the formatted
messages. If `None`, no truncation will be applied.
"""
super().__init__(token_counter=token_counter, max_tokens=max_tokens)
self.conversation_history_prompt = conversation_history_prompt
async def _format_system_message(
self,
msg: Msg,
) -> dict[str, Any]:
"""Format system message for the Gemini API."""
return {
"role": "user",
"parts": [
{
"text": msg.get_text_content(),
},
],
}
async def _format_tool_sequence(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Given a sequence of tool call/result messages, format them into
the required format for the Gemini API.
Args:
msgs (`list[Msg]`):
The list of messages containing tool calls/results to format.
Returns:
`list[dict[str, Any]]`:
A list of dictionaries formatted for the Gemini API.
"""
return await GeminiChatFormatter().format(msgs)
async def _format_agent_message(
self,
msgs: list[Msg],
is_first: bool = True,
) -> list[dict[str, Any]]:
"""Given a sequence of messages without tool calls/results, format
them into the required format for the Gemini API.
Args:
msgs (`list[Msg]`):
A list of Msg objects to be formatted.
is_first (`bool`, defaults to `True`):
Whether this is the first agent message in the conversation.
If `True`, the conversation history prompt will be included.
Returns:
`list[dict[str, Any]]`:
A list of dictionaries formatted for the Gemini API.
"""
if is_first:
conversation_history_prompt = self.conversation_history_prompt
else:
conversation_history_prompt = ""
# Format into Gemini API required format
formatted_msgs: list = []
# Collect the multimodal files
conversation_parts: list = []
accumulated_text = []
for msg in msgs:
for block in msg.get_content_blocks():
if block["type"] == "text":
accumulated_text.append(f"{msg.name}: {block['text']}")
elif block["type"] in ["image", "video", "audio"]:
# handle the accumulated text as a single part if exists
if accumulated_text:
conversation_parts.append(
{
"text": "\n".join(accumulated_text),
},
)
accumulated_text.clear()
# handle the multimodal data
if block["source"]["type"] == "url":
conversation_parts.append(
{
"inline_data": _to_gemini_inline_data(
block["source"]["url"],
),
},
)
elif block["source"]["type"] == "base64":
media_type = block["source"]["media_type"]
base64_data = block["source"]["data"]
conversation_parts.append(
{
"inline_data": {
"data": base64_data,
"mime_type": media_type,
},
},
)
if accumulated_text:
conversation_parts.append(
{
"text": "\n".join(accumulated_text),
},
)
# Add prompt and <history></history> tags around conversation history
if conversation_parts:
if conversation_parts[0].get("text"):
conversation_parts[0]["text"] = (
conversation_history_prompt
+ "<history>"
+ conversation_parts[0]["text"]
)
else:
conversation_parts.insert(
0,
{"text": conversation_history_prompt + "<history>"},
)
if conversation_parts[-1].get("text"):
conversation_parts[-1]["text"] += "\n</history>"
else:
conversation_parts.append(
{"text": "</history>"},
)
formatted_msgs.append(
{
"role": "user",
"parts": conversation_parts,
},
)
return formatted_msgs
---- _ollama_formatter.py ----
# -*- coding: utf-8 -*-
# pylint: disable=too-many-branches
"""The Ollama formatter module."""
import base64
import os
from typing import Any
from urllib.parse import urlparse
from ._truncated_formatter_base import TruncatedFormatterBase
from .._logging import logger
from .._utils._common import _get_bytes_from_web_url
from ..message import Msg, TextBlock, ImageBlock, ToolUseBlock, ToolResultBlock
from ..token import TokenCounterBase
def _convert_ollama_image_url_to_base64_data(url: str) -> str:
"""Convert image url to base64."""
parsed_url = urlparse(url)
if not os.path.exists(url) and parsed_url.scheme != "":
# Web url
data = _get_bytes_from_web_url(url)
return data
if os.path.exists(url):
# Local file
with open(url, "rb") as f:
data = base64.b64encode(f.read()).decode("utf-8")
return data
raise ValueError(
f"The URL `{url}` is not a valid image URL or local file.",
)
class OllamaChatFormatter(TruncatedFormatterBase):
"""Formatter for Ollama messages."""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = False
"""Whether support multi-agent conversations"""
support_vision: bool = True
"""Whether support vision data"""
supported_blocks: list[type] = [
TextBlock,
# Multimodal
ImageBlock,
# Tool use
ToolUseBlock,
ToolResultBlock,
]
"""The list of supported message blocks"""
async def _format(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Format message objects into Ollama API format.
Args:
msgs (`list[Msg]`):
The list of message objects to format.
Returns:
`list[dict[str, Any]]`:
The formatted messages as a list of dictionaries.
"""
self.assert_list_of_msgs(msgs)
messages: list[dict] = []
for msg in msgs:
content_blocks: list = []
tool_calls = []
images = []
for block in msg.get_content_blocks():
typ = block.get("type")
if typ == "text":
content_blocks.append({**block})
elif typ == "tool_use":
tool_calls.append(
{
"id": block.get("id"),
"type": "function",
"function": {
"name": block.get("name"),
"arguments": block.get("input", {}),
},
},
)
elif typ == "tool_result":
messages.append(
{
"role": "tool",
"tool_call_id": block.get("id"),
"content": self.convert_tool_result_to_string(
block.get("output"), # type: ignore[arg-type]
),
"name": block.get("name"),
},
)
elif typ == "image":
source_type = block["source"]["type"]
if source_type == "url":
images.append(
_convert_ollama_image_url_to_base64_data(
block["source"]["url"],
),
)
elif source_type == "base64":
images.append(block["source"]["data"])
else:
logger.warning(
"Unsupported block type %s in the message, skipped.",
typ,
)
content_msg = "\n".join(
content.get("text", "") for content in content_blocks
)
msg_ollama = {
"role": msg.role,
"content": content_msg or None,
}
if tool_calls:
msg_ollama["tool_calls"] = tool_calls
if images:
msg_ollama["images"] = images
if msg_ollama["content"] or msg_ollama.get("tool_calls"):
messages.append(msg_ollama)
return messages
class OllamaMultiAgentFormatter(TruncatedFormatterBase):
"""
Ollama formatter for multi-agent conversations, where more than
a user and an agent are involved.
"""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = True
"""Whether support multi-agent conversations"""
support_vision: bool = True
"""Whether support vision data"""
supported_blocks: list[type] = [
TextBlock,
# Multimodal
ImageBlock,
# Tool use
ToolUseBlock,
ToolResultBlock,
]
"""The list of supported message blocks"""
def __init__(
self,
conversation_history_prompt: str = (
"# Conversation History\n"
"The content between <history></history> tags contains "
"your conversation history\n"
),
token_counter: TokenCounterBase | None = None,
max_tokens: int | None = None,
) -> None:
"""Initialize the Ollama multi-agent formatter.
Args:
conversation_history_prompt (`str`):
The prompt to use for the conversation history section.
token_counter (`TokenCounterBase | None`, optional):
The token counter used for truncation.
max_tokens (`int | None`, optional):
The maximum number of tokens allowed in the formatted
messages. If `None`, no truncation will be applied.
"""
super().__init__(token_counter=token_counter, max_tokens=max_tokens)
self.conversation_history_prompt = conversation_history_prompt
async def _format_system_message(
self,
msg: Msg,
) -> dict[str, Any]:
"""Format system message for the Ollama API."""
return {
"role": "system",
"content": msg.get_text_content(),
}
async def _format_tool_sequence(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Given a sequence of tool call/result messages, format them into
the required format for the Ollama API.
Args:
msgs (`list[Msg]`):
The list of messages containing tool calls/results to format.
Returns:
`list[dict[str, Any]]`:
A list of dictionaries formatted for the Ollama API.
"""
return await OllamaChatFormatter().format(msgs)
async def _format_agent_message(
self,
msgs: list[Msg],
is_first: bool = True,
) -> list[dict[str, Any]]:
"""Given a sequence of messages without tool calls/results, format
them into the required format for the Ollama API.
Args:
msgs (`list[Msg]`):
A list of Msg objects to be formatted.
is_first (`bool`, defaults to `True`):
Whether this is the first agent message in the conversation.
If `True`, the conversation history prompt will be included.
Returns:
`list[dict[str, Any]]`:
A list of dictionaries formatted for the ollama API.
"""
if is_first:
conversation_history_prompt = self.conversation_history_prompt
else:
conversation_history_prompt = ""
# Format into required Ollama format
formatted_msgs: list[dict] = []
# Collect the multimodal files
conversation_blocks: list = []
accumulated_text = []
images = []
for msg in msgs:
for block in msg.get_content_blocks():
if block["type"] == "text":
accumulated_text.append(f"{msg.name}: {block['text']}")
elif block["type"] == "image":
# Handle the accumulated text as a single block
source = block["source"]
if accumulated_text:
conversation_blocks.append(
{"text": "\n".join(accumulated_text)},
)
accumulated_text.clear()
if source["type"] == "url":
images.append(
_convert_ollama_image_url_to_base64_data(
source["url"],
),
)
elif source["type"] == "base64":
images.append(source["data"])
conversation_blocks.append({**block})
if accumulated_text:
conversation_blocks.append(
{"text": "\n".join(accumulated_text)},
)
if conversation_blocks:
if conversation_blocks[0].get("text"):
conversation_blocks[0]["text"] = (
conversation_history_prompt
+ "<history>\n"
+ conversation_blocks[0]["text"]
)
else:
conversation_blocks.insert(
0,
{
"text": conversation_history_prompt + "<history>\n",
},
)
if conversation_blocks[-1].get("text"):
conversation_blocks[-1]["text"] += "\n</history>"
else:
conversation_blocks.append({"text": "</history>"})
conversation_blocks_text = "\n".join(
conversation_block.get("text", "")
for conversation_block in conversation_blocks
)
user_message = {
"role": "user",
"content": conversation_blocks_text,
}
if images:
user_message["images"] = images
if conversation_blocks:
formatted_msgs.append(user_message)
return formatted_msgs
---- _openai_formatter.py ----
# -*- coding: utf-8 -*-
# pylint: disable=too-many-branches
"""The OpenAI formatter for agentscope."""
import base64
import json
import os
from typing import Any
from urllib.parse import urlparse
import requests
from ._truncated_formatter_base import TruncatedFormatterBase
from .._logging import logger
from ..message import (
Msg,
URLSource,
TextBlock,
ImageBlock,
AudioBlock,
Base64Source,
ToolUseBlock,
ToolResultBlock,
)
from ..token import TokenCounterBase
def _to_openai_image_url(url: str) -> str:
"""Convert an image url to openai format. If the given url is a local
file, it will be converted to base64 format. Otherwise, it will be
returned directly.
Args:
url (`str`):
The local or public url of the image.
"""
# See https://platform.openai.com/docs/guides/vision for details of
# support image extensions.
support_image_extensions = (
".png",
".jpg",
".jpeg",
".gif",
".webp",
)
parsed_url = urlparse(url)
lower_url = url.lower()
# Web url
if not os.path.exists(url) and parsed_url.scheme != "":
if any(lower_url.endswith(_) for _ in support_image_extensions):
return url
# Check if it is a local file
elif os.path.exists(url) and os.path.isfile(url):
if any(lower_url.endswith(_) for _ in support_image_extensions):
with open(url, "rb") as image_file:
base64_image = base64.b64encode(image_file.read()).decode(
"utf-8",
)
extension = parsed_url.path.lower().split(".")[-1]
mime_type = f"image/{extension}"
return f"data:{mime_type};base64,{base64_image}"
raise TypeError(f'"{url}" should end with {support_image_extensions}.')
def _to_openai_audio_data(source: URLSource | Base64Source) -> dict:
"""Covert an audio source to OpenAI format."""
if source["type"] == "url":
extension = source["url"].split(".")[-1].lower()
if extension not in ["wav", "mp3"]:
raise TypeError(
f"Unsupported audio file extension: {extension}, "
"wav and mp3 are supported.",
)
parsed_url = urlparse(source["url"])
if os.path.exists(source["url"]):
with open(source["url"], "rb") as audio_file:
data = base64.b64encode(audio_file.read()).decode("utf-8")
# web url
elif parsed_url.scheme != "":
response = requests.get(source["url"])
response.raise_for_status()
data = base64.b64encode(response.content).decode("utf-8")
else:
raise ValueError(
f"Unsupported audio source: {source['url']}, "
"it should be a local file or a web URL.",
)
return {
"data": data,
"format": extension,
}
if source["type"] == "base64":
data = source["data"]
media_type = source["media_type"]
if media_type not in ["audio/wav", "audio/mp3"]:
raise TypeError(
f"Unsupported audio media type: {media_type}, "
"only audio/wav and audio/mp3 are supported.",
)
return {
"data": data,
"format": media_type.split("/")[-1],
}
raise TypeError(f"Unsupported audio source: {source['type']}.")
class OpenAIChatFormatter(TruncatedFormatterBase):
"""The class used to format message objects into the OpenAI API required
format."""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = True
"""Whether support multi-agent conversation"""
support_vision: bool = True
"""Whether support vision models"""
supported_blocks: list[type] = [
TextBlock,
ImageBlock,
AudioBlock,
ToolUseBlock,
ToolResultBlock,
]
"""Supported message blocks for OpenAI API"""
async def _format(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Format message objects into OpenAI API required format.
Args:
msgs (`list[Msg]`):
The list of Msg objects to format.
Returns:
`list[dict[str, Any]]`:
A list of dictionaries, where each dictionary has "name",
"role", and "content" keys.
"""
self.assert_list_of_msgs(msgs)
messages: list[dict] = []
for msg in msgs:
content_blocks = []
tool_calls = []
for block in msg.get_content_blocks():
typ = block.get("type")
if typ == "text":
content_blocks.append({**block})
elif typ == "tool_use":
tool_calls.append(
{
"id": block.get("id"),
"type": "function",
"function": {
"name": block.get("name"),
"arguments": json.dumps(
block.get("input", {}),
ensure_ascii=False,
),
},
},
)
elif typ == "tool_result":
messages.append(
{
"role": "tool",
"tool_call_id": block.get("id"),
"content": self.convert_tool_result_to_string(
block.get("output"), # type: ignore[arg-type]
),
"name": block.get("name"),
},
)
elif typ == "image":
source_type = block["source"]["type"]
if source_type == "url":
url = _to_openai_image_url(block["source"]["url"])
elif source_type == "base64":
data = block["source"]["data"]
media_type = block["source"]["media_type"]
url = f"data:{media_type};base64,{data}"
else:
raise ValueError(
f"Unsupported image source type: {source_type}",
)
content_blocks.append(
{
"type": "image_url",
"image_url": {
"url": url,
},
},
)
elif typ == "audio":
input_audio = _to_openai_audio_data(block["source"])
content_blocks.append(
{
"type": "input_audio",
"input_audio": input_audio,
},
)
else:
logger.warning(
"Unsupported block type %s in the message, skipped.",
typ,
)
msg_openai = {
"role": msg.role,
"name": msg.name,
"content": content_blocks or None,
}
if tool_calls:
msg_openai["tool_calls"] = tool_calls
# When both content and tool_calls are None, skipped
if msg_openai["content"] or msg_openai.get("tool_calls"):
messages.append(msg_openai)
return messages
class OpenAIMultiAgentFormatter(TruncatedFormatterBase):
"""
OpenAI formatter for multi-agent conversations, where more than
a user and an agent are involved.
.. tip:: This formatter is compatible with OpenAI API and
OpenAI-compatible services like vLLM, Azure OpenAI, and others.
"""
support_tools_api: bool = True
"""Whether support tools API"""
support_multiagent: bool = True
"""Whether support multi-agent conversation"""
support_vision: bool = True
"""Whether support vision models"""
supported_blocks: list[type] = [
TextBlock,
ImageBlock,
AudioBlock,
ToolUseBlock,
ToolResultBlock,
]
"""Supported message blocks for OpenAI API"""
def __init__(
self,
conversation_history_prompt: str = (
"# Conversation History\n"
"The content between <history></history> tags contains "
"your conversation history\n"
),
token_counter: TokenCounterBase | None = None,
max_tokens: int | None = None,
) -> None:
"""Initialize the OpenAI multi-agent formatter.
Args:
conversation_history_prompt (`str`):
The prompt to use for the conversation history section.
"""
super().__init__(token_counter=token_counter, max_tokens=max_tokens)
self.conversation_history_prompt = conversation_history_prompt
async def _format_tool_sequence(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Given a sequence of tool call/result messages, format them into
the required format for the OpenAI API."""
return await OpenAIChatFormatter().format(msgs)
async def _format_agent_message(
self,
msgs: list[Msg],
is_first: bool = True,
) -> list[dict[str, Any]]:
"""Given a sequence of messages without tool calls/results, format
them into the required format for the OpenAI API."""
if is_first:
conversation_history_prompt = self.conversation_history_prompt
else:
conversation_history_prompt = ""
# Format into required OpenAI format
formatted_msgs: list[dict] = []
conversation_blocks: list = []
accumulated_text = []
images = []
audios = []
for msg in msgs:
for block in msg.get_content_blocks():
if block["type"] == "text":
accumulated_text.append(f"{msg.name}: {block['text']}")
elif block["type"] == "image":
source_type = block["source"]["type"]
if source_type == "url":
url = _to_openai_image_url(block["source"]["url"])
elif source_type == "base64":
data = block["source"]["data"]
media_type = block["source"]["media_type"]
url = f"data:{media_type};base64,{data}"
else:
raise ValueError(
f"Unsupported image source type: {source_type}",
)
images.append(
{
"type": "image_url",
"image_url": {
"url": url,
},
},
)
elif block["type"] == "audio":
input_audio = _to_openai_audio_data(block["source"])
audios.append(
{
"type": "input_audio",
"input_audio": input_audio,
},
)
if accumulated_text:
conversation_blocks.append(
{"text": "\n".join(accumulated_text)},
)
if conversation_blocks:
if conversation_blocks[0].get("text"):
conversation_blocks[0]["text"] = (
conversation_history_prompt
+ "<history>\n"
+ conversation_blocks[0]["text"]
)
else:
conversation_blocks.insert(
0,
{
"text": conversation_history_prompt + "<history>\n",
},
)
if conversation_blocks[-1].get("text"):
conversation_blocks[-1]["text"] += "\n</history>"
else:
conversation_blocks.append({"text": "</history>"})
conversation_blocks_text = "\n".join(
conversation_block.get("text", "")
for conversation_block in conversation_blocks
)
content_list: list[dict[str, Any]] = []
if conversation_blocks_text:
content_list.append(
{
"type": "text",
"text": conversation_blocks_text,
},
)
if images:
content_list.extend(images)
if audios:
content_list.extend(audios)
user_message = {
"role": "user",
"content": content_list,
}
if content_list:
formatted_msgs.append(user_message)
return formatted_msgs
---- _truncated_formatter_base.py ----
# -*- coding: utf-8 -*-
"""The truncated formatter base class, which allows to truncate the input
messages."""
from abc import ABC
from copy import deepcopy
from typing import (
Any,
Tuple,
Literal,
AsyncGenerator,
)
from ._formatter_base import FormatterBase
from ..message import Msg
from ..token import TokenCounterBase
from ..tracing import trace_format
class TruncatedFormatterBase(FormatterBase, ABC):
"""Base class for truncated formatters, which formats input messages into
required formats with tokens under a specified limit."""
def __init__(
self,
token_counter: TokenCounterBase | None = None,
max_tokens: int | None = None,
) -> None:
"""Initialize the TruncatedFormatterBase.
Args:
token_counter (`TokenCounterBase | None`, optional):
A token counter instance used to count tokens in the messages.
If not provided, the formatter will format the messages
without considering token limits.
max_tokens (`int | None`, optional):
The maximum number of tokens allowed in the formatted
messages. If not provided, the formatter will not truncate
the messages.
"""
self.token_counter = token_counter
assert (
max_tokens is None or 0 < max_tokens
), "max_tokens must be greater than 0"
self.max_tokens = max_tokens
@trace_format
async def format(
self,
msgs: list[Msg],
**kwargs: Any,
) -> list[dict[str, Any]]:
"""Format the input messages into the required format. If token
counter and max token limit are provided, the messages will be
truncated to fit the limit.
Args:
msgs (`list[Msg]`):
The input messages to be formatted.
Returns:
`list[dict[str, Any]]`:
The formatted messages in the required format.
"""
# Check if the input messages are valid
self.assert_list_of_msgs(msgs)
msgs = deepcopy(msgs)
while True:
formatted_msgs = await self._format(msgs)
n_tokens = await self._count(formatted_msgs)
if (
n_tokens is None
or self.max_tokens is None
or n_tokens <= self.max_tokens
):
return formatted_msgs
# truncate the input messages
msgs = await self._truncate(msgs)
async def _format(self, msgs: list[Msg]) -> list[dict[str, Any]]:
"""Format the input messages into the required format. This method
should be implemented by the subclasses."""
formatted_msgs = []
start_index = 0
if len(msgs) > 0 and msgs[0].role == "system":
formatted_msgs.append(
await self._format_system_message(msgs[0]),
)
start_index = 1
is_first_agent_message = True
async for typ, group in self._group_messages(msgs[start_index:]):
match typ:
case "tool_sequence":
formatted_msgs.extend(
await self._format_tool_sequence(group),
)
case "agent_message":
formatted_msgs.extend(
await self._format_agent_message(
group,
is_first_agent_message,
),
)
is_first_agent_message = False
return formatted_msgs
async def _format_system_message(
self,
msg: Msg,
) -> dict[str, Any]:
"""Format system message for the LLM API.
.. note:: This is the default implementation. For certain LLM APIs
with specific requirements, you may need to implement a custom
formatting function to accommodate those particular needs.
"""
return {
"role": "system",
"content": msg.get_content_blocks("text"),
}
async def _format_tool_sequence(
self,
msgs: list[Msg],
) -> list[dict[str, Any]]:
"""Given a sequence of tool call/result messages, format them into
the required format for the LLM API."""
raise NotImplementedError(
"_format_tool_sequence is not implemented",
)
async def _format_agent_message(
self,
msgs: list[Msg],
is_first: bool = True,
) -> list[dict[str, Any]]:
"""Given a sequence of messages without tool calls/results, format
them into the required format for the LLM API."""
raise NotImplementedError(
"_format_agent_message is not implemented",
)
async def _truncate(self, msgs: list[Msg]) -> list[Msg]:
"""Truncate the input messages, so that it can fit the token limit.
This function is called only when
- both `token_counter` and `max_tokens` are provided,
- the formatted output of the input messages exceeds the token limit.
.. tip:: This function only provides a simple strategy, and developers
can override this method to implement more sophisticated
truncation strategies.
.. note:: The tool call message should be truncated together with
its corresponding tool result message to satisfy the LLM API
requirements.
Args:
msgs (`list[Msg]`):
The input messages to be truncated.
Raises:
`ValueError`:
If the system prompt message already exceeds the token limit,
or if there are tool calls without corresponding tool results.
Returns:
`list[Msg]`:
The truncated messages.
"""
start_index = 0
if len(msgs) > 0 and msgs[0].role == "system":
if len(msgs) == 1:
# If the system prompt already exceeds the token limit, we
# raise an error.
raise ValueError(
f"The system prompt message already exceeds the token "
f"limit ({self.max_tokens} tokens).",
)
start_index = 1
# Create a tool call IDs queues to delete the corresponding tool
# result message
tool_call_ids = set()
for i in range(start_index, len(msgs)):
msg = msgs[i]
for block in msg.get_content_blocks("tool_use"):
tool_call_ids.add(block["id"])
for block in msg.get_content_blocks("tool_result"):
try:
tool_call_ids.remove(block["id"])
except KeyError:
pass
# We can stop truncating if the queue is empty
if len(tool_call_ids) == 0:
return msgs[:start_index] + msgs[i + 1 :]
if len(tool_call_ids) > 0:
raise ValueError(
"The input messages contains tool call(s) that do not have "
f"the corresponding tool result(s): {tool_call_ids}. ",
)
return msgs[:start_index]
async def _count(self, msgs: list[dict[str, Any]]) -> int | None:
"""Count the number of tokens in the input messages. If token counter
is not provided, `None` will be returned.
Args:
msgs (`list[Msg]`):
The input messages to count tokens for.
"""
if self.token_counter is None:
return None
return await self.token_counter.count(msgs)
@staticmethod
async def _group_messages(
msgs: list[Msg],
) -> AsyncGenerator[
Tuple[Literal["tool_sequence", "agent_message"], list[Msg]],
None,
]:
"""Group the input messages into two types and yield them as a
generator. The two types are:
- agent message that doesn't contain tool calls/results, and
- tool sequence that consisted of a sequence of tool calls/results
.. note:: The group operation is used in multi-agent scenario, where
multiple entities are involved in the input messages. So that to be
compatible with tools API, we have to group the messages and format
them with different strategies.
Args:
msgs (`list[Msg]`):
The input messages to be grouped, where the system prompt
message shouldn't be included.
Yields:
`AsyncGenerator[Tuple[str, list[Msg]], None]`:
A generator that yields tuples of group type and the list of
messages in that group. The group type can be either
"tool_sequence" or "agent_message".
"""
group_type: Literal["tool_sequence", "agent_message"] | None = None
group = []
for msg in msgs:
if group_type is None:
if msg.has_content_blocks(
"tool_use",
) or msg.has_content_blocks("tool_result"):
group_type = "tool_sequence"
else:
group_type = "agent_message"
group.append(msg)
continue
# determine if this msg has the same type as the current group
if group_type == "tool_sequence":
if msg.has_content_blocks(
"tool_use",
) or msg.has_content_blocks("tool_result"):
group.append(msg)
else:
yield group_type, group
group = [msg]
group_type = "agent_message"
elif group_type == "agent_message":
if msg.has_content_blocks(
"tool_use",
) or msg.has_content_blocks("tool_result"):
yield group_type, group
group = [msg]
group_type = "tool_sequence"
else:
group.append(msg)
if group_type:
yield group_type, group
==== hooks ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The built-in hook functions in agentscope."""
from functools import partial
from ._studio_hooks import (
as_studio_forward_message_pre_print_hook,
)
from .. import _config
from ..agent import AgentBase
__all__ = [
"as_studio_forward_message_pre_print_hook",
]
def _equip_as_studio_hooks(
studio_url: str,
) -> None:
"""Connect to the agentscope studio."""
AgentBase.register_class_hook(
"pre_print",
"as_studio_forward_message_pre_print_hook",
partial(
as_studio_forward_message_pre_print_hook,
studio_url=studio_url,
run_id=_config.run_id,
),
)
---- _studio_hooks.py ----
# -*- coding: utf-8 -*-
"""The studio related hook functions in agentscope."""
from typing import Any
import requests
import shortuuid
from ..agent import AgentBase
def as_studio_forward_message_pre_print_hook(
self: AgentBase,
kwargs: dict[str, Any],
studio_url: str,
run_id: str,
) -> None:
"""The pre-speak hook to forward messages to the studio."""
msg = kwargs["msg"]
message_data = msg.to_dict()
if hasattr(self, "_reply_id"):
reply_id = getattr(self, "_reply_id")
else:
reply_id = shortuuid.uuid()
n_retry = 0
while True:
try:
res = requests.post(
f"{studio_url}/trpc/pushMessage",
json={
"runId": run_id,
"replyId": reply_id,
"name": reply_id,
"role": "assistant",
"msg": message_data,
},
)
res.raise_for_status()
break
except Exception as e:
if n_retry < 3:
n_retry += 1
continue
raise e from None
==== mcp ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The MCP module in AgentScope, that provides fine-grained control over
the MCP servers."""
from ._client_base import MCPClientBase
from ._mcp_function import MCPToolFunction
from ._stateful_client_base import StatefulClientBase
from ._stdio_stateful_client import StdIOStatefulClient
from ._http_stateless_client import HttpStatelessClient
from ._http_stateful_client import HttpStatefulClient
__all__ = [
"MCPToolFunction",
"MCPClientBase",
"StatefulClientBase",
"StdIOStatefulClient",
"HttpStatelessClient",
"HttpStatefulClient",
]
---- _client_base.py ----
# -*- coding: utf-8 -*-
"""The base class for MCP clients in AgentScope."""
from abc import abstractmethod
from typing import Callable, List
import mcp.types
from .._logging import logger
from ..message import ImageBlock, Base64Source, AudioBlock, TextBlock
class MCPClientBase:
"""Base class for MCP clients."""
def __init__(self, name: str) -> None:
"""Initialize the MCP client with a name.
Args:
name (`str`):
The name to identify the MCP server, which should be unique
across the MCP servers.
"""
self.name = name
@abstractmethod
async def get_callable_function(
self,
func_name: str,
wrap_tool_result: bool = True,
) -> Callable:
"""Get a tool function by its name."""
@staticmethod
def _convert_mcp_content_to_as_blocks(
mcp_content_blocks: list,
) -> List[TextBlock | ImageBlock | AudioBlock]:
"""Convert MCP content to AgentScope blocks."""
as_content: list = []
for content in mcp_content_blocks:
if isinstance(content, mcp.types.TextContent):
as_content.append(
TextBlock(
type="text",
text=content.text,
),
)
elif isinstance(content, mcp.types.ImageContent):
as_content.append(
ImageBlock(
type="image",
source=Base64Source(
type="base64",
media_type=content.mimeType,
data=content.data,
),
),
)
elif isinstance(content, mcp.types.AudioContent):
as_content.append(
AudioBlock(
type="audio",
source=Base64Source(
type="base64",
media_type=content.mimeType,
data=content.data,
),
),
)
elif isinstance(content, mcp.types.EmbeddedResource):
if isinstance(
content.resource,
mcp.types.TextResourceContents,
):
as_content.append(
TextBlock(
type="text",
text=content.resource.model_dump_json(indent=2),
),
)
else:
# TODO: support the BlobResourceContents in the future,
# which is a base64-encoded string representing the
# binary data
logger.error(
"Unsupported EmbeddedResource content type: %s. "
"Skipping this content.",
type(content.resource),
)
else:
logger.warning(
"Unsupported content type: %s. Skipping this content.",
type(content),
)
return as_content
---- _http_stateful_client.py ----
# -*- coding: utf-8 -*-
"""The MCP stateful HTTP client module in AgentScope."""
from typing import Any, Literal
from mcp.client.sse import sse_client
from mcp.client.streamable_http import streamablehttp_client
from ._stateful_client_base import StatefulClientBase
class HttpStatefulClient(StatefulClientBase):
"""The stateful sse/streamable HTTP MCP client implementation in
AgentScope.
.. tip:: The stateful client is recommended for MCP servers that need to
maintain session states, e.g. web browsers or other interactive
MCP servers.
.. note:: The stateful client will maintain one session across multiple
tool calls, until the client is closed by explicitly calling the
`close()` method.
.. note:: When multiple HttpStatefulClient instances are connected,
they should be closed following the Last In First Out (LIFO) principle
to avoid potential errors. Always close the most recently registered
client first, then work backwards to the first one.
For more details, please refer to this `issue
<https://github.com/modelcontextprotocol/python-sdk/issues/577>`_.
"""
def __init__(
self,
name: str,
transport: Literal["streamable_http", "sse"],
url: str,
headers: dict[str, str] | None = None,
timeout: float = 30,
sse_read_timeout: float = 60 * 5,
**client_kwargs: Any,
) -> None:
"""Initialize the streamable HTTP MCP client.
Args:
name (`str`):
The name to identify the MCP server, which should be unique
across the MCP servers.
transport (`Literal["streamable_http", "sse"]`):
The transport type of MCP server. Generally, the URL of sse
transport should end with `/sse`, while the streamable HTTP
URL ends with `/mcp`.
url (`str`):
The URL to the MCP server.
headers (`dict[str, str] | None`, optional):
Additional headers to include in the HTTP request.
timeout (`float`, optional):
The timeout for the HTTP request in seconds. Defaults to 30.
sse_read_timeout (`float`, optional):
The timeout for reading Server-Sent Events (SSE) in seconds.
Defaults to 300 (5 minutes).
**client_kwargs (`Any`):
The additional keyword arguments to pass to the streamable
HTTP client.
"""
super().__init__(name=name)
assert transport in ["streamable_http", "sse"]
self.transport = transport
if self.transport == "streamable_http":
self.client = streamablehttp_client(
url=url,
headers=headers,
timeout=timeout,
sse_read_timeout=sse_read_timeout,
**client_kwargs,
)
else:
self.client = sse_client(
url=url,
headers=headers,
timeout=timeout,
sse_read_timeout=sse_read_timeout,
**client_kwargs,
)
---- _http_stateless_client.py ----
# -*- coding: utf-8 -*-
"""The MCP streamable HTTP server."""
from contextlib import _AsyncGeneratorContextManager
from typing import Any, Callable, Awaitable, Literal, List
import mcp.types
from mcp import ClientSession
from mcp.client.sse import sse_client
from mcp.client.streamable_http import streamablehttp_client
from . import MCPToolFunction
from ._client_base import MCPClientBase
from ..tool import ToolResponse
class HttpStatelessClient(MCPClientBase):
"""The sse/streamable HTTP MCP client implementation in AgentScope.
.. note:: Note this client is stateless, meaning it won't maintain the
session state across multiple tool calls. Each tool call will start a
new session and close it after the call is done.
"""
stateful: bool = False
"""Whether the MCP server is stateful, meaning it will maintain the
session state across multiple tool calls, or stateless, meaning it
will start a new session for each tool call."""
def __init__(
self,
name: str,
transport: Literal["streamable_http", "sse"],
url: str,
headers: dict[str, str] | None = None,
timeout: float = 30,
sse_read_timeout: float = 60 * 5,
**client_kwargs: Any,
) -> None:
"""Initialize the streamable HTTP MCP server.
Args:
name (`str`):
The name to identify the MCP server, which should be unique
across the MCP servers.
transport (`Literal["streamable_http", "sse"]`):
The transport type of MCP server. Generally, the URL of sse
transport should end with `/sse`, while the streamable HTTP
URL ends with `/mcp`.
url (`str`):
The URL of the MCP server.
headers (`dict[str, str] | None`, optional):
Additional headers to include in the HTTP request.
timeout (`float`, optional):
The timeout for the HTTP request in seconds. Defaults to 30.
sse_read_timeout (`float`, optional):
The timeout for reading Server-Sent Events (SSE) in seconds.
Defaults to 300 (5 minutes).
**client_kwargs (`Any`):
The additional keyword arguments to pass to the streamable
HTTP client.
"""
super().__init__(name=name)
assert transport in ["streamable_http", "sse"]
self.transport = transport
self.client_config = {
"url": url,
"headers": headers or {},
"timeout": timeout,
"sse_read_timeout": sse_read_timeout,
**client_kwargs,
}
self._tools = None
def get_client(self) -> _AsyncGeneratorContextManager[Any]:
"""The disposable MCP client object, which is a context manager."""
if self.transport == "sse":
return sse_client(**self.client_config)
if self.transport == "streamable_http":
return streamablehttp_client(**self.client_config)
raise ValueError(
f"Unsupported transport type: {self.transport}. "
"Supported types are 'sse' and 'streamable_http'.",
)
async def get_callable_function(
self,
func_name: str,
wrap_tool_result: bool = True,
) -> Callable[..., Awaitable[mcp.types.CallToolResult | ToolResponse]]:
"""Get a tool function by its name.
Args:
func_name (`str`):
The name of the tool function.
wrap_tool_result (`bool`, defaults to `True`):
Whether to wrap the tool result into agentscope's
`ToolResponse` object. If `False`, the raw result type
`mcp.types.CallToolResult` will be returned.
Returns:
`Callable[..., Awaitable[mcp.types.CallToolResult | \
ToolResponse]]`:
An async tool function that returns either
`mcp.types.CallToolResult` or `ToolResponse` when called.
"""
if self._tools is None:
await self.list_tools()
target_tool = None
for tool in self._tools:
if tool.name == func_name:
target_tool = tool
break
if target_tool is None:
raise ValueError(
f"Tool '{func_name}' not found in the MCP server ",
)
return MCPToolFunction(
mcp_name=self.name,
tool=target_tool,
wrap_tool_result=wrap_tool_result,
client_gen=self.get_client,
)
async def list_tools(self) -> List[mcp.types.Tool]:
"""List all tools available on the MCP server.
Returns:
`mcp.types.ListToolsResult`:
The result containing the list of tools.
"""
async with self.get_client() as cli:
read_stream, write_stream = cli[0], cli[1]
async with ClientSession(read_stream, write_stream) as session:
await session.initialize()
res = await session.list_tools()
self._tools = res.tools
return res.tools
---- _mcp_function.py ----
# -*- coding: utf-8 -*-
"""The MCP tool function class in AgentScope."""
from contextlib import _AsyncGeneratorContextManager
from typing import Any, Callable
import mcp
from mcp import ClientSession
from ._client_base import MCPClientBase
from .._utils._common import _extract_json_schema_from_mcp_tool
from ..tool import ToolResponse
class MCPToolFunction:
"""An MCP tool function class that can be called directly."""
name: str
"""The name of the tool function."""
description: str
"""The description of the tool function."""
json_schema: dict[str, Any]
"""JSON schema of the tool function"""
def __init__(
self,
mcp_name: str,
tool: mcp.types.Tool,
wrap_tool_result: bool,
client_gen: Callable[..., _AsyncGeneratorContextManager[Any]]
| None = None,
session: ClientSession | None = None,
) -> None:
"""Initialize the MCP function."""
self.mcp_name = mcp_name
self.name = tool.name
self.description = tool.description
self.json_schema = _extract_json_schema_from_mcp_tool(tool)
self.wrap_tool_result = wrap_tool_result
# Cannot be None at the same time
if (
client_gen is None
and session is None
or (client_gen is not None and session is not None)
):
raise ValueError(
"Either client or session must be provided, but not both.",
)
self.client_gen = client_gen
self.session = session
async def __call__(
self,
**kwargs: Any,
) -> mcp.types.CallToolResult | ToolResponse:
"""Call the MCP tool function with the given arguments, and return
the result."""
if self.client_gen:
async with self.client_gen() as cli:
read_stream, write_stream = cli[0], cli[1]
async with ClientSession(read_stream, write_stream) as session:
await session.initialize()
res = await session.call_tool(
self.name,
arguments=kwargs,
)
else:
res = await self.session.call_tool(
self.name,
arguments=kwargs,
)
if self.wrap_tool_result:
as_content = MCPClientBase._convert_mcp_content_to_as_blocks(
res.content,
)
return ToolResponse(
content=as_content,
metadata=res.meta,
)
return res
---- _stateful_client_base.py ----
# -*- coding: utf-8 -*-
"""The base MCP stateful client class in AgentScope, that provides basic
functionality for stateful MCP clients."""
from abc import ABC
from contextlib import AsyncExitStack
from typing import List
import mcp
from mcp import ClientSession
from ._client_base import MCPClientBase
from ._mcp_function import MCPToolFunction
from .._logging import logger
class StatefulClientBase(MCPClientBase, ABC):
"""The base class for stateful MCP clients in AgentScope, which maintains
the session state across multiple tool calls.
The developers should use `connect()` and `close()` methods to manage
the client lifecycle.
"""
is_connected: bool
"""If connected to the MCP server"""
def __init__(self, name: str) -> None:
"""Initialize the stateful MCP client.
Args:
name (`str`):
The name to identify the MCP server, which should be unique
across the MCP servers.
"""
super().__init__(name=name)
self.client = None
self.stack = None
self.session = None
self.is_connected = False
# Cache the tools to avoid fetching them multiple times
self._cached_tools = None
async def connect(self) -> None:
"""Connect to MCP server."""
if self.is_connected:
raise RuntimeError(
"The MCP server is already connected. Call close() "
"before connecting again.",
)
self.stack = AsyncExitStack()
try:
context = await self.stack.enter_async_context(
self.client,
)
read_stream, write_stream = context[0], context[1]
self.session = ClientSession(read_stream, write_stream)
await self.stack.enter_async_context(self.session)
await self.session.initialize()
self.is_connected = True
logger.info("MCP client connected.")
except Exception:
await self.stack.aclose()
self.stack = None
raise
async def close(self) -> None:
"""Clean up the MCP client resources. You must call this method when
your application is done."""
if not self.is_connected:
raise RuntimeError(
"The MCP server is not connected. Call connect() before "
"closing.",
)
try:
await self.stack.aclose()
logger.info("MCP client closed.")
except Exception as e:
logger.warning("Error during MCP client cleanup: %s", e)
finally:
self.stack = None
self.session = None
self.is_connected = False
async def list_tools(self) -> List[mcp.types.Tool]:
"""Get all available tools from the server.
Returns:
`mcp.types.ListToolsResult`:
A list of available MCP tools.
"""
self._validate_connection()
res = await self.session.list_tools()
# Cache the tools for later use
self._cached_tools = res.tools
return res.tools
async def get_callable_function(
self,
func_name: str,
wrap_tool_result: bool = True,
) -> MCPToolFunction:
"""Get an async tool function from the MCP server by its name, so
that you can call it directly, wrap it into your own function, or
anyway you like.
.. note:: Currently, only the text, image, and audio results are
supported in this function.
Args:
func_name (`str`):
The name of the tool function to get.
wrap_tool_result (`bool`):
Whether to wrap the tool result into agentscope's
`ToolResponse` object. If `False`, the raw result type
`mcp.types.CallToolResult` will be returned.
Returns:
`MCPToolFunction`:
A callable async function that returns either
`mcp.types.CallToolResult` or `ToolResponse` when called.
"""
self._validate_connection()
if self._cached_tools is None:
await self.list_tools()
target_tool = None
for tool in self._cached_tools:
if tool.name == func_name:
target_tool = tool
break
if target_tool is None:
raise ValueError(
f"Tool '{func_name}' not found in the MCP server",
)
return MCPToolFunction(
mcp_name=self.name,
tool=target_tool,
wrap_tool_result=wrap_tool_result,
session=self.session,
)
def _validate_connection(self) -> None:
"""Validate the connection to the MCP server."""
if not self.is_connected:
raise RuntimeError(
"The connection is not established. Call connect() "
"before using the client.",
)
if not self.session:
raise RuntimeError(
"The session is not initialized. Call connect() "
"before using the client.",
)
---- _stdio_stateful_client.py ----
# -*- coding: utf-8 -*-
"""The StdIO MCP server implementation in AgentScope, which provides
function-level fine-grained control over the MCP servers using standard IO."""
from typing import Literal
from mcp import stdio_client, StdioServerParameters
from ._stateful_client_base import StatefulClientBase
class StdIOStatefulClient(StatefulClientBase):
"""A client class that sets up and manage StdIO MCP server connections, and
provides function-level fine-grained control over the MCP servers.
.. tip:: The stateful client is recommended for MCP servers that need to
maintain session states, e.g. web browsers or other interactive
MCP servers.
.. note:: The stateful client will maintain one session across multiple
tool calls, until the client is closed by explicitly calling the
`close()` method.
.. note:: When multiple StdIOStatefulClient instances are connected,
they should be closed following the Last In First Out (LIFO) principle
to avoid potential errors. Always close the most recently registered
client first, then work backwards to the first one.
For more details, please refer to this `issue
<https://github.com/modelcontextprotocol/python-sdk/issues/577>`_.
"""
def __init__(
self,
name: str,
command: str,
args: list[str] | None = None,
env: dict[str, str] | None = None,
cwd: str | None = None,
encoding: str = "utf-8",
encoding_error_handler: Literal[
"strict",
"ignore",
"replace",
] = "strict",
) -> None:
"""Initialize the MCP server with std IO.
Args:
name (`str`):
The name to identify the MCP server, which should be unique
across the MCP servers.
command (`str`):
The executable to run to start the server.
args (`list[str] | None`, optional):
Command line arguments to pass to the executable.
env (`dict[str, str] | None`, optional):
The environment to use when spawning the process.
cwd (`str | None`, optional):
The working directory to use when spawning the process.
encoding (`str`, optional):
The text encoding used when sending/receiving messages to the
server. Defaults to "utf-8".
encoding_error_handler (`Literal["strict", "ignore", "replace"]`, \
defaults to "strict"):
The text encoding error handler.
"""
super().__init__(name=name)
self.client = stdio_client(
StdioServerParameters(
command=command,
args=args or [],
env=env,
cwd=cwd,
encoding=encoding,
encoding_error_handler=encoding_error_handler,
),
)
==== memory ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The memory module."""
from ._in_memory_memory import InMemoryMemory
from ._long_term_memory_base import LongTermMemoryBase
from ._mem0_long_term_memory import Mem0LongTermMemory
from ._memory_base import MemoryBase
from ._reme import (
ReMePersonalLongTermMemory,
ReMeTaskLongTermMemory,
ReMeToolLongTermMemory,
)
__all__ = [
"MemoryBase",
"InMemoryMemory",
"LongTermMemoryBase",
"Mem0LongTermMemory",
"ReMePersonalLongTermMemory",
"ReMeTaskLongTermMemory",
"ReMeToolLongTermMemory",
]
---- _in_memory_memory.py ----
# -*- coding: utf-8 -*-
"""The dialogue memory class"""
from typing import Union, Iterable, Any
from ._memory_base import MemoryBase
from ..message import Msg
class InMemoryMemory(MemoryBase):
"""The in-memory memory class for storing messages."""
def __init__(
self,
) -> None:
"""Initialize the in-memory memory object."""
super().__init__()
self.content: list[Msg] = []
def state_dict(self) -> dict:
"""Convert the current memory into JSON data format."""
return {
"content": [_.to_dict() for _ in self.content],
}
def load_state_dict(
self,
state_dict: dict,
strict: bool = True,
) -> None:
"""Load the memory from JSON data.
Args:
state_dict (`dict`):
The state dictionary to load, which should have a "content"
field.
strict (`bool`, defaults to `True`):
If `True`, raises an error if any key in the module is not
found in the state_dict. If `False`, skips missing keys.
"""
self.content = []
for data in state_dict["content"]:
data.pop("type", None)
self.content.append(Msg.from_dict(data))
async def size(self) -> int:
"""The size of the memory."""
return len(self.content)
async def retrieve(self, *args: Any, **kwargs: Any) -> None:
"""Retrieve items from the memory."""
raise NotImplementedError(
"The retrieve method is not implemented in "
f"{self.__class__.__name__} class.",
)
async def delete(self, index: Union[Iterable, int]) -> None:
"""Delete the specified item by index(es).
Args:
index (`Union[Iterable, int]`):
The index to delete.
"""
if isinstance(index, int):
index = [index]
invalid_index = [_ for _ in index if 0 > _ or _ >= len(self.content)]
if invalid_index:
raise IndexError(
f"The index {invalid_index} does not exist.",
)
self.content = [
_ for idx, _ in enumerate(self.content) if idx not in index
]
async def add(
self,
memories: Union[list[Msg], Msg, None],
allow_duplicates: bool = False,
) -> None:
"""Add message into the memory.
Args:
memories (`Union[list[Msg], Msg, None]`):
The message to add.
allow_duplicates (`bool`, defaults to `False`):
If allow adding duplicate messages (with the same id) into
the memory.
"""
if memories is None:
return
if isinstance(memories, Msg):
memories = [memories]
if not isinstance(memories, list):
raise TypeError(
f"The memories should be a list of Msg or a single Msg, "
f"but got {type(memories)}.",
)
for msg in memories:
if not isinstance(msg, Msg):
raise TypeError(
f"The memories should be a list of Msg or a single Msg, "
f"but got {type(msg)}.",
)
if not allow_duplicates:
existing_ids = [_.id for _ in self.content]
memories = [_ for _ in memories if _.id not in existing_ids]
self.content.extend(memories)
async def get_memory(self) -> list[Msg]:
"""Get the memory content."""
return self.content
async def clear(self) -> None:
"""Clear the memory content."""
self.content = []
---- _long_term_memory_base.py ----
# -*- coding: utf-8 -*-
"""The long-term memory base class."""
from typing import Any
from ..message import Msg
from ..module import StateModule
from ..tool import ToolResponse
class LongTermMemoryBase(StateModule):
"""The long-term memory base class, which should be a time-series
memory management system.
The `record_to_memory` and `retrieve_from_memory` methods are two tool
functions for agent to manage the long-term memory voluntarily. You can
choose not to implement these two functions.
The `record` and `retrieve` methods are for developers to use. For example,
retrieving/recording memory at the beginning of each reply, and adding
the retrieved memory to the system prompt.
"""
async def record(
self,
msgs: list[Msg | None],
**kwargs: Any,
) -> None:
"""A developer-designed method to record information from the given
input message(s) to the long-term memory."""
raise NotImplementedError(
"The `record` method is not implemented. ",
)
async def retrieve(
self,
msg: Msg | list[Msg] | None,
**kwargs: Any,
) -> str:
"""A developer-designed method to retrieve information from the
long-term memory based on the given input message(s). The retrieved
information will be added to the system prompt of the agent."""
raise NotImplementedError(
"The `retrieve` method is not implemented. ",
)
async def record_to_memory(
self,
thinking: str,
content: list[str],
**kwargs: Any,
) -> ToolResponse:
"""Use this function to record important information that you may
need later. The target content should be specific and concise, e.g.
who, when, where, do what, why, how, etc.
Args:
thinking (`str`):
Your thinking and reasoning about what to record
content (`list[str]`):
The content to remember, which is a list of strings.
"""
raise NotImplementedError(
"The `record_to_memory` method is not implemented. "
"You can implement it in your own long-term memory class.",
)
async def retrieve_from_memory(
self,
keywords: list[str],
**kwargs: Any,
) -> ToolResponse:
"""Retrieve the memory based on the given keywords.
Args:
keywords (`list[str]`):
The keywords to search for in the memory, which should be
specific and concise, e.g. the person's name, the date, the
location, etc.
Returns:
`list[Msg]`:
A list of messages that match the keywords.
"""
raise NotImplementedError(
"The `retrieve_from_memory` method is not implemented. "
"You can implement it in your own long-term memory class.",
)
---- _mem0_long_term_memory.py ----
# -*- coding: utf-8 -*-
"""Long-term memory implementation using mem0 library.
This module provides a long-term memory implementation that integrates
with the mem0 library to provide persistent memory storage and retrieval
capabilities for AgentScope agents.
"""
import json
from typing import Any, TYPE_CHECKING
from importlib import metadata
from pydantic import field_validator
from ..embedding import EmbeddingModelBase
from ._long_term_memory_base import LongTermMemoryBase
from ..message import Msg, TextBlock
from ..model import ChatModelBase
from ..tool import ToolResponse
if TYPE_CHECKING:
from mem0.configs.base import MemoryConfig
from mem0.vector_stores.configs import VectorStoreConfig
else:
MemoryConfig = Any
VectorStoreConfig = Any
def _create_agentscope_config_classes() -> tuple:
"""Create custom config classes for agentscope providers."""
from mem0.embeddings.configs import EmbedderConfig
from mem0.llms.configs import LlmConfig
class _ASLlmConfig(LlmConfig):
"""Custom LLM config class that updates the validate_config method.
Attention: in mem0, the validate_config hardcodes the provider, so we
need to override the validate_config method to support the agentscope
providers. We will follow up with the mem0 to improve this.
"""
@field_validator("config")
@classmethod
def validate_config(cls, v: Any, values: Any) -> Any:
"""Validate the LLM configuration."""
from mem0.utils.factory import LlmFactory
provider = values.data.get("provider")
if provider in LlmFactory.provider_to_class:
return v
raise ValueError(f"Unsupported LLM provider: {provider}")
class _ASEmbedderConfig(EmbedderConfig):
"""Custom embedder config class that updates the validate_config
method."""
@field_validator("config")
@classmethod
def validate_config(cls, v: Any, values: Any) -> Any:
"""Validate the embedder configuration."""
from mem0.utils.factory import EmbedderFactory
provider = values.data.get("provider")
if provider in EmbedderFactory.provider_to_class:
return v
raise ValueError(f"Unsupported Embedder provider: {provider}")
return _ASLlmConfig, _ASEmbedderConfig
class Mem0LongTermMemory(LongTermMemoryBase):
"""A class that implements the LongTermMemoryBase interface using mem0."""
def __init__(
self,
agent_name: str | None = None,
user_name: str | None = None,
run_name: str | None = None,
model: ChatModelBase | None = None,
embedding_model: EmbeddingModelBase | None = None,
vector_store_config: VectorStoreConfig | None = None,
mem0_config: MemoryConfig | None = None,
default_memory_type: str | None = None,
**kwargs: Any,
) -> None:
"""Initialize the Mem0LongTermMemory instance
Args:
agent_name (`str | None`, optional):
The name of the agent. Default is None.
user_name (`str | None`, optional):
The name of the user. Default is None.
run_name (`str | None`, optional):
The name of the run/session. Default is None.
.. note::
1. At least one of `agent_name`, `user_name`, or `run_name` is
required.
2. During memory recording, these parameters become metadata
for the stored memories.
3. **Important**: mem0 will extract memories from messages
containing role of "user" by default. If you want to
extract memories from messages containing role of
"assistant", you need to provide `agent_name`.
4. During memory retrieval, only memories with matching
metadata values will be returned.
model (`ChatModelBase | None`, optional):
The chat model to use for the long-term memory. If
mem0_config is provided, this will override the LLM
configuration. If mem0_config is None, this is required.
embedding_model (`EmbeddingModelBase | None`, optional):
The embedding model to use for the long-term memory. If
mem0_config is provided, this will override the embedder
configuration. If mem0_config is None, this is required.
vector_store_config (`VectorStoreConfig | None`, optional):
The vector store config to use for the long-term memory.
If mem0_config is provided, this will override the vector store
configuration. If mem0_config is None and this is not
provided, defaults to Qdrant with on_disk=True.
mem0_config (`MemoryConfig | None`, optional):
The mem0 config to use for the long-term memory.
If provided, individual
model/embedding_model/vector_store_config parameters will
override the corresponding configurations in mem0_config. If
None, a new MemoryConfig will be created using the provided
parameters.
default_memory_type (`str | None`, optional):
The type of memory to use. Default is None, to create a
semantic memory.
Raises:
`ValueError`:
If `mem0_config` is None and either `model` or
`embedding_model` is None.
"""
super().__init__()
try:
import mem0
from mem0.configs.llms.base import BaseLlmConfig
from mem0.utils.factory import LlmFactory, EmbedderFactory
from packaging import version
# Check mem0 version
current_version = metadata.version("mem0ai")
is_mem0_version_low = version.parse(
current_version,
) <= version.parse("0.1.115")
# Register the agentscope providers with mem0
EmbedderFactory.provider_to_class[
"agentscope"
] = "agentscope.memory._mem0_utils.AgentScopeEmbedding"
if is_mem0_version_low:
# For mem0 version <= 0.1.115, use the old style
LlmFactory.provider_to_class[
"agentscope"
] = "agentscope.memory._mem0_utils.AgentScopeLLM"
else:
# For mem0 version > 0.1.115, use the new style
LlmFactory.provider_to_class["agentscope"] = (
"agentscope.memory._mem0_utils.AgentScopeLLM",
BaseLlmConfig,
)
except ImportError as e:
raise ImportError(
"Please install the mem0 library by `pip install mem0ai`",
) from e
# Create the custom config classes for agentscope providers dynamically
_ASLlmConfig, _ASEmbedderConfig = _create_agentscope_config_classes()
if agent_name is None and user_name is None and run_name is None:
raise ValueError(
"at least one of agent_name, user_name, and run_name is "
"required",
)
# Store agent and user identifiers for memory management
self.agent_id = agent_name
self.user_id = user_name
self.run_id = run_name
# Configuration logic: Handle mem0_config parameter
if mem0_config is not None:
# Case 1: mem0_config is provided - override specific
# configurations if individual params are given
# Override LLM configuration if model is provided
if model is not None:
mem0_config.llm = _ASLlmConfig(
provider="agentscope",
config={"model": model},
)
# Override embedder configuration if embedding_model is provided
if embedding_model is not None:
mem0_config.embedder = _ASEmbedderConfig(
provider="agentscope",
config={"model": embedding_model},
)
# Override vector store configuration if vector_store_config is
# provided
if vector_store_config is not None:
mem0_config.vector_store = vector_store_config
else:
# Case 2: mem0_config is not provided - create new configuration
# from individual parameters
# Validate that required parameters are provided
if model is None or embedding_model is None:
raise ValueError(
"model and embedding_model are required if mem0_config "
"is not provided",
)
# Create new MemoryConfig with provided LLM and embedder
mem0_config = mem0.configs.base.MemoryConfig(
llm=_ASLlmConfig(
provider="agentscope",
config={"model": model},
),
embedder=_ASEmbedderConfig(
provider="agentscope",
config={"model": embedding_model},
),
)
# Set vector store configuration
if vector_store_config is not None:
# Use provided vector store configuration
mem0_config.vector_store = vector_store_config
else:
# Use default Qdrant configuration with on-disk storage for
# persistence set on_disk to True to enable persistence,
# otherwise it will be in memory only
on_disk = kwargs.get("on_disk", True)
mem0_config.vector_store = (
mem0.vector_stores.configs.VectorStoreConfig(
config={"on_disk": on_disk},
)
)
# Initialize the async memory instance with the configured settings
self.long_term_working_memory = mem0.AsyncMemory(mem0_config)
# Store the default memory type for future use
self.default_memory_type = default_memory_type
async def record_to_memory(
self,
thinking: str,
content: list[str],
**kwargs: Any,
) -> ToolResponse:
"""Use this function to record important information that you may
need later. The target content should be specific and concise, e.g.
who, when, where, do what, why, how, etc.
Args:
thinking (`str`):
Your thinking and reasoning about what to record.
content (`list[str]`):
The content to remember, which is a list of strings.
"""
# Multi-strategy recording approach to ensure content persistence:
#
# This method employs a three-tier fallback strategy to maximize
# successful memory recording:
#
# 1. Primary: Record as "user" role message
# - This is the default approach for capturing user-related
# content
# - Mem0 extracts and infers memories from messages containing
# role of "user"
#
# 2. Fallback (if agent_id exists): Record as "assistant" role
# message
# - Triggered when primary recording yields no results
# - In this case, mem0 will use the AGENT_MEMORY_EXTRACTION_PROMPT
# in mem0/mem0/configs/prompts.py to extract memories from
# messages containing role of "assistant", if agent_id is
# provided, otherwise it will use the
# USER_MEMORY_EXTRACTION_PROMPT in mem0/mem0/configs/prompts.py
# to extract memories.
#
# 3. Last resort: Record as "assistant" with infer=False
# - Used when both previous attempts yield no results
# - Bypasses mem0's inference mechanism, which means no
# inference is performed, mem0 will only record the content
# as is.
#
# This graduated approach ensures that even if mem0's inference fails
# to extract meaningful memories, the raw content is still preserved.
try:
if thinking:
content = [thinking] + content
# Strategy 1: Record as user message first
results = await self._mem0_record(
[
{
"role": "user",
"content": "\n".join(content),
"name": "user",
},
],
**kwargs,
)
# Strategy 2: Fallback to assistant message. In this case, if
# agent_id is provided, mem0 will use the
# AGENT_MEMORY_EXTRACTION_PROMPT in mem0/mem0/configs/prompts.py
# to extract memories from messages containing role of
# "assistant". If agent_id is not provided, mem0 will still use
# the USER_MEMORY_EXTRACTION_PROMPT in
# mem0/mem0/configs/prompts.py to extract memories.
if (
results
and isinstance(results, dict)
and "results" in results
and len(results["results"]) == 0
):
results = await self._mem0_record(
[
{
"role": "assistant",
"content": "\n".join(content),
"name": "assistant",
},
],
**kwargs,
)
# Strategy 3: Last resort - direct recording without inference.
# In this case, mem0 will not use any prompts to extract
# memories, it will only record the content as is.
if (
results
and isinstance(results, dict)
and "results" in results
and len(results["results"]) == 0
):
results = await self._mem0_record(
[
{
"role": "assistant",
"content": "\n".join(content),
"name": "assistant",
},
],
infer=False,
**kwargs,
)
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Successfully recorded content to memory "
f"{results}",
),
],
)
except Exception as e:
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error recording memory: {str(e)}",
),
],
)
async def retrieve_from_memory(
self,
keywords: list[str],
limit: int = 5,
**kwargs: Any,
) -> ToolResponse:
"""Retrieve the memory based on the given keywords.
Args:
keywords (`list[str]`):
The keywords to search for in the memory, which should be
specific and concise, e.g. the person's name, the date, the
location, etc.
limit (`int`, optional):
The maximum number of memories to retrieve per search.
Returns:
`ToolResponse`:
A ToolResponse containing the retrieved memories as JSON text.
"""
try:
results = []
for keyword in keywords:
result = await self.long_term_working_memory.search(
query=keyword,
agent_id=self.agent_id,
user_id=self.user_id,
run_id=self.run_id,
limit=limit,
)
if result:
results.extend(
[item["memory"] for item in result["results"]],
)
return ToolResponse(
content=[
TextBlock(
type="text",
text="\n".join(results),
),
],
)
except Exception as e:
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error retrieving memory: {str(e)}",
),
],
)
async def record(
self,
msgs: list[Msg | None],
memory_type: str | None = None,
infer: bool = True,
**kwargs: Any,
) -> None:
"""Record the content to the long-term memory.
Args:
msgs (`list[Msg | None]`):
The messages to record to memory.
memory_type (`str | None`, optional):
The type of memory to use. Default is None, to create a
semantic memory. "procedural_memory" is explicitly used for
procedural memories.
infer (`bool`, optional):
Whether to infer memory from the content. Default is True.
**kwargs (`Any`):
Additional keyword arguments for the mem0 recording.
"""
if isinstance(msgs, Msg):
msgs = [msgs]
# Filter out None
msg_list = [_ for _ in msgs if _]
if not all(isinstance(_, Msg) for _ in msg_list):
raise TypeError(
"The input messages must be a list of Msg objects.",
)
messages = [
{
"role": "assistant",
"content": "\n".join([str(_.content) for _ in msg_list]),
"name": "assistant",
},
]
await self._mem0_record(
messages,
memory_type=memory_type,
infer=infer,
**kwargs,
)
async def _mem0_record(
self,
messages: str | list[dict],
memory_type: str | None = None,
infer: bool = True,
**kwargs: Any,
) -> dict:
"""Record the content to the long-term memory.
Args:
messages (`str`):
The content to remember, which is a string or a list of
dictionaries representing messages.
memory_type (`str | None`, optional):
The type of memory to use. Default is None, to create a
semantic memory. "procedural_memory" is explicitly used for
procedural memories.
infer (`bool`, optional):
Whether to infer memory from the content. Default is True.
**kwargs (`Any`):
Additional keyword arguments.
Returns:
`dict`:
The result from the memory recording operation.
"""
results = await self.long_term_working_memory.add(
messages=messages,
agent_id=self.agent_id,
user_id=self.user_id,
run_id=self.run_id,
memory_type=(
memory_type
if memory_type is not None
else self.default_memory_type
),
infer=infer,
**kwargs,
)
return results
async def retrieve(
self,
msg: Msg | list[Msg] | None,
limit: int = 5,
**kwargs: Any,
) -> str:
"""Retrieve the content from the long-term memory.
Args:
msg (`Msg | list[Msg] | None`):
The message to search for in the memory, which should be
specific and concise, e.g. the person's name, the date, the
location, etc.
limit (`int`, optional):
The maximum number of memories to retrieve per search.
**kwargs (`Any`):
Additional keyword arguments.
Returns:
`str`:
The retrieved memory
"""
if isinstance(msg, Msg):
msg = [msg]
if not isinstance(msg, list) or not all(
isinstance(_, Msg) for _ in msg
):
raise TypeError(
"The input message must be a Msg or a list of Msg objects.",
)
msg_strs = [
json.dumps(_.to_dict()["content"], ensure_ascii=False) for _ in msg
]
results = []
for item in msg_strs:
result = await self.long_term_working_memory.search(
query=item,
agent_id=self.agent_id,
user_id=self.user_id,
run_id=self.run_id,
limit=limit,
)
if result:
results.extend([item["memory"] for item in result["results"]])
return "\n".join(results)
---- _mem0_utils.py ----
# -*- coding: utf-8 -*-
"""Utility classes for integrating AgentScope with mem0 library.
This module provides wrapper classes that allow AgentScope models to be used
with the mem0 library for long-term memory functionality.
"""
import asyncio
from typing import Any, Dict, List, Literal
from mem0.configs.embeddings.base import BaseEmbedderConfig
from mem0.configs.llms.base import BaseLlmConfig
from mem0.embeddings.base import EmbeddingBase
from mem0.llms.base import LLMBase
from ..embedding import EmbeddingModelBase
from ..model import ChatModelBase, ChatResponse
class AgentScopeLLM(LLMBase):
"""Wrapper for the AgentScope LLM.
This class is a wrapper for the AgentScope LLM. It is used to generate
responses using the AgentScope LLM in mem0.
"""
def __init__(self, config: BaseLlmConfig | None = None):
"""Initialize the AgentScopeLLM wrapper.
Args:
config (`BaseLlmConfig | None`, optional):
Configuration object for the LLM. Default is None.
"""
super().__init__(config)
if self.config.model is None:
raise ValueError("`model` parameter is required")
if not isinstance(self.config.model, ChatModelBase):
raise ValueError("`model` must be an instance of ChatModelBase")
self.agentscope_model = self.config.model
def generate_response(
self,
messages: List[Dict[str, str]],
response_format: Any | None = None,
tools: List[Dict] | None = None,
tool_choice: str = "auto",
) -> str:
"""Generate a response based on the given messages using agentscope.
Args:
messages (`List[Dict[str, str]]`):
List of message dicts containing 'role' and 'content'.
response_format (`Any | None`, optional):
Format of the response. Not used in AgentScope.
tools (`List[Dict] | None`, optional):
List of tools that the model can call. Not used in AgentScope.
tool_choice (`str`, optional):
Tool choice method. Not used in AgentScope.
Returns:
`str`:
The generated response.
"""
# pylint: disable=unused-argument
try:
# Convert the messages to AgentScope's format
agentscope_messages = []
for message in messages:
role = message["role"]
content = message["content"]
if role in ["system", "user", "assistant"]:
agentscope_messages.append(
{"role": role, "content": content},
)
if not agentscope_messages:
raise ValueError(
"No valid messages found in the messages list",
)
# Use the agentscope model to generate response (async call)
async def _async_call() -> ChatResponse:
# TODO: handle the streaming response or forbidden streaming
# mode
return await self.agentscope_model( # type: ignore
agentscope_messages,
tools=tools,
)
response = asyncio.run(_async_call())
# Extract text from the response content blocks
if not response.content:
return ""
# Collect all text from different block types
text_parts = []
thinking_parts = []
tool_parts = []
for block in response.content:
# Handle TextBlock
if isinstance(block, dict) and block.get("type") == "text":
text_parts.append(block.get("text", ""))
# Handle ThinkingBlock
elif (
isinstance(block, dict) and block.get("type") == "thinking"
):
thinking_parts.append(
f"[Thinking: {block.get('thinking', '')}]",
)
# Handle ToolUseBlock
elif (
isinstance(block, dict) and block.get("type") == "tool_use"
):
tool_name = block.get("name")
tool_input = block.get("input", {})
tool_parts.append(
f"[Tool: {tool_name} - {str(tool_input)}]",
)
# Combine all parts in order: thinking, text, tools
all_parts: list[str] = thinking_parts + text_parts + tool_parts
if all_parts:
return "\n".join(all_parts)
# If no recognized blocks found, try to convert the entire
# content to string
return str(response.content)
except Exception as e:
raise RuntimeError(
f"Error generating response using agentscope model: {str(e)}",
) from e
class AgentScopeEmbedding(EmbeddingBase):
"""Wrapper for the AgentScope Embedding model.
This class is a wrapper for the AgentScope Embedding model. It is used
to generate embeddings using the AgentScope Embedding model in mem0.
"""
def __init__(self, config: BaseEmbedderConfig | None = None):
"""Initialize the AgentScopeEmbedding wrapper.
Args:
config (`BaseEmbedderConfig | None`, optional):
Configuration object for the embedder. Default is None.
"""
super().__init__(config)
if self.config.model is None:
raise ValueError("`model` parameter is required")
if not isinstance(self.config.model, EmbeddingModelBase):
raise ValueError(
"`model` must be an instance of EmbeddingModelBase",
)
self.agentscope_model = self.config.model
def embed(
self,
text: str | List[str],
memory_action: Literal[ # pylint: disable=unused-argument
"add",
"search",
"update",
]
| None = None,
) -> List[float]:
"""Get the embedding for the given text using AgentScope.
Args:
text (`str | List[str]`):
The text to embed.
memory_action (`Literal["add", "search", "update"] | None`, \
optional):
The type of embedding to use. Must be one of "add", "search",
or "update". Defaults to None.
Returns:
`List[float]`:
The embedding vector.
"""
try:
# Convert single text to list for AgentScope embedding model
text_list = [text] if isinstance(text, str) else text
# Use the agentscope model to generate embedding (async call)
async def _async_call() -> Any:
response = await self.agentscope_model(text_list)
return response
response = asyncio.run(_async_call())
# Extract the embedding vector from the first Embedding object
# response.embeddings is a list of Embedding objects
# Each Embedding object has an 'embedding' attribute containing
# the vector
embedding = response.embeddings[0]
if embedding is None:
raise ValueError("Failed to extract embedding from response")
return embedding
except Exception as e:
raise RuntimeError(
f"Error generating embedding using agentscope model: {str(e)}",
) from e
---- _memory_base.py ----
# -*- coding: utf-8 -*-
"""The memory base class."""
from abc import abstractmethod
from typing import Any
from ..message import Msg
from ..module import StateModule
class MemoryBase(StateModule):
"""The base class for memory in agentscope."""
@abstractmethod
async def add(self, *args: Any, **kwargs: Any) -> None:
"""Add items to the memory."""
@abstractmethod
async def delete(self, *args: Any, **kwargs: Any) -> None:
"""Delete items from the memory."""
@abstractmethod
async def retrieve(self, *args: Any, **kwargs: Any) -> None:
"""Retrieve items from the memory."""
@abstractmethod
async def size(self) -> int:
"""Get the size of the memory."""
@abstractmethod
async def clear(self) -> None:
"""Clear the memory content."""
@abstractmethod
async def get_memory(self, *args: Any, **kwargs: Any) -> list[Msg]:
"""Get the memory content."""
@abstractmethod
def state_dict(self) -> dict:
"""Get the state dictionary of the memory."""
@abstractmethod
def load_state_dict(self, state_dict: dict, strict: bool = True) -> None:
"""Load the state dictionary of the memory."""
==== _reme ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The reme memory module."""
from ._reme_personal_long_term_memory import ReMePersonalLongTermMemory
from ._reme_task_long_term_memory import ReMeTaskLongTermMemory
from ._reme_tool_long_term_memory import ReMeToolLongTermMemory
__all__ = [
"ReMePersonalLongTermMemory",
"ReMeTaskLongTermMemory",
"ReMeToolLongTermMemory",
]
---- _reme_long_term_memory_base.py ----
# -*- coding: utf-8 -*-
"""Base long-term memory implementation using ReMe library.
This module provides a base class for long-term memory implementations
that integrate with the ReMe library. ReMe enables agents to maintain
persistent, searchable memories across sessions and contexts.
The module handles the integration between AgentScope's memory system and
the ReMe library, including:
- Model configuration and API credential management
- Context lifecycle management (async context managers)
- Graceful handling of missing dependencies
- Error handling with helpful installation instructions
Key Features:
- Supports both DashScope and OpenAI model providers
- Automatic extraction of API credentials and endpoints
- Flexible configuration via config files or kwargs
- Safe fallback behavior when reme_ai is not installed
Dependencies:
The ReMe library is an optional dependency that must be installed:
.. code-block:: bash
pip install reme-ai
Python 3.12 or greater is required to use ReMe.
For more information, visit: https://github.com/modelscope/reMe
Subclasses:
This base class is extended by specific memory type implementations:
- ReMeToolLongTermMemory: For tool execution patterns and guidelines
- ReMeTaskLongTermMemory: For task execution experiences and learnings
- ReMePersonalLongTermMemory: For user preferences and personal information
Example:
.. code-block:: python
from agentscope.models import OpenAIChatModel
from agentscope.embedding import OpenAITextEmbedding
from agentscope.memory._reme import ReMeToolLongTermMemory
# Initialize models
model = OpenAIChatModel(model_name="gpt-4", api_key="...")
embedding = OpenAITextEmbedding(
model_name="text-embedding-3-small", api_key="...")
# Create memory instance
memory = ReMeToolLongTermMemory(
agent_name="my_agent",
user_name="user_123",
model=model,
embedding_model=embedding
)
# Use memory in async context
async with memory:
# Record tool execution
await memory.record_to_memory(
thinking="This tool worked well for data processing",
content=['{"tool_name": "process_data", "success": true, ...}']
)
# Retrieve tool guidelines
result = await memory.retrieve_from_memory(
keywords=["process_data"]
)
"""
from abc import ABCMeta
from typing import Any
from .._long_term_memory_base import LongTermMemoryBase
from ...embedding import (
DashScopeTextEmbedding,
OpenAITextEmbedding,
)
from ...model import (
DashScopeChatModel,
OpenAIChatModel,
)
class ReMeLongTermMemoryBase(LongTermMemoryBase, metaclass=ABCMeta):
"""Base class for ReMe-based long-term memory implementations.
This class provides the foundation for integrating AgentScope with the ReMe
library, enabling agents to maintain and retrieve long-term memories across
different contexts.
The ReMe library must be installed separately:
pip install reme-ai
Requirements:
Python 3.12 or greater is required to use ReMe.
If the library is not installed, a warning will be issued during
initialization,
and runtime errors with installation instructions will be raised
when attempting
to use memory operations.
"""
def __init__(
self,
agent_name: str | None = None,
user_name: str | None = None,
run_name: str | None = None,
model: DashScopeChatModel | OpenAIChatModel | None = None,
embedding_model: (
DashScopeTextEmbedding | OpenAITextEmbedding | None
) = None,
reme_config_path: str | None = None,
**kwargs: Any,
) -> None:
"""Initialize the ReMe-based long-term memory.
This constructor sets up the connection to the ReMe
library and configures
the necessary models for memory operations. The ReMe app
will be initialized
with the provided model configurations.
Args:
agent_name (`str | None`, optional):
Name identifier for the agent. Used for organizing
memories by agent.
user_name (`str | None`, optional):
Unique identifier for the user or workspace. This maps
to workspace_id in ReMe and helps isolate memories across
different users/workspaces.
run_name (`str | None`, optional):
Name identifier for the current execution run or session.
model (`DashScopeChatModel | OpenAIChatModel | None`, optional):
The chat model to use for memory operations. The model's
API credentials and endpoint will be extracted and
passed to ReMe.
embedding_model (`DashScopeTextEmbedding | OpenAITextEmbedding | \
None`, optional):
The embedding model to use for semantic memory retrieval.
The model's API credentials and endpoint will be
extracted and passed to ReMe.
reme_config_path (`str | None`, optional):
Path to a custom ReMe configuration file. If not provided, ReMe
will use its default configuration.
**kwargs (`Any`):
Additional keyword arguments to pass to the
ReMeApp constructor.
These can include custom ReMe configuration parameters.
Raises:
`ValueError`:
If the provided model is not a DashScopeChatModel or
OpenAIChatModel, or if the embedding_model is not a
DashScopeTextEmbedding or OpenAITextEmbedding.
Note:
If the reme_ai library is not installed, a warning will be
issued and self.app will be set to None. Subsequent memory
operations will raise RuntimeError with installation
instructions.
Example:
.. code-block:: python
from agentscope.models import OpenAIChatModel
from agentscope.embedding import OpenAITextEmbedding
from agentscope.memory._reme import ReMeToolLongTermMemory
# Initialize models
model = OpenAIChatModel(
model_name="gpt-4",
api_key="your-api-key"
)
embedding = OpenAITextEmbedding(
model_name="text-embedding-3-small",
api_key="your-api-key"
)
# Create memory instance
memory = ReMeToolLongTermMemory(
agent_name="my_agent",
user_name="user_123",
run_name="session_001",
model=model,
embedding_model=embedding
)
# Use with async context manager
async with memory:
# Memory operations...
pass
"""
super().__init__()
# Store agent and workspace identifiers
self.agent_name = agent_name
# Maps to ReMe's workspace_id concept
self.workspace_id = user_name
self.run_name = run_name
# Build configuration arguments for ReMeApp
# These will be passed as command-line style config overrides
config_args = []
# Extract LLM API credentials based on model type
# DashScope uses a fixed endpoint, OpenAI can have custom base_url
if isinstance(model, DashScopeChatModel):
llm_api_base = "https://dashscope.aliyuncs.com/compatible-mode/v1"
llm_api_key = model.api_key
elif isinstance(model, OpenAIChatModel):
llm_api_base = str(getattr(model.client, "base_url", None))
llm_api_key = str(getattr(model.client, "api_key", None))
else:
raise ValueError(
f"model must be a DashScopeChatModel or "
f"OpenAIChatModel instance. "
f"Got {type(model).__name__} instead.",
)
# Extract model name and add to config if provided
llm_model_name = model.model_name
if llm_model_name:
config_args.append(f"llm.default.model_name={llm_model_name}")
# Extract embedding model API credentials based on type
# Similar to LLM, DashScope uses fixed endpoint,
# OpenAI can be customized
if isinstance(embedding_model, DashScopeTextEmbedding):
embedding_api_base = (
"https://dashscope.aliyuncs.com/compatible-mode/v1"
)
embedding_api_key = embedding_model.api_key
elif isinstance(embedding_model, OpenAITextEmbedding):
embedding_api_base = getattr(
embedding_model.client,
"base_url",
None,
)
embedding_api_key = getattr(
embedding_model.client,
"api_key",
None,
)
else:
raise ValueError(
"embedding_model must be a DashScopeTextEmbedding or "
"OpenAITextEmbedding instance. "
f"Got {type(embedding_model).__name__} instead.",
)
# Extract embedding model name and add to config if provided
embedding_model_name = embedding_model.model_name
if embedding_model_name:
config_args.append(
f"embedding_model.default.model_name={embedding_model_name}",
)
# Attempt to import and initialize ReMe
# If import fails, set app to None and issue a warning
# This allows the class to be instantiated even without
# reme_ai installed
try:
from reme_ai import ReMeApp
except ImportError as e:
raise ImportError(
"The 'reme_ai' library is required for ReMe-based "
"long-term memory. Please install it by `pip install reme-ai`,"
"and visit: https://github.com/modelscope/reMe for more "
"information.",
) from e
# Initialize ReMe with extracted configurations
self.app = ReMeApp(
*config_args, # Config overrides as positional args
llm_api_key=llm_api_key,
llm_api_base=llm_api_base,
embedding_api_key=embedding_api_key,
embedding_api_base=embedding_api_base,
# Optional custom config file
config_path=reme_config_path,
# Additional ReMe-specific configurations
**kwargs,
)
# Track if the app context is active (started via __aenter__)
self._app_started = False
async def __aenter__(self) -> "ReMeLongTermMemoryBase":
"""Async context manager entry point.
This method is called when entering an async context
(using 'async with'). It initializes the ReMe app context if
available, enabling memory operations within the context block.
Returns:
`ReMeLongTermMemoryBase`:
The memory instance itself, allowing it to be used in
the context.
Example:
.. code-block:: python
memory = ReMeToolLongTermMemory(
agent_name="my_agent",
model=model,
embedding_model=embedding
)
async with memory:
# Memory operations can be performed here
await memory.record_to_memory(
thinking="Recording tool usage",
content=[...]
)
"""
if self.app is not None:
await self.app.__aenter__()
self._app_started = True
return self
async def __aexit__(
self,
exc_type: Any,
exc_val: Any,
exc_tb: Any,
) -> None:
"""Async context manager exit point.
This method is called when exiting an async context (at the end
of 'async with' block or when an exception occurs). It properly
cleans up the ReMe app context and resources.
Args:
exc_type (`Any`):
The type of exception that occurred, if any. None if no
exception.
exc_val (`Any`):
The exception instance that occurred, if any. None if no
exception.
exc_tb (`Any`):
The traceback object for the exception, if any. None if
no exception.
.. note:: This method will gracefully handle the case where self.app
is None (reme_ai not installed) by skipping the cleanup but still
marking the app as stopped. It will also always set _app_started
to False, ensuring the memory state is properly reset.
Example:
.. code-block:: python
async with memory:
try:
# Memory operations
await memory.record_to_memory(...)
except Exception as e:
# __aexit__ will be called even if an exception occurs
print(f"Error: {e}")
# __aexit__ has been called and resources are cleaned up
"""
if self.app is not None:
await self.app.__aexit__(exc_type, exc_val, exc_tb)
self._app_started = False
---- _reme_personal_long_term_memory.py ----
# -*- coding: utf-8 -*-
"""Personal memory implementation using ReMe library.
This module provides a personal memory implementation that integrates
with the ReMe library to provide persistent personal memory storage and
retrieval capabilities for AgentScope agents.
Requirements:
Python 3.12 or greater is required to use ReMe.
"""
from typing import Any
from ._reme_long_term_memory_base import ReMeLongTermMemoryBase
from ..._logging import logger
from ...message import Msg, TextBlock
from ...tool import ToolResponse
class ReMePersonalLongTermMemory(ReMeLongTermMemoryBase):
"""Personal memory implementation using ReMe library.
Requirements:
Python 3.12 or greater is required to use ReMe.
"""
async def record_to_memory(
self,
thinking: str,
content: list[str],
**kwargs: Any,
) -> ToolResponse:
"""Record important user information to long-term memory.
Record important user information to long-term memory for future
reference.
Use this function to save user's personal information,
preferences, habits, and facts that you may need in future
conversations. This enables you to provide personalized and
contextually relevant responses.
When to record:
- User shares personal preferences (e.g., "I prefer homestays
when traveling")
- User mentions habits or routines (e.g., "I start work at 9 AM")
- User states likes/dislikes (e.g., "I enjoy drinking green tea")
- User provides personal facts (e.g., "I work as a software
engineer")
What to record: Be specific and structured. Include who, when,
where, what, why, and how when relevant.
Args:
thinking (`str`):
Your reasoning about why this information is worth
recording and how it might be useful later.
content (`list[str]`):
List of specific facts to remember. Each string should be
a clear, standalone piece of information. Examples:
["User prefers homestays in Hangzhou", "User likes
visiting West Lake in the morning"].
**kwargs (`Any`):
Additional keyword arguments for the recording operation.
Returns:
`ToolResponse`:
Confirmation message indicating successful memory
recording.
"""
logger.info(
"[ReMePersonalMemory] Entering record_to_memory - "
"thinking: %s, content: %s, kwargs: %s",
thinking,
content,
kwargs,
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
# Prepare messages for personal memory recording
messages = []
# Add thinking as a user message if provided
if thinking:
messages.append(
{
"role": "user",
"content": thinking,
},
)
# Add content items as user messages
for item in content:
messages.append(
{
"role": "user",
"content": item,
},
)
# Add a simple assistant acknowledgment
messages.append(
{
"role": "assistant",
"content": (
"I understand and will remember this "
"information."
),
},
)
result = await self.app.async_execute(
name="summary_personal_memory",
workspace_id=self.workspace_id,
trajectories=[
{
"messages": messages,
},
],
**kwargs,
)
# Extract metadata about stored memories if available
metadata = result.get("metadata", {})
memory_list = metadata.get("memory_list", [])
if memory_list:
summary_text = (
f"Successfully recorded {len(memory_list)} "
f"memory/memories to personal memory."
)
else:
summary_text = "Memory recording completed."
return ToolResponse(
content=[
TextBlock(
type="text",
text=summary_text,
),
],
metadata={"result": result},
)
except Exception as e:
logger.exception("Error recording memory: %s", str(e))
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error recording memory: {str(e)}",
),
],
)
async def retrieve_from_memory(
self,
keywords: list[str],
**kwargs: Any,
) -> ToolResponse:
"""Search and retrieve relevant information from long-term memory.
.. note:: You should call this function BEFORE answering
questions about the user's preferences, past information, or
personal details. This ensures you provide accurate information
based on stored memories rather than guessing.
Use this when:
- User asks "what do I like?", "what are my preferences?",
"what do you know about me?"
- User asks about their past behaviors, habits, or stated
preferences
- User refers to information they shared in previous
conversations
- You need to personalize responses based on user's history
Args:
keywords (`list[str]`):
Keywords to search for in memory. Be specific and use
multiple keywords for better results. Examples:
["travel preferences", "Hangzhou"], ["work habits",
"morning routine"], ["food preferences", "tea"].
**kwargs (`Any`):
Additional keyword arguments for the retrieval operation.
Returns:
`ToolResponse`:
Retrieved memories matching the keywords. If no memories
found, you'll receive a message indicating that.
"""
logger.info(
"[ReMePersonalMemory] Entering retrieve_from_memory - "
"keywords: %s, kwargs: %s",
keywords,
kwargs,
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
results = []
# Search for each keyword
limit = kwargs.get("limit", 3)
for keyword in keywords:
result = await self.app.async_execute(
name="retrieve_personal_memory",
workspace_id=self.workspace_id,
query=keyword,
top_k=limit,
**kwargs,
)
# Extract the answer from the result
answer = result.get("answer", "")
if answer:
results.append(f"Keyword '{keyword}':\n{answer}")
# Combine all results
if results:
combined_text = "\n\n".join(results)
else:
combined_text = "No memories found for the given keywords."
return ToolResponse(
content=[
TextBlock(
type="text",
text=combined_text,
),
],
)
except Exception as e:
logger.exception("Error retrieving memory: %s", str(e))
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error retrieving memory: {str(e)}",
),
],
)
async def record(
self,
msgs: list[Msg | None],
**kwargs: Any,
) -> None:
"""Record the content to the long-term memory.
This method converts AgentScope messages to ReMe's format and
records them using the personal memory flow.
Args:
msgs (`list[Msg | None]`):
The messages to record to memory.
**kwargs (`Any`):
Additional keyword arguments for the mem0 recording.
"""
if isinstance(msgs, Msg):
msgs = [msgs]
# Filter out None
msg_list = [_ for _ in msgs if _]
if not msg_list:
return
if not all(isinstance(_, Msg) for _ in msg_list):
raise TypeError(
"The input messages must be a list of Msg objects.",
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
# Convert AgentScope messages to ReMe format
messages = []
for msg in msg_list:
# Extract content as string
if isinstance(msg.content, str):
content_str = msg.content
elif isinstance(msg.content, list):
# Join content blocks into a single string
content_parts = []
for block in msg.content:
if isinstance(block, dict) and "text" in block:
content_parts.append(block["text"])
elif isinstance(block, dict) and "thinking" in block:
content_parts.append(block["thinking"])
content_str = "\n".join(content_parts)
else:
content_str = str(msg.content)
messages.append(
{
"role": msg.role,
"content": content_str,
},
)
await self.app.async_execute(
name="summary_personal_memory",
workspace_id=self.workspace_id,
trajectories=[
{
"messages": messages,
},
],
**kwargs,
)
except Exception as e:
# Log the error but don't raise to maintain compatibility
logger.exception("Error recording messages to memory: %s", str(e))
import warnings
warnings.warn(f"Error recording messages to memory: {str(e)}")
async def retrieve(
self,
msg: Msg | list[Msg] | None,
**kwargs: Any,
) -> str:
"""Retrieve the content from the long-term memory.
Args:
msg (`Msg | list[Msg] | None`):
The message to search for in the memory, which should be
specific and concise, e.g. the person's name, the date, the
location, etc.
**kwargs (`Any`):
Additional keyword arguments.
Returns:
`str`:
The retrieved memory as a string.
"""
if msg is None:
return ""
if isinstance(msg, Msg):
msg = [msg]
if not isinstance(msg, list) or not all(
isinstance(_, Msg) for _ in msg
):
raise TypeError(
"The input message must be a Msg or a list of Msg objects.",
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
# Only use the last message's content for retrieval
last_msg = msg[-1]
query = ""
if isinstance(last_msg.content, str):
query = last_msg.content
elif isinstance(last_msg.content, list):
# Extract text from content blocks
content_parts = []
for block in last_msg.content:
if isinstance(block, dict) and "text" in block:
content_parts.append(block["text"])
elif isinstance(block, dict) and "thinking" in block:
content_parts.append(block["thinking"])
query = "\n".join(content_parts)
if not query:
return ""
# Retrieve using the query from the last message
# Extract top_k from kwargs if available, default to 3
top_k = kwargs.get("top_k", 3)
result = await self.app.async_execute(
name="retrieve_personal_memory",
workspace_id=self.workspace_id,
query=query,
top_k=top_k,
**kwargs,
)
return result.get("answer", "")
except Exception as e:
logger.exception("Error retrieving memory: %s", str(e))
import warnings
warnings.warn(f"Error retrieving memory: {str(e)}")
return ""
---- _reme_task_long_term_memory.py ----
# -*- coding: utf-8 -*-
"""Task memory implementation using ReMe library.
This module provides a task memory implementation that integrates
with the ReMe library to learn from execution trajectories and
retrieve relevant task experiences.
Requirements:
Python 3.12 or greater is required to use ReMe.
"""
from typing import Any
from ._reme_long_term_memory_base import ReMeLongTermMemoryBase
from ..._logging import logger
from ...message import Msg, TextBlock
from ...tool import ToolResponse
class ReMeTaskLongTermMemory(ReMeLongTermMemoryBase):
"""Task memory implementation using ReMe library.
Task memory learns from execution trajectories and provides
retrieval of relevant task experiences.
Requirements:
Python 3.12 or greater is required to use ReMe.
"""
async def record_to_memory(
self,
thinking: str,
content: list[str],
**kwargs: Any,
) -> ToolResponse:
"""Record task execution experiences and learnings.
Record task execution experiences and learnings to long-term
memory.
Use this function to save valuable task-related knowledge that
can help with future similar tasks. This enables learning from
experience and improving over time.
When to record:
- After solving technical problems or completing tasks
- When discovering useful techniques or approaches
- After implementing solutions with specific steps
- When learning best practices or important lessons
What to record: Be detailed and actionable. Include:
- Task description and context
- Step-by-step execution details
- Specific techniques and methods used
- Results, outcomes, and effectiveness
- Lessons learned and considerations
Args:
thinking (`str`):
Your reasoning about why this task experience is valuable
and what makes it worth remembering for future reference.
content (`list[str]`):
List of specific task insights to remember. Each string
should be a clear, actionable piece of information.
Examples: ["Add indexes on WHERE clause columns to speed
up queries", "Use EXPLAIN ANALYZE to identify missing
indexes"].
**kwargs (`Any`):
Additional keyword arguments. Can include 'score' (float)
to indicate the quality/success of this approach
(default: 1.0).
Returns:
`ToolResponse`:
Confirmation message indicating successful memory
recording.
"""
logger.info(
"[ReMeTaskMemory] Entering record_to_memory - "
"thinking: %s, content: %s, kwargs: %s",
thinking,
content,
kwargs,
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
# Prepare messages for task memory recording
messages = []
# Add thinking as a user message if provided
if thinking:
messages.append(
{
"role": "user",
"content": thinking,
},
)
# Add content items as user-assistant pairs
for item in content:
messages.append(
{
"role": "user",
"content": item,
},
)
# Add a simple assistant acknowledgment
messages.append(
{
"role": "assistant",
"content": "Task information recorded.",
},
)
result = await self.app.async_execute(
name="summary_task_memory",
workspace_id=self.workspace_id,
trajectories=[
{
"messages": messages,
"score": kwargs.pop("score", 1.0),
},
],
**kwargs,
)
# Extract metadata if available
summary_text = (
f"Successfully recorded {len(content)} task memory/memories."
)
return ToolResponse(
content=[
TextBlock(
type="text",
text=summary_text,
),
],
metadata={"result": result},
)
except Exception as e:
logger.exception("Error recording task memory: %s", str(e))
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error recording task memory: {str(e)}",
),
],
)
async def retrieve_from_memory(
self,
keywords: list[str],
**kwargs: Any,
) -> ToolResponse:
"""Search and retrieve relevant task experiences.
Search and retrieve relevant task experiences from long-term
memory.
IMPORTANT: You should call this function BEFORE attempting to
solve problems or answer technical questions. This ensures you
leverage experiences and proven solutions rather than
starting from scratch.
Use this when:
- Asked to solve a technical problem or implement a solution
- Asked for recommendations, best practices, or approaches
- Asked "what do you know about...?" or "have you seen this
before?"
- Dealing with tasks that may be similar to experiences
- Need to recall specific techniques or methods
Benefits of retrieving first:
- Learn from past successes and mistakes
- Provide more accurate, battle-tested solutions
- Avoid reinventing the wheel
- Give consistent, informed recommendations
Args:
keywords (`list[str]`):
Keywords describing the task or problem domain. Be
specific and use technical terms. Examples:
["database optimization", "slow queries"], ["API design",
"rate limiting"], ["code refactoring", "Python"].
**kwargs (`Any`):
Additional keyword arguments. Can include 'top_k' (int)
to specify number of experiences to retrieve
(default: 3).
Returns:
`ToolResponse`:
Retrieved task experiences and learnings. If no relevant
experiences found, you'll receive a message indicating
that.
"""
logger.info(
"[ReMeTaskMemory] Entering retrieve_from_memory - "
"keywords: %s, kwargs: %s",
keywords,
kwargs,
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
results = []
# Search for each keyword
top_k = kwargs.get("top_k", 3)
for keyword in keywords:
result = await self.app.async_execute(
name="retrieve_task_memory",
workspace_id=self.workspace_id,
query=keyword,
top_k=top_k,
**kwargs,
)
# Extract the answer from the result
answer = result.get("answer", "")
if answer:
results.append(f"Keyword '{keyword}':\n{answer}")
# Combine all results
if results:
combined_text = "\n\n".join(results)
else:
combined_text = (
"No task experiences found for the given keywords."
)
return ToolResponse(
content=[
TextBlock(
type="text",
text=combined_text,
),
],
)
except Exception as e:
logger.exception("Error retrieving task memory: %s", str(e))
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error retrieving task memory: {str(e)}",
),
],
)
async def record(
self,
msgs: list[Msg | None],
**kwargs: Any,
) -> None:
"""Record the content to the task memory.
This method converts AgentScope messages to ReMe's format and
records them as a task execution trajectory.
Args:
msgs (`list[Msg | None]`):
The messages to record to memory.
**kwargs (`Any`):
Additional keyword arguments for the recording.
Can include 'score' (float) for trajectory scoring
(default: 1.0).
"""
if isinstance(msgs, Msg):
msgs = [msgs]
# Filter out None
msg_list = [_ for _ in msgs if _]
if not msg_list:
return
if not all(isinstance(_, Msg) for _ in msg_list):
raise TypeError(
"The input messages must be a list of Msg objects.",
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
# Convert AgentScope messages to ReMe format
messages = []
for msg in msg_list:
# Extract content as string
if isinstance(msg.content, str):
content_str = msg.content
elif isinstance(msg.content, list):
# Join content blocks into a single string
content_parts = []
for block in msg.content:
if isinstance(block, dict) and "text" in block:
content_parts.append(block["text"])
elif isinstance(block, dict) and "thinking" in block:
content_parts.append(block["thinking"])
content_str = "\n".join(content_parts)
else:
content_str = str(msg.content)
messages.append(
{
"role": msg.role,
"content": content_str,
},
)
# Extract score from kwargs if provided, default to 1.0
score = kwargs.pop("score", 1.0)
await self.app.async_execute(
name="summary_task_memory",
workspace_id=self.workspace_id,
trajectories=[
{
"messages": messages,
"score": score,
},
],
**kwargs,
)
except Exception as e:
# Log the error but don't raise to maintain compatibility
logger.exception(
"Error recording messages to task memory: %s",
str(e),
)
import warnings
warnings.warn(
f"Error recording messages to task memory: {str(e)}",
)
async def retrieve(
self,
msg: Msg | list[Msg] | None,
**kwargs: Any,
) -> str:
"""Retrieve relevant task experiences from memory.
Args:
msg (`Msg | list[Msg] | None`):
The message to search for relevant task experiences.
**kwargs (`Any`):
Additional keyword arguments.
Returns:
`str`:
The retrieved task experiences as a string.
"""
if msg is None:
return ""
if isinstance(msg, Msg):
msg = [msg]
if not isinstance(msg, list) or not all(
isinstance(_, Msg) for _ in msg
):
raise TypeError(
"The input message must be a Msg or a list of Msg objects.",
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
# Only use the last message's content for retrieval
last_msg = msg[-1]
query = ""
if isinstance(last_msg.content, str):
query = last_msg.content
elif isinstance(last_msg.content, list):
# Extract text from content blocks
content_parts = []
for block in last_msg.content:
if isinstance(block, dict) and "text" in block:
content_parts.append(block["text"])
elif isinstance(block, dict) and "thinking" in block:
content_parts.append(block["thinking"])
query = "\n".join(content_parts)
if not query:
return ""
# Retrieve using the query from the last message
top_k = kwargs.get("top_k", 3)
result = await self.app.async_execute(
name="retrieve_task_memory",
workspace_id=self.workspace_id,
query=query,
top_k=top_k,
**kwargs,
)
return result.get("answer", "")
except Exception as e:
logger.exception("Error retrieving task memory: %s", str(e))
import warnings
warnings.warn(f"Error retrieving task memory: {str(e)}")
return ""
---- _reme_tool_long_term_memory.py ----
# -*- coding: utf-8 -*-
"""Tool memory implementation using ReMe library.
This module provides a tool memory implementation that integrates
with the ReMe library to record tool execution results and retrieve
tool usage guidelines.
Requirements:
Python 3.12 or greater is required to use ReMe.
"""
from typing import Any
from ._reme_long_term_memory_base import ReMeLongTermMemoryBase
from ..._logging import logger
from ...message import Msg, TextBlock
from ...tool import ToolResponse
class ReMeToolLongTermMemory(ReMeLongTermMemoryBase):
"""Tool memory implementation using ReMe library.
Tool memory records tool execution results and generates usage
guidelines from the execution history.
Requirements:
Python 3.12 or greater is required to use ReMe.
"""
async def record_to_memory(
self,
thinking: str,
content: list[str],
**kwargs: Any,
) -> ToolResponse:
"""Record tool execution results to build tool usage patterns.
Record tool execution results to build a knowledge base of tool
usage patterns.
Use this function after successfully using tools to capture
execution details, results, and performance metrics. Over time,
this builds comprehensive usage guidelines and best practices
for each tool.
When to record:
- After successfully executing any tool
- After tool failures (to learn what doesn't work)
- When discovering effective parameter combinations
- After noteworthy tool usage patterns
What to record: Each tool execution should include complete
execution details.
Args:
thinking (`str`):
Your reasoning about why this tool execution is worth
recording. Mention what worked well, what could be
improved, or lessons learned.
content (`list[str]`):
List of JSON strings, each representing a tool execution.
Each JSON must have these fields:
- create_time: Timestamp in format "YYYY-MM-DD HH:MM:SS"
- tool_name: Name of the tool executed
- input: Input parameters as a dict
- output: Tool's output as a string
- token_cost: Token cost (integer)
- success: Whether execution succeeded (boolean)
- time_cost: Execution time in seconds (float)
Example: '{"create_time": "2024-01-01 10:00:00",
"tool_name": "search", "input": {"query": "Python"},
"output": "Found 10 results", "token_cost": 100,
"success": true, "time_cost": 1.2}'
**kwargs (`Any`):
Additional keyword arguments for the recording operation.
Returns:
`ToolResponse`:
Confirmation message with number of executions recorded
and guidelines generated.
"""
logger.info(
"[ReMeToolMemory] Entering record_to_memory - "
"thinking: %s, content: %s, kwargs: %s",
thinking,
content,
kwargs,
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
import json
# Parse each content item as a tool_call_result
tool_call_results = []
tool_names_set = set()
for item in content:
try:
# Parse JSON string to dict
tool_call_result = json.loads(item)
tool_call_results.append(tool_call_result)
# Track tool names for summary
if "tool_name" in tool_call_result:
tool_names_set.add(tool_call_result["tool_name"])
except json.JSONDecodeError as e:
# Skip invalid JSON items
import warnings
warnings.warn(
f"Failed to parse tool call result JSON: {item}. "
f"Error: {str(e)}",
)
continue
if not tool_call_results:
return ToolResponse(
content=[
TextBlock(
type="text",
text="No valid tool call results to record.",
),
],
)
# First, add the tool call results
await self.app.async_execute(
name="add_tool_call_result",
workspace_id=self.workspace_id,
tool_call_results=tool_call_results,
**kwargs,
)
# Then, summarize the tool memory for the affected tools
if tool_names_set:
tool_names_list = list(tool_names_set)
await self.app.async_execute(
name="summary_tool_memory",
workspace_id=self.workspace_id,
tool_names=tool_names_list,
**kwargs,
)
num_results = len(tool_call_results)
summary_text = (
f"Successfully recorded {num_results} tool execution "
f"result{'s' if num_results > 1 else ''} and generated "
f"usage guidelines."
)
return ToolResponse(
content=[
TextBlock(
type="text",
text=summary_text,
),
],
)
except Exception as e:
logger.exception("Error recording tool memory: %s", str(e))
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error recording tool memory: {str(e)}",
),
],
)
async def retrieve_from_memory(
self,
keywords: list[str],
**kwargs: Any,
) -> ToolResponse:
"""Retrieve usage guidelines and best practices for tools.
Retrieve usage guidelines and best practices for specific tools.
.. note:: You should call this function BEFORE using a tool,
especially if you're uncertain about its proper usage or want to
follow established best practices. This retrieves synthesized
guidelines based on past tool executions.
Use this when:
- About to use a tool and want to know the best practices
- Uncertain about tool parameters or usage patterns
- Want to learn from past successful/failed tool executions
- User asks "how should I use this tool?" or "what's the best
way to..."
- Need to understand tool performance characteristics or
limitations
Benefits of retrieving first:
- Learn from accumulated tool usage experience
- Avoid common mistakes and pitfalls
- Use optimal parameter combinations
- Understand tool performance and cost characteristics
- Follow established best practices
Args:
keywords (`list[str]`):
List of tool names to retrieve guidelines for. Use the
exact tool names. Examples: ["search"],
["database_query", "cache_get"], ["api_call"].
**kwargs (`Any`):
Additional keyword arguments for the retrieval operation.
Returns:
`ToolResponse`:
Retrieved usage guidelines and best practices for the
specified tools. If no guidelines exist yet, you'll
receive a message indicating that.
"""
logger.info(
"[ReMeToolMemory] Entering retrieve_from_memory - "
"keywords: %s, kwargs: %s",
keywords,
kwargs,
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
# Join all tool names with comma
tool_names = ",".join(keywords)
# Retrieve tool guidelines for all tools at once
result = await self.app.async_execute(
name="retrieve_tool_memory",
workspace_id=self.workspace_id,
tool_names=tool_names,
**kwargs,
)
# Extract the answer from the result
answer = result.get("answer", "")
if answer:
combined_text = answer
else:
combined_text = f"No tool guidelines found for: {tool_names}"
return ToolResponse(
content=[
TextBlock(
type="text",
text=combined_text,
),
],
)
except Exception as e:
logger.exception("Error retrieving tool memory: %s", str(e))
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error retrieving tool memory: {str(e)}",
),
],
)
def _extract_content_from_messages(self, msg_list: list[Msg]) -> list[str]:
"""Extract content strings from messages.
Args:
msg_list (`list[Msg]`):
List of messages to extract content from.
Returns:
`list[str]`:
List of extracted content strings.
"""
content_list = []
for msg in msg_list:
if isinstance(msg.content, str):
content_list.append(msg.content)
elif isinstance(msg.content, list):
content_list.extend(
self._extract_text_from_blocks(msg.content),
)
return content_list
def _extract_text_from_blocks(self, blocks: list) -> list[str]:
"""Extract text from content blocks.
Args:
blocks (`list`):
List of content blocks.
Returns:
`list[str]`:
List of extracted text strings.
"""
texts = []
for block in blocks:
if isinstance(block, dict) and block.get("type") == "text":
texts.append(block.get("text", ""))
elif isinstance(block, str):
texts.append(block)
return texts
def _parse_tool_call_results(
self,
content_list: list[str],
) -> tuple[list[dict], set[str]]:
"""Parse JSON content strings into tool call results.
Args:
content_list (`list[str]`):
List of JSON strings to parse.
Returns:
`tuple[list[dict], set[str]]`:
Tuple of (tool_call_results, tool_names_set).
"""
import json
import warnings
tool_call_results = []
tool_names_set = set()
for item in content_list:
try:
tool_call_result = json.loads(item)
tool_call_results.append(tool_call_result)
if "tool_name" in tool_call_result:
tool_names_set.add(tool_call_result["tool_name"])
except json.JSONDecodeError as e:
warnings.warn(
f"Failed to parse tool call result JSON: {item}. "
f"Error: {str(e)}",
)
return tool_call_results, tool_names_set
async def record(
self,
msgs: list[Msg | None],
**kwargs: Any,
) -> None:
"""Record the content to the tool memory.
This method extracts content from messages and treats them as
JSON strings representing tool_call_results, similar to
record_to_memory.
Args:
msgs (`list[Msg | None]`):
The messages to record to memory. Each message's content
should be a JSON string or list of JSON strings
representing tool_call_results.
**kwargs (`Any`):
Additional keyword arguments for the recording.
"""
if isinstance(msgs, Msg):
msgs = [msgs]
# Filter out None
msg_list = [_ for _ in msgs if _]
if not msg_list:
return
if not all(isinstance(_, Msg) for _ in msg_list):
raise TypeError(
"The input messages must be a list of Msg objects.",
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
# Extract content from messages and parse as tool_call_results
content_list = self._extract_content_from_messages(msg_list)
if not content_list:
return
# Parse each content item as a tool_call_result
tool_call_results, tool_names_set = self._parse_tool_call_results(
content_list,
)
if not tool_call_results:
return
# First, add the tool call results
await self.app.async_execute(
name="add_tool_call_result",
workspace_id=self.workspace_id,
tool_call_results=tool_call_results,
**kwargs,
)
# Then, summarize the tool memory for the affected tools
if tool_names_set:
tool_names_list = list(tool_names_set)
await self.app.async_execute(
name="summary_tool_memory",
workspace_id=self.workspace_id,
tool_names=tool_names_list,
**kwargs,
)
except Exception as e:
# Log the error but don't raise to maintain compatibility
logger.exception(
"Error recording tool messages to memory: %s",
str(e),
)
import warnings
warnings.warn(
f"Error recording tool messages to memory: {str(e)}",
)
def _extract_tool_names_from_message(self, msg: Msg) -> str:
"""Extract tool names from a message.
Args:
msg (`Msg`):
Message to extract tool names from.
Returns:
`str`:
Extracted tool names as a string.
"""
if isinstance(msg.content, str):
return msg.content
if isinstance(msg.content, list):
content_parts = []
for block in msg.content:
if isinstance(block, dict) and "text" in block:
content_parts.append(block["text"])
return " ".join(content_parts)
return ""
def _format_retrieve_result(self, result: Any) -> str:
"""Format the retrieve result into a string.
Args:
result (`Any`):
Result from the retrieve operation.
Returns:
`str`:
Formatted result string.
"""
if isinstance(result, dict) and "answer" in result:
return result["answer"]
if isinstance(result, str):
return result
return str(result)
async def retrieve(
self,
msg: Msg | list[Msg] | None,
**kwargs: Any,
) -> str:
"""Retrieve tool guidelines from memory.
Retrieve tool guidelines from memory based on message content.
Args:
msg (`Msg | list[Msg] | None`):
The message containing tool names or queries to
retrieve guidelines for.
**kwargs (`Any`):
Additional keyword arguments.
Returns:
`str`:
The retrieved tool guidelines as a string.
"""
if msg is None:
return ""
if isinstance(msg, Msg):
msg = [msg]
if not isinstance(msg, list) or not all(
isinstance(_, Msg) for _ in msg
):
raise TypeError(
"The input message must be a Msg or a list of Msg objects.",
)
if not self._app_started:
raise RuntimeError(
"ReMeApp context not started. "
"Please use 'async with' to initialize the app.",
)
try:
# Extract tool names from the last message
last_msg = msg[-1]
tool_names = self._extract_tool_names_from_message(last_msg)
if not tool_names:
return ""
# Retrieve tool guidelines
result = await self.app.async_execute(
name="retrieve_tool_memory",
workspace_id=self.workspace_id,
tool_names=tool_names,
**kwargs,
)
return self._format_retrieve_result(result)
except Exception as e:
logger.exception("Error retrieving tool guidelines: %s", str(e))
import warnings
warnings.warn(f"Error retrieving tool guidelines: {str(e)}")
return ""
==== message ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The message module in agentscope."""
from ._message_block import (
ContentBlock,
TextBlock,
ThinkingBlock,
ToolUseBlock,
ToolResultBlock,
ImageBlock,
AudioBlock,
VideoBlock,
Base64Source,
URLSource,
)
from ._message_base import Msg
__all__ = [
"TextBlock",
"ThinkingBlock",
"Base64Source",
"URLSource",
"ImageBlock",
"AudioBlock",
"VideoBlock",
"ToolUseBlock",
"ToolResultBlock",
"ContentBlock",
"Msg",
]
---- _message_base.py ----
# -*- coding: utf-8 -*-
"""The message class in agentscope."""
from datetime import datetime
from typing import Literal, List, overload, Sequence
import shortuuid
from . import ToolResultBlock
from ._message_block import (
TextBlock,
ToolUseBlock,
ImageBlock,
AudioBlock,
ContentBlock,
VideoBlock,
ThinkingBlock,
)
from ..types import JSONSerializableObject
class Msg:
"""The message class in agentscope."""
def __init__(
self,
name: str,
content: str | Sequence[ContentBlock],
role: Literal["user", "assistant", "system"],
metadata: dict[str, JSONSerializableObject] | None = None,
timestamp: str | None = None,
invocation_id: str | None = None,
) -> None:
"""Initialize the Msg object.
Args:
name (`str`):
The name of the message sender.
content (`str | list[ContentBlock]`):
The content of the message.
role (`Literal["user", "assistant", "system"]`):
The role of the message sender.
metadata (`dict[str, JSONSerializableObject] | None`, optional):
The metadata of the message, e.g. structured output.
timestamp (`str | None`, optional):
The created timestamp of the message. If not given, the
timestamp will be set automatically.
invocation_id (`str | None`, optional):
The related API invocation id, if any. This is useful for
tracking the message in the context of an API call.
"""
self.name = name
assert isinstance(
content,
(list, str),
), "The content must be a string or a list of content blocks."
self.content = content
assert role in ["user", "assistant", "system"]
self.role = role
self.metadata = metadata
self.id = shortuuid.uuid()
self.timestamp = (
timestamp
or datetime.now().strftime(
"%Y-%m-%d %H:%M:%S.%f",
)[:-3]
)
self.invocation_id = invocation_id
def to_dict(self) -> dict:
"""Convert the message into JSON dict data."""
return {
"id": self.id,
"name": self.name,
"role": self.role,
"content": self.content,
"metadata": self.metadata,
"timestamp": self.timestamp,
}
@classmethod
def from_dict(cls, json_data: dict) -> "Msg":
"""Load a message object from the given JSON data."""
new_obj = cls(
name=json_data["name"],
content=json_data["content"],
role=json_data["role"],
metadata=json_data.get("metadata", None),
timestamp=json_data.get("timestamp", None),
invocation_id=json_data.get("invocation_id", None),
)
new_obj.id = json_data.get("id", new_obj.id)
return new_obj
def has_content_blocks(
self,
block_type: Literal[
"text",
"tool_use",
"tool_result",
"image",
"audio",
"video",
]
| None = None,
) -> bool:
"""Check if the message has content blocks of the given type.
Args:
block_type (Literal["text", "tool_use", "tool_result", "image", \
"audio", "video"] | None, defaults to None):
The type of the block to be checked. If `None`, it will
check if there are any content blocks.
"""
return len(self.get_content_blocks(block_type)) > 0
def get_text_content(self) -> str | None:
"""Get the pure text blocks from the message content."""
if isinstance(self.content, str):
return self.content
gathered_text = None
for block in self.content:
if block.get("type") == "text":
if gathered_text is None:
gathered_text = str(block.get("text"))
else:
gathered_text += block.get("text")
return gathered_text
@overload
def get_content_blocks(
self,
block_type: Literal["text"],
) -> List[TextBlock]:
...
@overload
def get_content_blocks(
self,
block_type: Literal["tool_use"],
) -> List[ToolUseBlock]:
...
@overload
def get_content_blocks(
self,
block_type: Literal["tool_result"],
) -> List[ToolResultBlock]:
...
@overload
def get_content_blocks(
self,
block_type: Literal["image"],
) -> List[ImageBlock]:
...
@overload
def get_content_blocks(
self,
block_type: Literal["audio"],
) -> List[AudioBlock]:
...
@overload
def get_content_blocks(
self,
block_type: Literal["video"],
) -> List[VideoBlock]:
...
@overload
def get_content_blocks(
self,
block_type: None = None,
) -> List[ContentBlock]:
...
def get_content_blocks(
self,
block_type: Literal[
"text",
"thinking",
"tool_use",
"tool_result",
"image",
"audio",
"video",
]
| None = None,
) -> (
List[ContentBlock]
| List[TextBlock]
| List[ThinkingBlock]
| List[ToolUseBlock]
| List[ToolResultBlock]
| List[ImageBlock]
| List[AudioBlock]
| List[VideoBlock]
):
"""Get the content in block format. If the content is a string,
it will be converted to a text block.
Args:
block_type (`Literal["text", "thinking", "tool_use", \
"tool_result", "image", "audio", "video"] | None`, optional):
The type of the block to be extracted. If `None`, all blocks
will be returned.
Returns:
`List[ContentBlock]`:
The content blocks.
"""
blocks = []
if isinstance(self.content, str):
blocks.append(
TextBlock(type="text", text=self.content),
)
else:
blocks = self.content or []
if block_type is not None:
blocks = [_ for _ in blocks if _["type"] == block_type]
return blocks
def __repr__(self) -> str:
"""Get the string representation of the message."""
return (
f"Msg(id='{self.id}', "
f"name='{self.name}', "
f"content={repr(self.content)}, "
f"role='{self.role}', "
f"metadata={repr(self.metadata)}, "
f"timestamp='{self.timestamp}', "
f"invocation_id='{self.invocation_id}')"
)
---- _message_block.py ----
# -*- coding: utf-8 -*-
# pylint: disable=R0901
"""The content blocks of messages"""
from typing import Literal, List
from typing_extensions import TypedDict, Required
class TextBlock(TypedDict, total=False):
"""The text block."""
type: Required[Literal["text"]]
"""The type of the block"""
text: str
"""The text content"""
class ThinkingBlock(TypedDict, total=False):
"""The thinking block."""
type: Required[Literal["thinking"]]
"""The type of the block"""
thinking: str
class Base64Source(TypedDict, total=False):
"""The base64 source"""
type: Required[Literal["base64"]]
"""The type of the src, must be `base64`"""
media_type: Required[str]
"""The media type of the data, e.g. `image/jpeg` or `audio/mpeg`"""
data: Required[str]
"""The base64 data, in format of RFC 2397"""
class URLSource(TypedDict, total=False):
"""The URL source"""
type: Required[Literal["url"]]
"""The type of the src, must be `url`"""
url: Required[str]
"""The URL of the image or audio"""
class ImageBlock(TypedDict, total=False):
"""The image block"""
type: Required[Literal["image"]]
"""The type of the block"""
source: Required[Base64Source | URLSource]
"""The src of the image"""
class AudioBlock(TypedDict, total=False):
"""The audio block"""
type: Required[Literal["audio"]]
"""The type of the block"""
source: Required[Base64Source | URLSource]
"""The src of the audio"""
class VideoBlock(TypedDict, total=False):
"""The video block"""
type: Required[Literal["video"]]
"""The type of the block"""
source: Required[Base64Source | URLSource]
"""The src of the audio"""
class ToolUseBlock(TypedDict, total=False):
"""The tool use block."""
type: Required[Literal["tool_use"]]
"""The type of the block, must be `tool_use`"""
id: Required[str]
"""The identity of the tool call"""
name: Required[str]
"""The name of the tool"""
input: Required[dict[str, object]]
"""The input of the tool"""
class ToolResultBlock(TypedDict, total=False):
"""The tool result block."""
type: Required[Literal["tool_result"]]
"""The type of the block"""
id: Required[str]
"""The identity of the tool call result"""
output: Required[str | List[TextBlock | ImageBlock | AudioBlock]]
"""The output of the tool function"""
name: Required[str]
"""The name of the tool function"""
# The content block
ContentBlock = (
ToolUseBlock
| ToolResultBlock
| TextBlock
| ThinkingBlock
| ImageBlock
| AudioBlock
| VideoBlock
)
==== model ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The model module."""
from ._model_base import ChatModelBase
from ._model_response import ChatResponse
from ._dashscope_model import DashScopeChatModel
from ._openai_model import OpenAIChatModel
from ._anthropic_model import AnthropicChatModel
from ._ollama_model import OllamaChatModel
from ._gemini_model import GeminiChatModel
__all__ = [
"ChatModelBase",
"ChatResponse",
"DashScopeChatModel",
"OpenAIChatModel",
"AnthropicChatModel",
"OllamaChatModel",
"GeminiChatModel",
]
---- _anthropic_model.py ----
# -*- coding: utf-8 -*-
# pylint: disable=too-many-branches, too-many-statements
"""The Anthropic API model classes."""
from datetime import datetime
from typing import (
Any,
AsyncGenerator,
TYPE_CHECKING,
List,
Literal,
Type,
)
from collections import OrderedDict
from pydantic import BaseModel
from ._model_base import ChatModelBase
from ._model_response import ChatResponse
from ._model_usage import ChatUsage
from .._logging import logger
from .._utils._common import (
_json_loads_with_repair,
_create_tool_from_base_model,
)
from ..message import TextBlock, ToolUseBlock, ThinkingBlock
from ..tracing import trace_llm
from ..types._json import JSONSerializableObject
if TYPE_CHECKING:
from anthropic.types.message import Message
from anthropic import AsyncStream
else:
Message = "anthropic.types.message.Message"
AsyncStream = "anthropic.AsyncStream"
class AnthropicChatModel(ChatModelBase):
"""The Anthropic model wrapper for AgentScope."""
def __init__(
self,
model_name: str,
api_key: str | None = None,
max_tokens: int = 2048,
stream: bool = True,
thinking: dict | None = None,
client_args: dict | None = None,
generate_kwargs: dict[str, JSONSerializableObject] | None = None,
) -> None:
"""Initialize the Anthropic chat model.
Args:
model_name (`str`):
The model names.
api_key (`str`):
The anthropic API key.
stream (`bool`):
The streaming output or not
max_tokens (`int`):
Limit the maximum token count the model can generate.
thinking (`dict | None`, default `None`):
Configuration for Claude's internal reasoning process.
.. code-block:: python
:caption: Example of thinking
{
"type": "enabled" | "disabled",
"budget_tokens": 1024
}
client_args (`dict | None`, optional):
The extra keyword arguments to initialize the Anthropic client.
generate_kwargs (`dict[str, JSONSerializableObject] | None`, \
optional):
The extra keyword arguments used in Gemini API generation,
e.g. `temperature`, `seed`.
"""
try:
import anthropic
except ImportError as e:
raise ImportError(
"Please install the `anthropic` package by running "
"`pip install anthropic`.",
) from e
super().__init__(model_name, stream)
self.client = anthropic.AsyncAnthropic(
api_key=api_key,
**(client_args or {}),
)
self.max_tokens = max_tokens
self.thinking = thinking
self.generate_kwargs = generate_kwargs or {}
@trace_llm
async def __call__(
self,
messages: list[dict[str, Any]],
tools: list[dict] | None = None,
tool_choice: Literal["auto", "none", "any", "required"]
| str
| None = None,
structured_model: Type[BaseModel] | None = None,
**generate_kwargs: Any,
) -> ChatResponse | AsyncGenerator[ChatResponse, None]:
"""Get the response from Anthropic chat completions API by the given
arguments.
Args:
messages (`list[dict]`):
A list of dictionaries, where `role` and `content` fields are
required, and `name` field is optional.
tools (`list[dict]`, default `None`):
The tools JSON schemas that in format of:
.. code-block:: python
:caption: Example of tools JSON schemas
[
{
"type": "function",
"function": {
"name": "xxx",
"description": "xxx",
"parameters": {
"type": "object",
"properties": {
"param1": {
"type": "string",
"description": "..."
},
# Add more parameters as needed
},
"required": ["param1"]
}
},
# More schemas here
]
tool_choice (`Literal["auto", "none", "any", "required"] | str \
| None`, default `None`):
Controls which (if any) tool is called by the model.
Can be "auto", "none", "any", "required", or specific tool
name. For more details, please refer to
https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/implement-tool-use
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output. When provided, the model will be forced
to return data that conforms to this schema by automatically
converting the BaseModel to a tool function and setting
`tool_choice` to enforce its usage. This enables structured
output generation.
.. note:: When `structured_model` is specified,
both `tools` and `tool_choice` parameters are ignored,
and the model will only perform structured output
generation without calling any other tools.
**generate_kwargs (`Any`):
The keyword arguments for Anthropic chat completions API,
e.g. `temperature`, `top_p`, etc. Please
refer to the Anthropic API documentation for more details.
Returns:
`ChatResponse | AsyncGenerator[ChatResponse, None]`:
The response from the Anthropic chat completions API."""
kwargs: dict[str, Any] = {
"model": self.model_name,
"max_tokens": self.max_tokens,
"stream": self.stream,
**self.generate_kwargs,
**generate_kwargs,
}
if self.thinking and "thinking" not in kwargs:
kwargs["thinking"] = self.thinking
if tools:
kwargs["tools"] = self._format_tools_json_schemas(tools)
if tool_choice:
self._validate_tool_choice(tool_choice, tools)
kwargs["tool_choice"] = self._format_tool_choice(tool_choice)
if structured_model:
if tools or tool_choice:
logger.warning(
"structured_model is provided. Both 'tools' and "
"'tool_choice' parameters will be overridden and "
"ignored. The model will only perform structured output "
"generation without calling any other tools.",
)
format_tool = _create_tool_from_base_model(structured_model)
kwargs["tools"] = self._format_tools_json_schemas(
[format_tool],
)
kwargs["tool_choice"] = self._format_tool_choice(
format_tool["function"]["name"],
)
# Extract the system message
if messages[0]["role"] == "system":
kwargs["system"] = messages[0]["content"]
messages = messages[1:]
kwargs["messages"] = messages
start_datetime = datetime.now()
response = await self.client.messages.create(**kwargs)
if self.stream:
return self._parse_anthropic_stream_completion_response(
start_datetime,
response,
structured_model,
)
# Non-streaming response
parsed_response = await self._parse_anthropic_completion_response(
start_datetime,
response,
structured_model,
)
return parsed_response
async def _parse_anthropic_completion_response(
self,
start_datetime: datetime,
response: Message,
structured_model: Type[BaseModel] | None = None,
) -> ChatResponse:
"""Given an Anthropic Message object, extract the content blocks and
usages from it.
Args:
start_datetime (`datetime`):
The start datetime of the response generation.
response (`Message`):
Anthropic Message object to parse.
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
Returns:
ChatResponse (`ChatResponse`):
A ChatResponse object containing the content blocks and usage.
.. note::
If `structured_model` is not `None`, the expected structured output
will be stored in the metadata of the `ChatResponse`.
"""
content_blocks: List[ThinkingBlock | TextBlock | ToolUseBlock] = []
metadata = None
if hasattr(response, "content") and response.content:
for content_block in response.content:
if (
hasattr(content_block, "type")
and content_block.type == "thinking"
):
thinking_block = ThinkingBlock(
type="thinking",
thinking=content_block.thinking,
)
thinking_block["signature"] = content_block.signature
content_blocks.append(thinking_block)
elif hasattr(content_block, "text"):
content_blocks.append(
TextBlock(
type="text",
text=content_block.text,
),
)
elif (
hasattr(content_block, "type")
and content_block.type == "tool_use"
):
content_blocks.append(
ToolUseBlock(
type="tool_use",
id=content_block.id,
name=content_block.name,
input=content_block.input,
),
)
if structured_model:
metadata = content_block.input
usage = None
if response.usage:
usage = ChatUsage(
input_tokens=response.usage.input_tokens,
output_tokens=response.usage.output_tokens,
time=(datetime.now() - start_datetime).total_seconds(),
)
parsed_response = ChatResponse(
content=content_blocks,
usage=usage,
metadata=metadata,
)
return parsed_response
async def _parse_anthropic_stream_completion_response(
self,
start_datetime: datetime,
response: AsyncStream,
structured_model: Type[BaseModel] | None = None,
) -> AsyncGenerator[ChatResponse, None]:
"""Given an Anthropic streaming response, extract the content blocks
and usages from it and yield ChatResponse objects.
Args:
start_datetime (`datetime`):
The start datetime of the response generation.
response (`AsyncStream`):
Anthropic AsyncStream object to parse.
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
Returns:
`AsyncGenerator[ChatResponse, None]`:
An async generator that yields ChatResponse objects containing
the content blocks and usage information for each chunk in
the streaming response.
.. note::
If `structured_model` is not `None`, the expected structured output
will be stored in the metadata of the `ChatResponse`.
"""
usage = None
text_buffer = ""
thinking_buffer = ""
thinking_signature = ""
tool_calls = OrderedDict()
tool_call_buffers = {}
res = None
metadata = None
async for event in response:
content_changed = False
thinking_changed = False
if event.type == "message_start":
message = event.message
if message.usage:
usage = ChatUsage(
input_tokens=message.usage.input_tokens,
output_tokens=getattr(
message.usage,
"output_tokens",
0,
),
time=(datetime.now() - start_datetime).total_seconds(),
)
elif event.type == "content_block_start":
if event.content_block.type == "tool_use":
block_index = event.index
tool_block = event.content_block
tool_calls[block_index] = {
"type": "tool_use",
"id": tool_block.id,
"name": tool_block.name,
"input": "",
}
tool_call_buffers[block_index] = ""
content_changed = True
elif event.type == "content_block_delta":
block_index = event.index
delta = event.delta
if delta.type == "text_delta":
text_buffer += delta.text
content_changed = True
elif delta.type == "thinking_delta":
thinking_buffer += delta.thinking
thinking_changed = True
elif delta.type == "signature_delta":
thinking_signature = delta.signature
elif (
delta.type == "input_json_delta"
and block_index in tool_calls
):
tool_call_buffers[block_index] += delta.partial_json or ""
tool_calls[block_index]["input"] = tool_call_buffers[
block_index
]
content_changed = True
elif event.type == "message_delta":
if event.usage and usage:
usage.output_tokens = event.usage.output_tokens
if (thinking_changed or content_changed) and usage:
contents: list = []
if thinking_buffer:
thinking_block = ThinkingBlock(
type="thinking",
thinking=thinking_buffer,
)
thinking_block["signature"] = thinking_signature
contents.append(thinking_block)
if text_buffer:
contents.append(
TextBlock(
type="text",
text=text_buffer,
),
)
for block_index, tool_call in tool_calls.items():
input_str = tool_call["input"]
try:
input_obj = _json_loads_with_repair(input_str or "{}")
if not isinstance(input_obj, dict):
input_obj = {}
except Exception:
input_obj = {}
contents.append(
ToolUseBlock(
type=tool_call["type"],
id=tool_call["id"],
name=tool_call["name"],
input=input_obj,
),
)
if structured_model:
metadata = input_obj
if contents:
res = ChatResponse(
content=contents,
usage=usage,
metadata=metadata,
)
yield res
def _format_tools_json_schemas(
self,
schemas: list[dict[str, Any]],
) -> list[dict[str, Any]]:
"""Format the JSON schemas of the tool functions to the format that
Anthropic API expects."""
formatted_schemas = []
for schema in schemas:
assert (
"function" in schema
), f"Invalid schema: {schema}, expect key 'function'."
assert "name" in schema["function"], (
f"Invalid schema: {schema}, "
"expect key 'name' in 'function' field."
)
formatted_schemas.append(
{
"name": schema["function"]["name"],
"description": schema["function"].get("description", ""),
"input_schema": schema["function"].get("parameters", {}),
},
)
return formatted_schemas
def _format_tool_choice(
self,
tool_choice: Literal["auto", "none", "any", "required"] | str | None,
) -> dict | None:
"""Format tool_choice parameter for API compatibility.
Args:
tool_choice (`Literal["auto", "none", "any", "required"] | str \
| None`, default `None`):
Controls which (if any) tool is called by the model.
Can be "auto", "none", "any", "required", or specific tool
name. For more details, please refer to
https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/implement-tool-use
Returns:
`dict | None`:
The formatted tool choice configuration dict, or None if
tool_choice is None.
"""
if tool_choice is None:
return None
type_mapping = {
"auto": {"type": "auto"},
"none": {"type": "none"},
"any": {"type": "any"},
"required": {"type": "any"},
}
if tool_choice in type_mapping:
return type_mapping[tool_choice]
return {"type": "tool", "name": tool_choice}
---- _dashscope_model.py ----
# -*- coding: utf-8 -*-
"""The dashscope API model classes."""
import collections
from datetime import datetime
from http import HTTPStatus
from typing import (
Any,
AsyncGenerator,
Generator,
Union,
TYPE_CHECKING,
List,
Literal,
Type,
)
from pydantic import BaseModel
from aioitertools import iter as giter
from ._model_base import ChatModelBase
from ._model_response import ChatResponse
from ._model_usage import ChatUsage
from .._utils._common import (
_json_loads_with_repair,
_create_tool_from_base_model,
)
from ..message import TextBlock, ToolUseBlock, ThinkingBlock
from ..tracing import trace_llm
from ..types import JSONSerializableObject
from .._logging import logger
if TYPE_CHECKING:
from dashscope.api_entities.dashscope_response import GenerationResponse
from dashscope.api_entities.dashscope_response import (
MultiModalConversationResponse,
)
else:
GenerationResponse = (
"dashscope.api_entities.dashscope_response.GenerationResponse"
)
MultiModalConversationResponse = (
"dashscope.api_entities.dashscope_response."
"MultiModalConversationResponse"
)
class DashScopeChatModel(ChatModelBase):
"""The DashScope chat model class, which unifies the Generation and
MultimodalConversation APIs into one method."""
def __init__(
self,
model_name: str,
api_key: str,
stream: bool = True,
enable_thinking: bool | None = None,
generate_kwargs: dict[str, JSONSerializableObject] | None = None,
base_http_api_url: str | None = None,
) -> None:
"""Initialize the DashScope chat model.
Args:
model_name (`str`):
The model names.
api_key (`str`):
The dashscope API key.
stream (`bool`):
The streaming output or not
enable_thinking (`bool | None`, optional):
Enable thinking or not, only support Qwen3, QwQ, DeepSeek-R1.
Refer to `DashScope documentation
<https://help.aliyun.com/zh/model-studio/deep-thinking>`_
for more details.
generate_kwargs (`dict[str, JSONSerializableObject] | None`, \
optional):
The extra keyword arguments used in DashScope API generation,
e.g. `temperature`, `seed`.
base_http_api_url (`str | None`, optional):
The base URL for DashScope API requests. If not provided,
the default base URL from the DashScope SDK will be used.
"""
if enable_thinking and not stream:
logger.info(
"In DashScope API, `stream` must be True when "
"`enable_thinking` is True. ",
)
stream = True
super().__init__(model_name, stream)
self.api_key = api_key
self.enable_thinking = enable_thinking
self.generate_kwargs = generate_kwargs or {}
if base_http_api_url is not None:
import dashscope
dashscope.base_http_api_url = base_http_api_url
@trace_llm
async def __call__(
self,
messages: list[dict[str, Any]],
tools: list[dict] | None = None,
tool_choice: Literal["auto", "none", "any", "required"]
| str
| None = None,
structured_model: Type[BaseModel] | None = None,
**kwargs: Any,
) -> ChatResponse | AsyncGenerator[ChatResponse, None]:
"""Get the response from the dashscope
Generation/MultimodalConversation API by the given arguments.
.. note:: We unify the dashscope generation and multimodal conversation
APIs into one method, since they support similar arguments and share
the same functionality.
Args:
messages (`list[dict[str, Any]]`):
A list of dictionaries, where `role` and `content` fields are
required.
tools (`list[dict] | None`, default `None`):
The tools JSON schemas that the model can use.
tool_choice (`Literal["auto", "none", "any", "required"] | str \
| None`, default `None`):
Controls which (if any) tool is called by the model.
Can be "auto", "none", or specific tool name.
For more details, please refer to
https://help.aliyun.com/zh/model-studio/qwen-function-calling
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output. When provided, the model will be forced
to return data that conforms to this schema by automatically
converting the BaseModel to a tool function and setting
`tool_choice` to enforce its usage. This enables structured
output generation.
.. note:: When `structured_model` is specified,
both `tools` and `tool_choice` parameters are ignored,
and the model will only perform structured output
generation without calling any other tools.
**kwargs (`Any`):
The keyword arguments for DashScope chat completions API,
e.g. `temperature`, `max_tokens`, `top_p`, etc. Please
refer to `DashScope documentation
<https://help.aliyun.com/zh/dashscope/developer-reference/api-details>`_
for more detailed arguments.
"""
import dashscope
# For qvq and qwen-vl models, the content field cannot be `None` or
# `[{"text": None}]`, so we need to convert it to an empty list.
if self.model_name.startswith("qvq") or "-vl" in self.model_name:
for msg in messages:
if msg["content"] is None or msg["content"] == [
{"text": None},
]:
msg["content"] = []
kwargs = {
"messages": messages,
"model": self.model_name,
"stream": self.stream,
**self.generate_kwargs,
**kwargs,
"result_format": "message",
# In agentscope, the `incremental_output` must be `True` when
# `self.stream` is True
"incremental_output": self.stream,
}
if tools:
kwargs["tools"] = self._format_tools_json_schemas(tools)
if tool_choice:
self._validate_tool_choice(tool_choice, tools)
kwargs["tool_choice"] = self._format_tool_choice(tool_choice)
if (
self.enable_thinking is not None
and "enable_thinking" not in kwargs
):
kwargs["enable_thinking"] = self.enable_thinking
if structured_model:
if tools or tool_choice:
logger.warning(
"structured_model is provided. Both 'tools' and "
"'tool_choice' parameters will be overridden and "
"ignored. The model will only perform structured output "
"generation without calling any other tools.",
)
format_tool = _create_tool_from_base_model(structured_model)
kwargs["tools"] = self._format_tools_json_schemas(
[format_tool],
)
kwargs["tool_choice"] = self._format_tool_choice(
format_tool["function"]["name"],
)
start_datetime = datetime.now()
if self.model_name.startswith("qvq") or "-vl" in self.model_name:
response = dashscope.MultiModalConversation.call(
api_key=self.api_key,
**kwargs,
)
else:
response = await dashscope.aigc.generation.AioGeneration.call(
api_key=self.api_key,
**kwargs,
)
if self.stream:
return self._parse_dashscope_stream_response(
start_datetime,
response,
structured_model,
)
parsed_response = await self._parse_dashscope_generation_response(
start_datetime,
response,
structured_model,
)
return parsed_response
# pylint: disable=too-many-branches
async def _parse_dashscope_stream_response(
self,
start_datetime: datetime,
response: Union[
AsyncGenerator[GenerationResponse, None],
Generator[MultiModalConversationResponse, None, None],
],
structured_model: Type[BaseModel] | None = None,
) -> AsyncGenerator[ChatResponse, Any]:
"""Given a DashScope streaming response generator, extract the content
blocks and usages from it and yield ChatResponse objects.
Args:
start_datetime (`datetime`):
The start datetime of the response generation.
response (
`Union[AsyncGenerator[GenerationResponse, None], Generator[ \
MultiModalConversationResponse, None, None]]`
):
DashScope streaming response generator (GenerationResponse or
MultiModalConversationResponse) to parse.
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
Returns:
AsyncGenerator[ChatResponse, Any]:
An async generator that yields ChatResponse objects containing
the content blocks and usage information for each chunk in the
streaming response.
.. note::
If `structured_model` is not `None`, the expected structured output
will be stored in the metadata of the `ChatResponse`.
"""
acc_content, acc_thinking_content = "", ""
acc_tool_calls = collections.defaultdict(dict)
metadata = None
async for chunk in giter(response):
if chunk.status_code != HTTPStatus.OK:
raise RuntimeError(
f"Failed to get response from _ API: {chunk}",
)
message = chunk.output.choices[0].message
# Update reasoning content
if isinstance(message.get("reasoning_content"), str):
acc_thinking_content += message["reasoning_content"]
# Update text content
if isinstance(message.content, str):
acc_content += message.content
elif isinstance(message.content, list):
for item in message.content:
if isinstance(item, dict) and "text" in item:
acc_content += item["text"]
# Update tool calls
for tool_call in message.get("tool_calls", []):
index = tool_call.get("index", 0)
if "id" in tool_call and tool_call["id"] != acc_tool_calls[
index
].get("id"):
acc_tool_calls[index]["id"] = (
acc_tool_calls[index].get("id", "") + tool_call["id"]
)
if "function" in tool_call:
func = tool_call["function"]
if "name" in func:
acc_tool_calls[index]["name"] = (
acc_tool_calls[index].get("name", "")
+ func["name"]
)
if "arguments" in func:
acc_tool_calls[index]["arguments"] = (
acc_tool_calls[index].get("arguments", "")
+ func["arguments"]
)
# to content blocks
content_blocks: list[TextBlock | ToolUseBlock | ThinkingBlock] = []
if acc_thinking_content:
content_blocks.append(
ThinkingBlock(
type="thinking",
thinking=acc_thinking_content,
),
)
if acc_content:
content_blocks.append(
TextBlock(
type="text",
text=acc_content,
),
)
for tool_call in acc_tool_calls.values():
repaired_input = _json_loads_with_repair(
tool_call.get("arguments", "{}") or "{}",
)
if not isinstance(repaired_input, dict):
repaired_input = {}
content_blocks.append(
ToolUseBlock(
type="tool_use",
id=tool_call.get("id", ""),
name=tool_call.get("name", ""),
input=repaired_input,
),
)
if structured_model:
metadata = repaired_input
usage = None
if chunk.usage:
usage = ChatUsage(
input_tokens=chunk.usage.input_tokens,
output_tokens=chunk.usage.output_tokens,
time=(datetime.now() - start_datetime).total_seconds(),
)
parsed_chunk = ChatResponse(
content=content_blocks,
usage=usage,
metadata=metadata,
)
yield parsed_chunk
async def _parse_dashscope_generation_response(
self,
start_datetime: datetime,
response: Union[
GenerationResponse,
MultiModalConversationResponse,
],
structured_model: Type[BaseModel] | None = None,
) -> ChatResponse:
"""Given a DashScope GenerationResponse object, extract the content
blocks and usages from it.
Args:
start_datetime (`datetime`):
The start datetime of the response generation.
response (
`Union[GenerationResponse, MultiModalConversationResponse]`
):
Dashscope GenerationResponse | MultiModalConversationResponse
object to parse.
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
Returns:
ChatResponse (`ChatResponse`):
A ChatResponse object containing the content blocks and usage.
.. note::
If `structured_model` is not `None`, the expected structured output
will be stored in the metadata of the `ChatResponse`.
"""
# Collect the content blocks from the response.
if response.status_code != 200:
raise RuntimeError(response)
content_blocks: List[TextBlock | ToolUseBlock] = []
metadata: dict | None = None
message = response.output.choices[0].message
content = message.get("content")
if response.output.choices[0].message.get("content") not in [
None,
"",
[],
]:
if isinstance(content, list):
for item in content:
if isinstance(item, dict) and "text" in item:
content_blocks.append(
TextBlock(
type="text",
text=item["text"],
),
)
else:
content_blocks.append(
TextBlock(
type="text",
text=content,
),
)
if message.get("tool_calls"):
for tool_call in message["tool_calls"]:
input_ = _json_loads_with_repair(
tool_call["function"].get(
"arguments",
"{}",
)
or "{}",
)
content_blocks.append(
ToolUseBlock(
type="tool_use",
name=tool_call["function"]["name"],
input=input_,
id=tool_call["id"],
),
)
if structured_model:
metadata = input_
# Usage information
usage = None
if response.usage:
usage = ChatUsage(
input_tokens=response.usage.input_tokens,
output_tokens=response.usage.output_tokens,
time=(datetime.now() - start_datetime).total_seconds(),
)
parsed_response = ChatResponse(
content=content_blocks,
usage=usage,
metadata=metadata,
)
return parsed_response
def _format_tools_json_schemas(
self,
schemas: list[dict[str, Any]],
) -> list[dict[str, Any]]:
"""Format the tools JSON schema into required format for DashScope API.
Args:
schemas (`dict[str, dict[str, Any]]`):
The tools JSON schemas.
"""
# Check schemas format
for value in schemas:
if (
not isinstance(value, dict)
or "type" not in value
or value["type"] != "function"
or "function" not in value
):
raise ValueError(
f"Each schema must be a dict with 'type' as 'function' "
f"and 'function' key, got {value}",
)
return schemas
def _format_tool_choice(
self,
tool_choice: Literal["auto", "none", "any", "required"] | str | None,
) -> str | dict | None:
"""Format tool_choice parameter for API compatibility.
Args:
tool_choice (`Literal["auto", "none", "any", "required"] | str \
| None`, default `None`):
Controls which (if any) tool is called by the model.
Can be "auto", "none", or specific tool name.
For more details, please refer to
https://help.aliyun.com/zh/model-studio/qwen-function-calling
Returns:
`dict | None`:
The formatted tool choice configuration dict, or None if
tool_choice is None.
"""
if tool_choice is None:
return None
if tool_choice in ["auto", "none"]:
return tool_choice
if tool_choice in ["any", "required"]:
logger.warning(
"tool_choice '%s' is not supported by DashScope API. "
"Supported options are 'auto', 'none', or specific function "
"name. Automatically using 'auto' instead.",
tool_choice,
)
return "auto"
return {"type": "function", "function": {"name": tool_choice}}
---- _gemini_model.py ----
# -*- coding: utf-8 -*-
# mypy: disable-error-code="dict-item"
"""The Google Gemini model in agentscope."""
from datetime import datetime
from typing import (
AsyncGenerator,
Any,
TYPE_CHECKING,
AsyncIterator,
Literal,
Type,
List,
)
from pydantic import BaseModel
from .._logging import logger
from .._utils._common import _json_loads_with_repair
from ..message import ToolUseBlock, TextBlock, ThinkingBlock
from ._model_usage import ChatUsage
from ._model_base import ChatModelBase
from ._model_response import ChatResponse
from ..tracing import trace_llm
from ..types import JSONSerializableObject
if TYPE_CHECKING:
from google.genai.types import GenerateContentResponse
else:
GenerateContentResponse = "google.genai.types.GenerateContentResponse"
class GeminiChatModel(ChatModelBase):
"""The Google Gemini chat model class in agentscope."""
def __init__(
self,
model_name: str,
api_key: str,
stream: bool = True,
thinking_config: dict | None = None,
client_args: dict = None,
generate_kwargs: dict[str, JSONSerializableObject] | None = None,
) -> None:
"""Initialize the Gemini chat model.
Args:
model_name (`str`):
The name of the Gemini model to use, e.g. "gemini-2.5-flash".
api_key (`str`):
The API key for Google Gemini.
stream (`bool`, default `True`):
Whether to use streaming output or not.
thinking_config (`dict | None`, optional):
Thinking config, supported models are 2.5 Pro, 2.5 Flash, etc.
Refer to https://ai.google.dev/gemini-api/docs/thinking for
more details.
.. code-block:: python
:caption: Example of thinking_config
{
"include_thoughts": True, # enable thoughts or not
"thinking_budget": 1024 # Max tokens for reasoning
}
client_args (`dict`, default `None`):
The extra keyword arguments to initialize the OpenAI client.
generate_kwargs (`dict[str, JSONSerializableObject] | None`, \
optional):
The extra keyword arguments used in Gemini API generation,
e.g. `temperature`, `seed`.
"""
try:
from google import genai
except ImportError as e:
raise ImportError(
"Please install gemini Python sdk with "
"`pip install -q -U google-genai`",
) from e
super().__init__(model_name, stream)
self.client = genai.Client(
api_key=api_key,
**(client_args or {}),
)
self.thinking_config = thinking_config
self.generate_kwargs = generate_kwargs or {}
@trace_llm
async def __call__(
self,
messages: list[dict],
tools: list[dict] | None = None,
tool_choice: Literal["auto", "none", "any", "required"]
| str
| None = None,
structured_model: Type[BaseModel] | None = None,
**config_kwargs: Any,
) -> ChatResponse | AsyncGenerator[ChatResponse, None]:
"""Call the Gemini model with the provided arguments.
Args:
messages (`list[dict[str, Any]]`):
A list of dictionaries, where `role` and `content` fields are
required.
tools (`list[dict] | None`, default `None`):
The tools JSON schemas that the model can use.
tool_choice (`Literal["auto", "none", "any", "required"] | str \
| None`, default `None`):
Controls which (if any) tool is called by the model.
Can be "auto", "none", "any", "required", or specific tool
name. For more details, please refer to
https://ai.google.dev/gemini-api/docs/function-calling?hl=en&example=meeting#function_calling_modes
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
.. note:: When `structured_model` is specified,
both `tools` and `tool_choice` parameters are ignored,
and the model will only perform structured output
generation without calling any other tools.
For more details, please refer to
https://ai.google.dev/gemini-api/docs/structured-output
**config_kwargs (`Any`):
The keyword arguments for Gemini chat completions API.
"""
config: dict = {
"thinking_config": self.thinking_config,
**self.generate_kwargs,
**config_kwargs,
}
if tools:
config["tools"] = self._format_tools_json_schemas(tools)
if tool_choice:
self._validate_tool_choice(tool_choice, tools)
config["tool_config"] = self._format_tool_choice(tool_choice)
if structured_model:
if tools or tool_choice:
logger.warning(
"structured_model is provided. Both 'tools' and "
"'tool_choice' parameters will be overridden and "
"ignored. The model will only perform structured output "
"generation without calling any other tools.",
)
config.pop("tools", None)
config.pop("tool_config", None)
config["response_mime_type"] = "application/json"
config["response_schema"] = structured_model
# Prepare the arguments for the Gemini API call
kwargs: dict[str, JSONSerializableObject] = {
"model": self.model_name,
"contents": messages,
"config": config,
}
start_datetime = datetime.now()
if self.stream:
response = await self.client.aio.models.generate_content_stream(
**kwargs,
)
return self._parse_gemini_stream_generation_response(
start_datetime,
response,
structured_model,
)
# non-streaming
response = await self.client.aio.models.generate_content(
**kwargs,
)
parsed_response = self._parse_gemini_generation_response(
start_datetime,
response,
structured_model,
)
return parsed_response
async def _parse_gemini_stream_generation_response(
self,
start_datetime: datetime,
response: AsyncIterator[GenerateContentResponse],
structured_model: Type[BaseModel] | None = None,
) -> AsyncGenerator[ChatResponse, None]:
"""Given a Gemini streaming generation response, extract the
content blocks and usages from it and yield ChatResponse objects.
Args:
start_datetime (`datetime`):
The start datetime of the response generation.
response (`AsyncIterator[GenerateContentResponse]`):
Gemini GenerateContentResponse async iterator to parse.
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
Returns:
`AsyncGenerator[ChatResponse, None]`:
An async generator that yields ChatResponse objects containing
the content blocks and usage information for each chunk in the
streaming response.
.. note::
If `structured_model` is not `None`, the expected structured output
will be stored in the metadata of the `ChatResponse`.
"""
text = ""
thinking = ""
metadata: dict | None = None
async for chunk in response:
content_block: list = []
# Thinking parts
if (
chunk.candidates
and chunk.candidates[0].content
and chunk.candidates[0].content.parts
):
for part in chunk.candidates[0].content.parts:
if part.thought and part.text:
thinking += part.text
# Text parts
if chunk.text:
text += chunk.text
if structured_model:
metadata = _json_loads_with_repair(text)
# Function calls
tool_calls = []
if chunk.function_calls:
for function_call in chunk.function_calls:
tool_calls.append(
ToolUseBlock(
type="tool_use",
id=function_call.id,
name=function_call.name,
input=function_call.args or {},
),
)
usage = None
if chunk.usage_metadata:
usage = ChatUsage(
input_tokens=chunk.usage_metadata.prompt_token_count,
output_tokens=chunk.usage_metadata.total_token_count
- chunk.usage_metadata.prompt_token_count,
time=(datetime.now() - start_datetime).total_seconds(),
)
if thinking:
content_block.append(
ThinkingBlock(
type="thinking",
thinking=thinking,
),
)
if text:
content_block.append(
TextBlock(
type="text",
text=text,
),
)
content_block.extend(
[
*tool_calls,
],
)
parsed_chunk = ChatResponse(
content=content_block,
usage=usage,
metadata=metadata,
)
yield parsed_chunk
def _parse_gemini_generation_response(
self,
start_datetime: datetime,
response: GenerateContentResponse,
structured_model: Type[BaseModel] | None = None,
) -> ChatResponse:
"""Given a Gemini chat completion response object, extract the content
blocks and usages from it.
Args:
start_datetime (`datetime`):
The start datetime of the response generation.
response (`ChatCompletion`):
The OpenAI chat completion response object to parse.
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
Returns:
ChatResponse (`ChatResponse`):
A ChatResponse object containing the content blocks and usage.
.. note::
If `structured_model` is not `None`, the expected structured output
will be stored in the metadata of the `ChatResponse`.
"""
content_blocks: List[TextBlock | ToolUseBlock | ThinkingBlock] = []
metadata: dict | None = None
if (
response.candidates
and response.candidates[0].content
and response.candidates[0].content.parts
):
for part in response.candidates[0].content.parts:
if part.thought and part.text:
content_blocks.append(
ThinkingBlock(
type="thinking",
thinking=part.text,
),
)
if response.text:
content_blocks.append(
TextBlock(
type="text",
text=response.text,
),
)
if structured_model:
metadata = _json_loads_with_repair(response.text)
if response.function_calls:
for tool_call in response.function_calls:
content_blocks.append(
ToolUseBlock(
type="tool_use",
id=tool_call.id,
name=tool_call.name,
input=tool_call.args or {},
),
)
if response.usage_metadata:
usage = ChatUsage(
input_tokens=response.usage_metadata.prompt_token_count,
output_tokens=response.usage_metadata.total_token_count
- response.usage_metadata.prompt_token_count,
time=(datetime.now() - start_datetime).total_seconds(),
)
else:
usage = None
return ChatResponse(
content=content_blocks,
usage=usage,
metadata=metadata,
)
def _format_tools_json_schemas(
self,
schemas: list[dict[str, Any]],
) -> list[dict[str, Any]]:
"""Format the tools JSON schema into required format for Gemini API.
Args:
schemas (`dict[str, Any]`):
The tools JSON schemas.
Returns:
List[Dict[str, Any]]:
A list containing a dictionary with the
"function_declarations" key, which maps to a list of
function definitions.
Example:
.. code-block:: python
:caption: Example tool schemas of Gemini API
# Input JSON schema
schemas = [
{
'type': 'function',
'function': {
'name': 'execute_shell_command',
'description': 'xxx',
'parameters': {
'type': 'object',
'properties': {
'command': {
'type': 'string',
'description': 'xxx.'
},
'timeout': {
'type': 'integer',
'default': 300
}
},
'required': ['command']
}
}
}
]
# Output format (Gemini API expected):
[
{
'function_declarations': [
{
'name': 'execute_shell_command',
'description': 'xxx.',
'parameters': {
'type': 'object',
'properties': {
'command': {
'type': 'string',
'description': 'xxx.'
},
'timeout': {
'type': 'integer',
'default': 300
}
},
'required': ['command']
}
}
]
}
]
"""
return [
{
"function_declarations": [
_["function"] for _ in schemas if "function" in _
],
},
]
def _format_tool_choice(
self,
tool_choice: Literal["auto", "none", "any", "required"] | str | None,
) -> dict | None:
"""Format tool_choice parameter for API compatibility.
Args:
tool_choice (`Literal["auto", "none"] | str | None`, default \
`None`):
Controls which (if any) tool is called by the model.
Can be "auto", "none", "any", "required", or specific tool
name.
For more details, please refer to
https://ai.google.dev/gemini-api/docs/function-calling?hl=en&example=meeting#function_calling_modes
Returns:
`dict | None`:
The formatted tool choice configuration dict, or None if
tool_choice is None.
"""
if tool_choice is None:
return None
mode_mapping = {
"auto": "AUTO",
"none": "NONE",
"any": "ANY",
"required": "ANY",
}
mode = mode_mapping.get(tool_choice)
if mode:
return {"function_calling_config": {"mode": mode}}
return {
"function_calling_config": {
"mode": "ANY",
"allowed_function_names": [tool_choice],
},
}
---- _model_base.py ----
# -*- coding: utf-8 -*-
"""The chat model base class."""
from abc import abstractmethod
from typing import AsyncGenerator, Any
from ._model_response import ChatResponse
TOOL_CHOICE_MODES = ["auto", "none", "any", "required"]
class ChatModelBase:
"""Base class for chat models."""
model_name: str
"""The model name"""
stream: bool
"""Is the model output streaming or not"""
def __init__(
self,
model_name: str,
stream: bool,
) -> None:
"""Initialize the chat model base class.
Args:
model_name (`str`):
The name of the model
stream (`bool`):
Whether the model output is streaming or not
"""
self.model_name = model_name
self.stream = stream
@abstractmethod
async def __call__(
self,
*args: Any,
**kwargs: Any,
) -> ChatResponse | AsyncGenerator[ChatResponse, None]:
pass
def _validate_tool_choice(
self,
tool_choice: str,
tools: list[dict] | None,
) -> None:
"""
Validate tool_choice parameter.
Args:
tool_choice (`str`):
Tool choice mode or function name
tools (`list[dict] | None`):
Available tools list
Raises:
TypeError: If tool_choice is not string
ValueError: If tool_choice is invalid
"""
if not isinstance(tool_choice, str):
raise TypeError(
f"tool_choice must be str, got {type(tool_choice)}",
)
if tool_choice in TOOL_CHOICE_MODES:
return
available_functions = [tool["function"]["name"] for tool in tools]
if tool_choice not in available_functions:
all_options = TOOL_CHOICE_MODES + available_functions
raise ValueError(
f"Invalid tool_choice '{tool_choice}'. "
f"Available options: {', '.join(sorted(all_options))}",
)
---- _model_response.py ----
# -*- coding: utf-8 -*-
"""The model response module."""
from dataclasses import dataclass, field
from typing import Literal, Sequence
from ._model_usage import ChatUsage
from .._utils._common import _get_timestamp
from .._utils._mixin import DictMixin
from ..message import (
TextBlock,
ToolUseBlock,
ThinkingBlock,
AudioBlock,
)
from ..types import JSONSerializableObject
@dataclass
class ChatResponse(DictMixin):
"""The response of chat models."""
content: Sequence[TextBlock | ToolUseBlock | ThinkingBlock | AudioBlock]
"""The content of the chat response, which can include text blocks,
tool use blocks, or thinking blocks."""
id: str = field(default_factory=lambda: _get_timestamp(True))
"""The unique identifier formatter """
created_at: str = field(default_factory=_get_timestamp)
"""When the response was created"""
type: Literal["chat"] = field(default_factory=lambda: "chat")
"""The type of the response, which is always 'chat'."""
usage: ChatUsage | None = field(default_factory=lambda: None)
"""The usage information of the chat response, if available."""
metadata: dict[str, JSONSerializableObject] | None = field(
default_factory=lambda: None,
)
"""The metadata of the chat response"""
---- _model_usage.py ----
# -*- coding: utf-8 -*-
"""The model usage class in agentscope."""
from dataclasses import dataclass, field
from typing import Literal
from .._utils._mixin import DictMixin
@dataclass
class ChatUsage(DictMixin):
"""The usage of a chat model API invocation."""
input_tokens: int
"""The number of input tokens."""
output_tokens: int
"""The number of output tokens."""
time: float
"""The time used in seconds."""
type: Literal["chat"] = field(default_factory=lambda: "chat")
"""The type of the usage, must be `chat`."""
---- _ollama_model.py ----
# -*- coding: utf-8 -*-
"""Model wrapper for Ollama models."""
from datetime import datetime
from typing import (
Any,
TYPE_CHECKING,
List,
AsyncGenerator,
AsyncIterator,
Literal,
Type,
)
from collections import OrderedDict
from pydantic import BaseModel
from . import ChatResponse
from ._model_base import ChatModelBase
from ._model_usage import ChatUsage
from .._logging import logger
from .._utils._common import _json_loads_with_repair
from ..message import ToolUseBlock, TextBlock, ThinkingBlock
from ..tracing import trace_llm
if TYPE_CHECKING:
from ollama._types import ChatResponse as OllamaChatResponse
else:
OllamaChatResponse = "ollama._types.ChatResponse"
class OllamaChatModel(ChatModelBase):
"""The Ollama chat model class in agentscope."""
def __init__(
self,
model_name: str,
stream: bool = False,
options: dict = None,
keep_alive: str = "5m",
enable_thinking: bool | None = None,
host: str | None = None,
**kwargs: Any,
) -> None:
"""Initialize the Ollama chat model.
Args:
model_name (`str`):
The name of the model.
stream (`bool`, default `True`):
Streaming mode or not.
options (`dict`, default `None`):
Additional parameters to pass to the Ollama API. These can
include temperature etc.
keep_alive (`str`, default `"5m"`):
Duration to keep the model loaded in memory. The format is a
number followed by a unit suffix (s for seconds, m for minutes
, h for hours).
enable_thinking (`bool | None`, default `None`)
Whether enable thinking or not, only for models such as qwen3,
deepseek-r1, etc. For more details, please refer to
https://ollama.com/search?c=thinking
host (`str | None`, default `None`):
The host address of the Ollama server. If None, uses the
default address (typically http://localhost:11434).
**kwargs (`Any`):
Additional keyword arguments to pass to the base chat model
class.
"""
try:
import ollama
except ImportError as e:
raise ImportError(
"The package ollama is not found. Please install it by "
'running command `pip install "ollama>=0.1.7"`',
) from e
super().__init__(model_name, stream)
self.client = ollama.AsyncClient(
host=host,
**kwargs,
)
self.options = options
self.keep_alive = keep_alive
self.think = enable_thinking
@trace_llm
async def __call__(
self,
messages: list[dict[str, Any]],
tools: list[dict] | None = None,
tool_choice: Literal["auto", "none", "any", "required"]
| str
| None = None,
structured_model: Type[BaseModel] | None = None,
**kwargs: Any,
) -> ChatResponse | AsyncGenerator[ChatResponse, None]:
"""Get the response from Ollama chat completions API by the given
arguments.
Args:
messages (`list[dict]`):
A list of dictionaries, where `role` and `content` fields are
required, and `name` field is optional.
tools (`list[dict]`, default `None`):
The tools JSON schemas that the model can use.
tool_choice (`Literal["auto", "none", "any", "required"] | str \
| None`, default `None`):
Controls which (if any) tool is called by the model.
Can be "auto", "none", "any", "required", or specific tool
name.
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
**kwargs (`Any`):
The keyword arguments for Ollama chat completions API,
e.g. `think`etc. Please refer to the Ollama API
documentation for more details.
Returns:
`ChatResponse | AsyncGenerator[ChatResponse, None]`:
The response from the Ollama chat completions API.
"""
kwargs = {
"model": self.model_name,
"messages": messages,
"stream": self.stream,
"options": self.options,
"keep_alive": self.keep_alive,
**kwargs,
}
if self.think is not None and "think" not in kwargs:
kwargs["think"] = self.think
if tools:
kwargs["tools"] = self._format_tools_json_schemas(tools)
if tool_choice:
logger.warning("Ollama does not support tool_choice yet, ignored.")
if structured_model:
kwargs["format"] = structured_model.model_json_schema()
start_datetime = datetime.now()
response = await self.client.chat(**kwargs)
if self.stream:
return self._parse_ollama_stream_completion_response(
start_datetime,
response,
structured_model,
)
parsed_response = await self._parse_ollama_completion_response(
start_datetime,
response,
structured_model,
)
return parsed_response
async def _parse_ollama_stream_completion_response(
self,
start_datetime: datetime,
response: AsyncIterator[OllamaChatResponse],
structured_model: Type[BaseModel] | None = None,
) -> AsyncGenerator[ChatResponse, None]:
"""Given an Ollama streaming completion response, extract the
content blocks and usages from it and yield ChatResponse objects.
Args:
start_datetime (`datetime`):
The start datetime of the response generation.
response (`AsyncIterator[OllamaChatResponse]`):
Ollama streaming response async iterator to parse.
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
Returns:
AsyncGenerator[ChatResponse, None]:
An async generator that yields ChatResponse objects containing
the content blocks and usage information for each chunk in the
streaming response.
.. note::
If `structured_model` is not `None`, the expected structured output
will be stored in the metadata of the `ChatResponse`.
"""
accumulated_text = ""
acc_thinking_content = ""
tool_calls = OrderedDict() # Store tool calls
metadata: dict | None = None
async for chunk in response:
# Handle text content
msg = chunk.message
acc_thinking_content += msg.thinking or ""
accumulated_text += msg.content or ""
# Handle tool calls
for idx, tool_call in enumerate(msg.tool_calls or []):
function = tool_call.function
tool_id = f"{idx}_{function.name}"
tool_calls[tool_id] = {
"type": "tool_use",
"id": tool_id,
"name": function.name,
"input": function.arguments,
}
# Calculate usage statistics
current_time = (datetime.now() - start_datetime).total_seconds()
usage = ChatUsage(
input_tokens=getattr(chunk, "prompt_eval_count", 0) or 0,
output_tokens=getattr(chunk, "eval_count", 0) or 0,
time=current_time,
)
# Create content blocks
contents: list = []
if acc_thinking_content:
contents.append(
ThinkingBlock(
type="thinking",
thinking=acc_thinking_content,
),
)
if accumulated_text:
contents.append(TextBlock(type="text", text=accumulated_text))
if structured_model:
metadata = _json_loads_with_repair(accumulated_text)
# Add tool call blocks
for tool_call in tool_calls.values():
try:
input_data = tool_call["input"]
if isinstance(input_data, str):
input_data = _json_loads_with_repair(input_data)
contents.append(
ToolUseBlock(
type=tool_call["type"],
id=tool_call["id"],
name=tool_call["name"],
input=input_data,
),
)
except Exception as e:
print(f"Error parsing tool call input: {e}")
# Generate response when there's new content or at final chunk
if chunk.done and contents:
res = ChatResponse(
content=contents,
usage=usage,
metadata=metadata,
)
yield res
async def _parse_ollama_completion_response(
self,
start_datetime: datetime,
response: OllamaChatResponse,
structured_model: Type[BaseModel] | None = None,
) -> ChatResponse:
"""Given an Ollama chat completion response object, extract the content
blocks and usages from it.
Args:
start_datetime (`datetime`):
The start datetime of the response generation.
response (`OllamaChatResponse`):
Ollama OllamaChatResponse object to parse.
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
Returns:
`ChatResponse`:
A ChatResponse object containing the content blocks and usage.
.. note::
If `structured_model` is not `None`, the expected structured output
will be stored in the metadata of the `ChatResponse`.
"""
content_blocks: List[TextBlock | ToolUseBlock | ThinkingBlock] = []
metadata: dict | None = None
if response.message.thinking:
content_blocks.append(
ThinkingBlock(
type="thinking",
thinking=response.message.thinking,
),
)
if response.message.content:
content_blocks.append(
TextBlock(
type="text",
text=response.message.content,
),
)
if structured_model:
metadata = _json_loads_with_repair(
response.message.content,
)
for idx, tool_call in enumerate(response.message.tool_calls or []):
content_blocks.append(
ToolUseBlock(
type="tool_use",
id=f"{idx}_{tool_call.function.name}",
name=tool_call.function.name,
input=tool_call.function.arguments,
),
)
usage = None
if "prompt_eval_count" in response and "eval_count" in response:
usage = ChatUsage(
input_tokens=response.get("prompt_eval_count", 0),
output_tokens=response.get("eval_count", 0),
time=(datetime.now() - start_datetime).total_seconds(),
)
parsed_response = ChatResponse(
content=content_blocks,
usage=usage,
metadata=metadata,
)
return parsed_response
def _format_tools_json_schemas(
self,
schemas: list[dict[str, Any]],
) -> list[dict[str, Any]]:
"""Format the tools JSON schemas to the Ollama format."""
return schemas
---- _openai_model.py ----
# -*- coding: utf-8 -*-
# pylint: disable=too-many-branches
"""OpenAI Chat model class."""
from datetime import datetime
from typing import (
Any,
TYPE_CHECKING,
List,
AsyncGenerator,
Literal,
Type,
)
from collections import OrderedDict
from pydantic import BaseModel
from . import ChatResponse
from ._model_base import ChatModelBase
from ._model_usage import ChatUsage
from .._logging import logger
from .._utils._common import _json_loads_with_repair
from ..message import (
ToolUseBlock,
TextBlock,
ThinkingBlock,
AudioBlock,
Base64Source,
)
from ..tracing import trace_llm
from ..types import JSONSerializableObject
if TYPE_CHECKING:
from openai.types.chat import ChatCompletion
from openai import AsyncStream
else:
ChatCompletion = "openai.types.chat.ChatCompletion"
AsyncStream = "openai.types.chat.AsyncStream"
def _format_audio_data_for_qwen_omni(messages: list[dict]) -> None:
"""Qwen-omni uses OpenAI-compatible API but requires different audio
data format than OpenAI with "data:;base64," prefix.
Refer to `Qwen-omni documentation
<https://bailian.console.aliyun.com/?tab=doc#/doc/?type=model&url=2867839>`_
for more details.
Args:
messages (`list[dict]`):
The list of message dictionaries from OpenAI formatter.
"""
for msg in messages:
if isinstance(msg.get("content"), list):
for block in msg["content"]:
if (
isinstance(block, dict)
and "input_audio" in block
and isinstance(block["input_audio"].get("data"), str)
):
if not block["input_audio"]["data"].startswith("http"):
block["input_audio"]["data"] = (
"data:;base64," + block["input_audio"]["data"]
)
class OpenAIChatModel(ChatModelBase):
"""The OpenAI chat model class."""
def __init__(
self,
model_name: str,
api_key: str | None = None,
stream: bool = True,
reasoning_effort: Literal["low", "medium", "high"] | None = None,
organization: str = None,
client_args: dict = None,
generate_kwargs: dict[str, JSONSerializableObject] | None = None,
) -> None:
"""Initialize the openai client.
Args:
model_name (`str`, default `None`):
The name of the model to use in OpenAI API.
api_key (`str`, default `None`):
The API key for OpenAI API. If not specified, it will
be read from the environment variable `OPENAI_API_KEY`.
stream (`bool`, default `True`):
Whether to use streaming output or not.
reasoning_effort (`Literal["low", "medium", "high"] | None`, \
optional):
Reasoning effort, supported for o3, o4, etc. Please refer to
`OpenAI documentation
<https://platform.openai.com/docs/guides/reasoning?api-mode=chat>`_
for more details.
organization (`str`, default `None`):
The organization ID for OpenAI API. If not specified, it will
be read from the environment variable `OPENAI_ORGANIZATION`.
client_args (`dict`, default `None`):
The extra keyword arguments to initialize the OpenAI client.
generate_kwargs (`dict[str, JSONSerializableObject] | None`, \
optional):
The extra keyword arguments used in OpenAI API generation,
e.g. `temperature`, `seed`.
"""
super().__init__(model_name, stream)
import openai
self.client = openai.AsyncClient(
api_key=api_key,
organization=organization,
**(client_args or {}),
)
self.reasoning_effort = reasoning_effort
self.generate_kwargs = generate_kwargs or {}
@trace_llm
async def __call__(
self,
messages: list[dict],
tools: list[dict] | None = None,
tool_choice: Literal["auto", "none", "any", "required"]
| str
| None = None,
structured_model: Type[BaseModel] | None = None,
**kwargs: Any,
) -> ChatResponse | AsyncGenerator[ChatResponse, None]:
"""Get the response from OpenAI chat completions API by the given
arguments.
Args:
messages (`list[dict]`):
A list of dictionaries, where `role` and `content` fields are
required, and `name` field is optional.
tools (`list[dict]`, default `None`):
The tools JSON schemas that the model can use.
tool_choice (`Literal["auto", "none", "any", "required"] | str \
| None`, default `None`):
Controls which (if any) tool is called by the model.
Can be "auto", "none", "any", "required", or specific tool
name. For more details, please refer to
https://platform.openai.com/docs/api-reference/responses/create#responses_create-tool_choice
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output. When provided, the model will be forced
to return data that conforms to this schema by automatically
converting the BaseModel to a tool function and setting
`tool_choice` to enforce its usage. This enables structured
output generation.
.. note:: When `structured_model` is specified,
both `tools` and `tool_choice` parameters are ignored,
and the model will only perform structured output
generation without calling any other tools.
For more details, please refer to the `official document
<https://platform.openai.com/docs/guides/structured-outputs>`_
**kwargs (`Any`):
The keyword arguments for OpenAI chat completions API,
e.g. `temperature`, `max_tokens`, `top_p`, etc. Please
refer to the OpenAI API documentation for more details.
Returns:
`ChatResponse | AsyncGenerator[ChatResponse, None]`:
The response from the OpenAI chat completions API.
"""
# checking messages
if not isinstance(messages, list):
raise ValueError(
"OpenAI `messages` field expected type `list`, "
f"got `{type(messages)}` instead.",
)
if not all("role" in msg and "content" in msg for msg in messages):
raise ValueError(
"Each message in the 'messages' list must contain a 'role' "
"and 'content' key for OpenAI API.",
)
# Qwen-omni requires different base64 audio format from openai
if "omni" in self.model_name.lower():
_format_audio_data_for_qwen_omni(messages)
kwargs = {
"model": self.model_name,
"messages": messages,
"stream": self.stream,
**self.generate_kwargs,
**kwargs,
}
if self.reasoning_effort and "reasoning_effort" not in kwargs:
kwargs["reasoning_effort"] = self.reasoning_effort
if tools:
kwargs["tools"] = self._format_tools_json_schemas(tools)
if tool_choice:
self._validate_tool_choice(tool_choice, tools)
kwargs["tool_choice"] = self._format_tool_choice(tool_choice)
if self.stream:
kwargs["stream_options"] = {"include_usage": True}
start_datetime = datetime.now()
if structured_model:
if tools or tool_choice:
logger.warning(
"structured_model is provided. Both 'tools' and "
"'tool_choice' parameters will be overridden and "
"ignored. The model will only perform structured output "
"generation without calling any other tools.",
)
kwargs.pop("stream", None)
kwargs.pop("tools", None)
kwargs.pop("tool_choice", None)
kwargs["response_format"] = structured_model
if not self.stream:
response = await self.client.chat.completions.parse(**kwargs)
else:
response = self.client.chat.completions.stream(**kwargs)
return self._parse_openai_stream_response(
start_datetime,
response,
structured_model,
)
else:
response = await self.client.chat.completions.create(**kwargs)
if self.stream:
return self._parse_openai_stream_response(
start_datetime,
response,
structured_model,
)
# Non-streaming response
parsed_response = self._parse_openai_completion_response(
start_datetime,
response,
structured_model,
)
return parsed_response
async def _parse_openai_stream_response(
self,
start_datetime: datetime,
response: AsyncStream,
structured_model: Type[BaseModel] | None = None,
) -> AsyncGenerator[ChatResponse, None]:
"""Given an OpenAI streaming completion response, extract the content
blocks and usages from it and yield ChatResponse objects.
Args:
start_datetime (`datetime`):
The start datetime of the response generation.
response (`AsyncStream`):
OpenAI AsyncStream object to parse.
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
Returns:
`AsyncGenerator[ChatResponse, None]`:
An async generator that yields ChatResponse objects containing
the content blocks and usage information for each chunk in
the streaming response.
.. note::
If `structured_model` is not `None`, the expected structured output
will be stored in the metadata of the `ChatResponse`.
"""
usage, res = None, None
text = ""
thinking = ""
audio = ""
tool_calls = OrderedDict()
metadata: dict | None = None
contents: List[
TextBlock | ToolUseBlock | ThinkingBlock | AudioBlock
] = []
async with response as stream:
async for item in stream:
if structured_model:
if item.type != "chunk":
continue
chunk = item.chunk
else:
chunk = item
if chunk.usage:
usage = ChatUsage(
input_tokens=chunk.usage.prompt_tokens,
output_tokens=chunk.usage.completion_tokens,
time=(datetime.now() - start_datetime).total_seconds(),
)
if not chunk.choices:
if usage and contents:
res = ChatResponse(
content=contents,
usage=usage,
metadata=metadata,
)
yield res
continue
choice = chunk.choices[0]
thinking += (
getattr(choice.delta, "reasoning_content", None) or ""
)
text += choice.delta.content or ""
if (
hasattr(choice.delta, "audio")
and "data" in choice.delta.audio
):
audio += choice.delta.audio["data"]
if (
hasattr(choice.delta, "audio")
and "transcript" in choice.delta.audio
):
text += choice.delta.audio["transcript"]
for tool_call in choice.delta.tool_calls or []:
if tool_call.index in tool_calls:
if tool_call.function.arguments is not None:
tool_calls[tool_call.index][
"input"
] += tool_call.function.arguments
else:
tool_calls[tool_call.index] = {
"type": "tool_use",
"id": tool_call.id,
"name": tool_call.function.name,
"input": tool_call.function.arguments or "",
}
contents = []
if thinking:
contents.append(
ThinkingBlock(
type="thinking",
thinking=thinking,
),
)
if audio:
media_type = self.generate_kwargs.get("audio", {}).get(
"format",
"wav",
)
contents.append(
AudioBlock(
type="audio",
source=Base64Source(
data=audio,
media_type=f"audio/{media_type}",
type="base64",
),
),
)
if text:
contents.append(
TextBlock(
type="text",
text=text,
),
)
if structured_model:
metadata = _json_loads_with_repair(text)
for tool_call in tool_calls.values():
contents.append(
ToolUseBlock(
type=tool_call["type"],
id=tool_call["id"],
name=tool_call["name"],
input=_json_loads_with_repair(
tool_call["input"] or "{}",
),
),
)
if not contents:
continue
res = ChatResponse(
content=contents,
usage=usage,
metadata=metadata,
)
yield res
def _parse_openai_completion_response(
self,
start_datetime: datetime,
response: ChatCompletion,
structured_model: Type[BaseModel] | None = None,
) -> ChatResponse:
"""Given an OpenAI chat completion response object, extract the content
blocks and usages from it.
Args:
start_datetime (`datetime`):
The start datetime of the response generation.
response (`ChatCompletion`):
OpenAI ChatCompletion object to parse.
structured_model (`Type[BaseModel] | None`, default `None`):
A Pydantic BaseModel class that defines the expected structure
for the model's output.
Returns:
ChatResponse (`ChatResponse`):
A ChatResponse object containing the content blocks and usage.
.. note::
If `structured_model` is not `None`, the expected structured output
will be stored in the metadata of the `ChatResponse`.
"""
content_blocks: List[
TextBlock | ToolUseBlock | ThinkingBlock | AudioBlock
] = []
metadata: dict | None = None
if response.choices:
choice = response.choices[0]
if (
hasattr(choice.message, "reasoning_content")
and choice.message.reasoning_content is not None
):
content_blocks.append(
ThinkingBlock(
type="thinking",
thinking=response.choices[0].message.reasoning_content,
),
)
if choice.message.content:
content_blocks.append(
TextBlock(
type="text",
text=response.choices[0].message.content,
),
)
if choice.message.audio:
media_type = self.generate_kwargs.get("audio", {}).get(
"format",
"mp3",
)
content_blocks.append(
AudioBlock(
type="audio",
source=Base64Source(
data=choice.message.audio.data,
media_type=f"audio/{media_type}",
type="base64",
),
),
)
if choice.message.audio.transcript:
content_blocks.append(
TextBlock(
type="text",
text=choice.message.audio.transcript,
),
)
for tool_call in choice.message.tool_calls or []:
content_blocks.append(
ToolUseBlock(
type="tool_use",
id=tool_call.id,
name=tool_call.function.name,
input=_json_loads_with_repair(
tool_call.function.arguments,
),
),
)
if structured_model:
metadata = choice.message.parsed.model_dump()
usage = None
if response.usage:
usage = ChatUsage(
input_tokens=response.usage.prompt_tokens,
output_tokens=response.usage.completion_tokens,
time=(datetime.now() - start_datetime).total_seconds(),
)
parsed_response = ChatResponse(
content=content_blocks,
usage=usage,
metadata=metadata,
)
return parsed_response
def _format_tools_json_schemas(
self,
schemas: list[dict[str, Any]],
) -> list[dict[str, Any]]:
"""Format the tools JSON schemas to the OpenAI format."""
return schemas
def _format_tool_choice(
self,
tool_choice: Literal["auto", "none", "any", "required"] | str | None,
) -> str | dict | None:
"""Format tool_choice parameter for API compatibility.
Args:
tool_choice (`Literal["auto", "none", "any", "required"] | str \
| None`, default `None`):
Controls which (if any) tool is called by the model.
Can be "auto", "none", "any", "required", or specific tool
name. For more details, please refer to
https://platform.openai.com/docs/api-reference/responses/create#responses_create-tool_choice
Returns:
`dict | None`:
The formatted tool choice configuration dict, or None if
tool_choice is None.
"""
if tool_choice is None:
return None
mode_mapping = {
"auto": "auto",
"none": "none",
"any": "required",
"required": "required",
}
if tool_choice in mode_mapping:
return mode_mapping[tool_choice]
return {"type": "function", "function": {"name": tool_choice}}
==== module ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The module in agentscope."""
from ._state_module import StateModule
__all__ = [
"StateModule",
]
---- _state_module.py ----
# -*- coding: utf-8 -*-
"""The state module in agentscope."""
import json
from collections import OrderedDict
from dataclasses import dataclass
from typing import Callable, Any, Optional
from ..types import JSONSerializableObject
@dataclass
class _JSONSerializeFunction:
to_json: Optional[Callable[[Any], Any]] = None
"""The function converting the original data to JSON data."""
load_json: Optional[Callable[[Any], Any]] = None
"""The function converting the JSON data to original data."""
class StateModule:
"""The state module class in agentscope to support nested state
serialization and deserialization."""
def __init__(self) -> None:
"""Initialize the state module."""
self._module_dict = OrderedDict()
self._attribute_dict = OrderedDict()
def __setattr__(self, key: str, value: Any) -> None:
"""Set attributes and record state modules."""
if isinstance(value, StateModule):
if not hasattr(self, "_module_dict"):
raise AttributeError(
f"Call the super().__init__() method within the "
f"constructor of {self.__class__.__name__} before setting "
f"any attributes.",
)
self._module_dict[key] = value
super().__setattr__(key, value)
def __delattr__(self, key: str) -> None:
"""Delete attributes and remove from state modules."""
if key in self._module_dict:
self._module_dict.pop(key)
if key in self._attribute_dict:
self._attribute_dict.pop(key)
super().__delattr__(key)
def state_dict(self) -> dict:
"""Get the state dictionary of the module, including the nested
state modules and registered attributes.
Returns:
`dict`:
A dictionary that keys are attribute names and values are
the state of the attribute.
"""
state = {}
for key in self._module_dict:
attr = getattr(self, key, None)
if isinstance(attr, StateModule):
state[key] = attr.state_dict()
for key in self._attribute_dict:
attr = getattr(self, key)
to_json_function = self._attribute_dict[key].to_json
if to_json_function is not None:
state[key] = to_json_function(attr)
else:
state[key] = attr
return state
def load_state_dict(self, state_dict: dict, strict: bool = True) -> None:
"""Load the state dictionary into the module.
Args:
state_dict (`dict`):
The state dictionary to load.
strict (`bool`, defaults to `True`):
If `True`, raises an error if any key in the module is not
found in the state_dict. If `False`, skips missing keys.
"""
for key in self._module_dict:
if key not in state_dict:
if strict:
raise KeyError(
f"Key '{key}' not found in state_dict. Ensure that "
f"the state_dict contains all required keys.",
)
continue
self._module_dict[key].load_state_dict(state_dict[key])
for key in self._attribute_dict:
if key not in state_dict:
if strict:
raise KeyError(
f"Key '{key}' not found in state_dict. Ensure that "
f"the state_dict contains all required keys.",
)
continue
from_json_func = self._attribute_dict[key].load_json
if from_json_func is not None:
setattr(self, key, from_json_func(state_dict[key]))
else:
setattr(self, key, state_dict[key])
def register_state(
self,
attr_name: str,
custom_to_json: Callable[[Any], JSONSerializableObject] | None = None,
custom_from_json: Callable[[JSONSerializableObject], Any]
| None = None,
) -> None:
"""Register an attribute to be tracked as a state variable.
Args:
attr_name (`str`):
The name of the attribute to register.
custom_to_json (`Callable[[Any], JSONSerializableObject] | None`, \
optional):
A custom function to convert the attribute to a
JSON-serializable format. If not provided, `json.dumps` will
be used.
custom_from_json (`Callable[[JSONSerializableObject], Any] | None`\
, defaults to `None`):
A custom function to convert the JSON dictionary back to the
original attribute format.
"""
attr = getattr(self, attr_name)
if custom_to_json is None:
# Make sure the attribute is JSON serializable natively
try:
json.dumps(attr)
except Exception as e:
raise TypeError(
f"Attribute '{attr_name}' is not JSON serializable. "
"Please provide a custom function to convert the "
"attribute to a JSON-serializable format.",
) from e
if attr_name in self._module_dict:
raise ValueError(
f"Attribute `{attr_name}` is already registered as a module. ",
)
self._attribute_dict[attr_name] = _JSONSerializeFunction(
to_json=custom_to_json,
load_json=custom_from_json,
)
==== pipeline ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The pipeline module in AgentScope, that provides syntactic sugar for
complex workflows and multi-agent conversations."""
from ._msghub import MsgHub
from ._class import SequentialPipeline, FanoutPipeline
from ._functional import (
sequential_pipeline,
fanout_pipeline,
stream_printing_messages,
)
__all__ = [
"MsgHub",
"SequentialPipeline",
"sequential_pipeline",
"FanoutPipeline",
"fanout_pipeline",
"stream_printing_messages",
]
---- _class.py ----
# -*- coding: utf-8 -*-
"""Pipeline classes."""
from typing import Any
from ._functional import sequential_pipeline, fanout_pipeline
from ..agent import AgentBase
from ..message import Msg
class SequentialPipeline:
"""An async sequential pipeline class, which executes a sequence of
agents sequentially. Compared with functional pipeline, this class
can be re-used."""
def __init__(
self,
agents: list[AgentBase],
) -> None:
"""Initialize a sequential pipeline class
Args:
agents (`list[AgentBase]`):
A list of agents.
"""
self.agents = agents
async def __call__(
self,
msg: Msg | list[Msg] | None = None,
) -> Msg | list[Msg] | None:
"""Execute the sequential pipeline
Args:
msg (`Msg | list[Msg] | None`, defaults to `None`):
The initial input that will be passed to the first agent.
"""
return await sequential_pipeline(
agents=self.agents,
msg=msg,
)
class FanoutPipeline:
"""An async fanout pipeline class, which distributes the same input to
multiple agents. Compared with functional pipeline, this class can be
re-used and configured with default parameters."""
def __init__(
self,
agents: list[AgentBase],
enable_gather: bool = True,
) -> None:
"""Initialize a fanout pipeline class
Args:
agents (`list[AgentBase]`):
A list of agents to execute.
enable_gather (`bool`, defaults to `True`):
Whether to execute agents concurrently
using `asyncio.gather()`. If False, agents are executed
sequentially.
"""
self.agents = agents
self.enable_gather = enable_gather
async def __call__(
self,
msg: Msg | list[Msg] | None = None,
**kwargs: Any,
) -> list[Msg]:
"""Execute the fanout pipeline
Args:
msg (`Msg | list[Msg] | None`, defaults to `None`):
The input message that will be distributed to all agents.
**kwargs (`Any`):
Additional keyword arguments passed to each agent during
execution.
Returns:
`list[Msg]`:
A list of output messages from all agents.
"""
return await fanout_pipeline(
agents=self.agents,
msg=msg,
enable_gather=self.enable_gather,
**kwargs,
)
---- _functional.py ----
# -*- coding: utf-8 -*-
"""Functional counterpart for Pipeline"""
import asyncio
from copy import deepcopy
from typing import Any, AsyncGenerator, Tuple, Coroutine
from ..agent import AgentBase
from ..message import Msg
async def sequential_pipeline(
agents: list[AgentBase],
msg: Msg | list[Msg] | None = None,
) -> Msg | list[Msg] | None:
"""An async syntactic sugar pipeline that executes a sequence of agents
sequentially. The output of the previous agent will be passed as the
input to the next agent. The final output will be the output of the
last agent.
Example:
.. code-block:: python
agent1 = ReActAgent(...)
agent2 = ReActAgent(...)
agent3 = ReActAgent(...)
msg_input = Msg("user", "Hello", "user")
msg_output = await sequential_pipeline(
[agent1, agent2, agent3],
msg_input
)
Args:
agents (`list[AgentBase]`):
A list of agents.
msg (`Msg | list[Msg] | None`, defaults to `None`):
The initial input that will be passed to the first agent.
Returns:
`Msg | list[Msg] | None`:
The output of the last agent in the sequence.
"""
for agent in agents:
msg = await agent(msg)
return msg
async def fanout_pipeline(
agents: list[AgentBase],
msg: Msg | list[Msg] | None = None,
enable_gather: bool = True,
**kwargs: Any,
) -> list[Msg]:
"""A fanout pipeline that distributes the same input to multiple agents.
This pipeline sends the same message (or a deep copy of it) to all agents
and collects their responses. Agents can be executed either concurrently
using asyncio.gather() or sequentially depending on the enable_gather
parameter.
Example:
.. code-block:: python
agent1 = ReActAgent(...)
agent2 = ReActAgent(...)
agent3 = ReActAgent(...)
msg_input = Msg("user", "Hello", "user")
# Concurrent execution (default)
results = await fanout_pipeline(
[agent1, agent2, agent3],
msg_input
)
# Sequential execution
results = await fanout_pipeline(
[agent1, agent2, agent3],
msg_input,
enable_gather=False
)
Args:
agents (`list[AgentBase]`):
A list of agents.
msg (`Msg | list[Msg] | None`, defaults to `None`):
The initial input that will be passed to all agents.
enable_gather (`bool`, defaults to `True`):
Whether to execute agents concurrently using `asyncio.gather()`.
If False, agents are executed sequentially.
**kwargs (`Any`):
Additional keyword arguments passed to each agent during execution.
Returns:
`list[Msg]`:
A list of response messages from each agent.
"""
if enable_gather:
tasks = [
asyncio.create_task(agent(deepcopy(msg), **kwargs))
for agent in agents
]
return await asyncio.gather(*tasks)
else:
return [await agent(deepcopy(msg), **kwargs) for agent in agents]
async def stream_printing_messages(
agents: list[AgentBase],
coroutine_task: Coroutine,
end_signal: str = "[END]",
) -> AsyncGenerator[Tuple[Msg, bool], None]:
"""This pipeline will gather the printing messages from agents when
execute the given coroutine task, and yield them one by one.
Only the messages that are printed by `await self.print(msg)` in the agent
will be forwarded to the message queue and yielded by this pipeline.
.. note:: The boolean in the yielded tuple indicates whether the message
is the last **chunk** for a streaming message, not the last message
returned by the agent. That means, there'll be multiple tuples with
`is_last_chunk=True` if the agent prints multiple messages.
.. note:: The messages with the same ``id`` is considered as the same
message, e.g., the chunks of a streaming message.
Args:
agents (`list[AgentBase]`):
A list of agents whose printing messages will be gathered and
yielded.
coroutine_task (`Coroutine`):
The coroutine task to be executed. This task should involve the
execution of the provided agents, so that their printing messages
can be captured and yielded.
end_signal (`str`, defaults to `"[END]"`):
A special signal to indicate the end of message streaming. When
this signal is received from the message queue, the generator will
stop yielding messages and exit the loop.
Returns:
`AsyncGenerator[Tuple[Msg, bool], None]`:
An async generator that yields tuples of (message, is_last_chunk).
The `is_last_chunk` boolean indicates whether the message is the
last chunk in a streaming message.
"""
# Enable the message queue to get the intermediate messages
queue = asyncio.Queue()
for agent in agents:
# Use one queue to gather messages from all agents
agent.set_msg_queue_enabled(True, queue)
# Execute the agent asynchronously
task = asyncio.create_task(coroutine_task)
if task.done():
await queue.put(end_signal)
else:
task.add_done_callback(lambda _: queue.put_nowait(end_signal))
# Receive the messages from the agent's message queue
while True:
# The message obj, and a boolean indicating whether it's the last chunk
# in a streaming message
printing_msg = await queue.get()
# End the loop when the message is None
if isinstance(printing_msg, str) and printing_msg == end_signal:
break
yield printing_msg
---- _msghub.py ----
# -*- coding: utf-8 -*-
"""MsgHub is designed to share messages among a group of agents."""
from typing import Any
import shortuuid
from .._logging import logger
from ..agent import AgentBase
from ..message import Msg
class MsgHub:
"""MsgHub class that controls the subscription of the participated agents.
Example:
In the following example, the reply message from `agent1`, `agent2`,
and `agent3` will be broadcasted to all the other agents in the MsgHub.
.. code-block:: python
with MsgHub(participant=[agent1, agent2, agent3]):
agent1()
agent2()
Actually, it has the same effect as the following code, but much more
easy and elegant!
.. code-block:: python
x1 = agent1()
agent2.observe(x1)
agent3.observe(x1)
x2 = agent2()
agent1.observe(x2)
agent3.observe(x2)
"""
def __init__(
self,
participants: list[AgentBase],
announcement: list[Msg] | Msg | None = None,
enable_auto_broadcast: bool = True,
name: str | None = None,
) -> None:
"""Initialize a MsgHub context manager.
Args:
participants (`list[AgentBase]`):
A list of agents that participate in the MsgHub.
announcement （`list[Msg] | Msg | None`):
The message that will be broadcast to all participants when
entering the MsgHub.
enable_auto_broadcast (`bool`, defaults to `True`):
Whether to enable automatic broadcasting of the replied
message from any participant to all other participants. If
disabled, the MsgHub will only serve as a manual message
broadcaster with the `announcement` argument and the
`broadcast()` method.
name (`str | None`):
The name of this MsgHub. If not provided, a random ID
will be generated.
"""
self.name = name or shortuuid.uuid()
self.participants = participants
self.announcement = announcement
self.enable_auto_broadcast = enable_auto_broadcast
async def __aenter__(self) -> "MsgHub":
"""Will be called when entering the MsgHub."""
self._reset_subscriber()
# broadcast the input message to all participants
if self.announcement is not None:
await self.broadcast(msg=self.announcement)
return self
async def __aexit__(self, *args: Any, **kwargs: Any) -> None:
"""Will be called when exiting the MsgHub."""
if self.enable_auto_broadcast:
for agent in self.participants:
agent.remove_subscribers(self.name)
def _reset_subscriber(self) -> None:
"""Reset the subscriber for agent in `self.participant`"""
if self.enable_auto_broadcast:
for agent in self.participants:
agent.reset_subscribers(self.name, self.participants)
def add(
self,
new_participant: list[AgentBase] | AgentBase,
) -> None:
"""Add new participant into this hub"""
if isinstance(new_participant, AgentBase):
new_participant = [new_participant]
for agent in new_participant:
if agent not in self.participants:
self.participants.append(agent)
self._reset_subscriber()
def delete(
self,
participant: list[AgentBase] | AgentBase,
) -> None:
"""Delete agents from participant."""
if isinstance(participant, AgentBase):
participant = [participant]
for agent in participant:
if agent in self.participants:
# remove agent from self.participant
self.participants.pop(self.participants.index(agent))
else:
logger.warning(
"Cannot find the agent with ID %s, skip its deletion.",
agent.id,
)
# Remove this agent from the subscriber of other agents
self._reset_subscriber()
async def broadcast(self, msg: list[Msg] | Msg) -> None:
"""Broadcast the message to all participants.
Args:
msg (`list[Msg] | Msg`):
Message(s) to be broadcast among all participants.
"""
for agent in self.participants:
await agent.observe(msg)
def set_auto_broadcast(self, enable: bool) -> None:
"""Enable automatic broadcasting of the replied message from any
participant to all other participants.
Args:
enable (`bool`):
Whether to enable automatic broadcasting. If disabled, the
MsgHub will only serve as a manual message broadcaster with
the `announcement` argument and the `broadcast()` method.
"""
if enable:
self.enable_auto_broadcast = True
self._reset_subscriber()
else:
self.enable_auto_broadcast = False
for agent in self.participants:
agent.remove_subscribers(self.name)
==== plan ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The plan module in AgentScope."""
from ._plan_model import (
SubTask,
Plan,
)
from ._plan_notebook import (
DefaultPlanToHint,
PlanNotebook,
)
from ._storage_base import PlanStorageBase
from ._in_memory_storage import InMemoryPlanStorage
__all__ = [
"SubTask",
"Plan",
"DefaultPlanToHint",
"PlanNotebook",
"PlanStorageBase",
"InMemoryPlanStorage",
]
---- _in_memory_storage.py ----
# -*- coding: utf-8 -*-
"""The in-memory plan storage class."""
from collections import OrderedDict
from ._plan_model import Plan
from ._storage_base import PlanStorageBase
class InMemoryPlanStorage(PlanStorageBase):
"""In-memory plan storage."""
def __init__(self) -> None:
"""Initialize the in-memory plan storage."""
super().__init__()
self.plans = OrderedDict()
async def add_plan(self, plan: Plan, override: bool = True) -> None:
"""Add a plan to the storage.
Args:
plan (`Plan`):
The plan to be added.
override (`bool`, defaults to `True`):
Whether to override the existing plan with the same ID.
"""
if plan.id in self.plans and not override:
raise ValueError(
f"Plan with id {plan.id} already exists.",
)
self.plans[plan.id] = plan
async def delete_plan(self, plan_id: str) -> None:
"""Delete a plan from the storage.
Args:
plan_id (`str`):
The ID of the plan to be deleted.
"""
self.plans.pop(plan_id, None)
async def get_plans(self) -> list[Plan]:
"""Get all plans from the storage.
Returns:
`list[Plan]`:
A list of all plans in the storage.
"""
return list(self.plans.values())
async def get_plan(self, plan_id: str) -> Plan | None:
"""Get a plan by its ID.
Args:
plan_id (`str`):
The ID of the plan to be retrieved.
Returns:
`Plan | None`:
The plan with the specified ID, or None if not found.
"""
return self.plans.get(plan_id, None)
---- _plan_model.py ----
# -*- coding: utf-8 -*-
"""The models used in the plan module."""
from typing import Literal
import shortuuid
from pydantic import BaseModel, Field
from .._utils._common import _get_timestamp
class SubTask(BaseModel):
"""The subtask model used in the plan module."""
name: str = Field(
description=(
"The subtask name, should be concise, descriptive and not"
"exceed 10 words."
),
)
description: str = Field(
description=(
"The subtask description, including the constraints, target and "
"outcome to be achieved. The description should be clear, "
"specific and concise, and all the constraints, target and "
"outcome should be specific and measurable."
),
)
expected_outcome: str = Field(
description=(
"The expected outcome of the subtask, which should be specific, "
"concrete and measurable."
),
)
outcome: str | None = Field(
description="The actual outcome of the subtask.",
exclude=True,
default=None,
)
state: Literal["todo", "in_progress", "done", "abandoned"] = Field(
description="The state of the subtask.",
default="todo",
)
created_at: str = Field(
description="The time the subtask was created.",
default_factory=_get_timestamp,
)
# Result related fields
finished_at: str | None = Field(
description="The time the subtask was finished.",
default=None,
exclude=True,
)
def finish(self, outcome: str) -> None:
"""Finish the subtask with the actual outcome."""
self.state = "done"
self.outcome = outcome
self.finished_at = _get_timestamp()
def to_oneline_markdown(self) -> str:
"""Convert the subtask to MarkDown format."""
status_map = {
"todo": "- []",
"in_progress": "- [][WIP]",
"done": "- [x]",
"abandoned": "- [][Abandoned]",
}
return f"{status_map[self.state]} {self.name}"
def to_markdown(self, detailed: bool = False) -> str:
"""Convert the subtask to MarkDown format.
Args:
detailed (`bool`, defaults to `False`):
Whether to include detailed information about the subtask.
"""
status_map = {
"todo": "- [ ] ",
"in_progress": "- [ ] [WIP]",
"done": "- [x] ",
"abandoned": "- [ ] [Abandoned]",
}
if detailed:
markdown_strs = [
f"{status_map[self.state]}{self.name}",
f"\t- Created At: {self.created_at}",
f"\t- Description: {self.description}",
f"\t- Expected Outcome: {self.expected_outcome}",
f"\t- State: {self.state}",
]
if self.state == "done":
markdown_strs.extend(
[
f"\t- Finished At: {self.finished_at}",
f"\t- Actual Outcome: {self.outcome}",
],
)
return "\n".join(markdown_strs)
return f"{status_map[self.state]}{self.name}"
class Plan(BaseModel):
"""The plan model used in the plan module, contains a list of subtasks."""
id: str = Field(exclude=True, default_factory=shortuuid.uuid)
name: str = Field(
description=(
"The plan name, should be concise, descriptive and not exceed 10 "
"words."
),
)
description: str = Field(
description=(
"The plan description, including the constraints, target and "
"outcome to be achieved. The description should be clear, "
"specific and concise, and all the constraints, target and "
"outcome should be specific and measurable."
),
)
expected_outcome: str = Field(
description=(
"The expected outcome of the plan, which should be specific, "
"concrete and measurable."
),
)
subtasks: list[SubTask] = Field(
description=("A list of subtasks that make up the plan."),
)
created_at: str = Field(
description="The time the plan was created.",
default_factory=_get_timestamp,
exclude=True,
)
state: Literal["todo", "in_progress", "done", "abandoned"] = Field(
description="The state of the plan.",
default="todo",
exclude=True,
)
finished_at: str | None = Field(
description="The time the plan was finished.",
default=None,
exclude=True,
)
outcome: str | None = Field(
description="The actual outcome of the plan.",
default=None,
exclude=True,
)
def finish(
self,
state: Literal["done", "abandoned"],
outcome: str,
) -> None:
"""Finish the plan."""
self.state = state
self.outcome = outcome
self.finished_at = _get_timestamp()
def to_markdown(self, detailed: bool = False) -> str:
"""Convert the plan to MarkDown format."""
subtasks_markdown = "\n".join(
[
subtask.to_markdown(
detailed=detailed,
)
for subtask in self.subtasks
],
)
return "\n".join(
[
f"# {self.name}",
f"**Description**: {self.description}",
f"**Expected Outcome**: {self.expected_outcome}",
f"**State**: {self.state}",
f"**Created At**: {self.created_at}",
"## Subtasks",
subtasks_markdown,
],
)
---- _plan_notebook.py ----
# -*- coding: utf-8 -*-
"""The plan notebook class, used to manage the plan, providing hints and
tool functions to the agent."""
from collections import OrderedDict
from typing import Callable, Literal, Coroutine, Any
from ._in_memory_storage import InMemoryPlanStorage
from ._plan_model import SubTask, Plan
from ._storage_base import PlanStorageBase
from .._utils._common import _execute_async_or_sync_func
from ..message import TextBlock, Msg
from ..module import StateModule
from ..tool import ToolResponse
class DefaultPlanToHint:
"""The default function to generate the hint message based on the current
plan to guide the agent on next steps."""
hint_prefix: str = "<system-hint>"
hint_suffix: str = "</system-hint>"
no_plan: str = (
"If the user's query is complex (e.g. programming a website, game or "
"app), or requires a long chain of steps to complete (e.g. conduct "
"research on a certain topic from different sources), you NEED to "
"create a plan first by calling 'create_plan'. Otherwise, you can "
"directly execute the user's query without planning."
)
at_the_beginning: str = (
"The current plan:\n"
"```\n"
"{plan}\n"
"```\n"
"Your options include:\n"
"- Mark the first subtask as 'in_progress' by calling "
"'update_subtask_state' with subtask_idx=0 and state='in_progress', "
"and start executing it.\n"
"- If the first subtask is not executable, analyze why and what you "
"can do to advance the plan, e.g. ask user for more information, "
"revise the plan by calling 'revise_current_plan'.\n"
"- If the user asks you to do something unrelated to the plan, "
"prioritize the completion of user's query first, and then return "
"to the plan afterward.\n"
"- If the user no longer wants to perform the current plan, confirm "
"with the user and call the 'finish_plan' function.\n"
)
when_a_subtask_in_progress: str = (
"The current plan:\n"
"```\n"
"{plan}\n"
"```\n"
"Now the subtask at index {subtask_idx}, named '{subtask_name}', is "
"'in_progress'. Its details are as follows:\n"
"```\n"
"{subtask}\n"
"```\n"
"Your options include:\n"
"- Go on execute the subtask and get the outcome.\n"
"- Call 'finish_subtask' with the specific outcome if the subtask is "
"finished.\n"
"- Ask the user for more information if you need.\n"
"- Revise the plan by calling 'revise_current_plan' if necessary.\n"
"- If the user asks you to do something unrelated to the plan, "
"prioritize the completion of user's query first, and then return to "
"the plan afterward."
)
when_no_subtask_in_progress: str = (
"The current plan:\n"
"```\n"
"{plan}\n"
"```\n"
"The first {index} subtasks are done, and there is no subtask "
"'in_progress'. Now Your options include:\n"
"- Mark the next subtask as 'in_progress' by calling "
"'update_subtask_state', and start executing it.\n"
"- Ask the user for more information if you need.\n"
"- Revise the plan by calling 'revise_current_plan' if necessary.\n"
"- If the user asks you to do something unrelated to the plan, "
"prioritize the completion of user's query first, and then return to "
"the plan afterward."
)
at_the_end: str = (
"The current plan:\n"
"```\n"
"{plan}\n"
"```\n"
"All the subtasks are done. Now your options are:\n"
"- Finish the plan by calling 'finish_plan' with the specific "
"outcome, and summarize the whole process and outcome to the user.\n"
"- Revise the plan by calling 'revise_current_plan' if necessary.\n"
"- If the user asks you to do something unrelated to the plan, "
"prioritize the completion of user's query first, and then return to "
"the plan afterward."
)
def __call__(self, plan: Plan | None) -> str | None:
"""Generate the hint message based on the input plan to guide the
agent on next steps.
Args:
plan (`Plan | None`):
The current plan, used to generate the hint message.
Returns:
`str | None`:
The generated hint message, or None if the plan is None or
there is no relevant hint.
"""
if plan is None:
hint = self.no_plan
else:
# Count the number of subtasks in each state
n_todo, n_in_progress, n_done, n_abandoned = 0, 0, 0, 0
in_progress_subtask_idx = None
for idx, subtask in enumerate(plan.subtasks):
if subtask.state == "todo":
n_todo += 1
elif subtask.state == "in_progress":
n_in_progress += 1
in_progress_subtask_idx = idx
elif subtask.state == "done":
n_done += 1
elif subtask.state == "abandoned":
n_abandoned += 1
hint = None
if n_in_progress == 0 and n_done == 0:
# All subtasks are todo
hint = self.at_the_beginning.format(
plan=plan.to_markdown(),
)
elif n_in_progress > 0 and in_progress_subtask_idx is not None:
# One subtask is in_progress
hint = self.when_a_subtask_in_progress.format(
plan=plan.to_markdown(),
subtask_idx=in_progress_subtask_idx,
subtask_name=plan.subtasks[in_progress_subtask_idx].name,
subtask=plan.subtasks[in_progress_subtask_idx].to_markdown(
detailed=True,
),
)
elif n_in_progress == 0 and n_done > 0:
# No subtask is in_progress, and some subtasks are done
hint = self.when_no_subtask_in_progress.format(
plan=plan.to_markdown(),
index=n_done,
)
elif n_done + n_abandoned == len(plan.subtasks):
# All subtasks are done or abandoned
hint = self.at_the_end.format(
plan=plan.to_markdown(),
)
if hint:
return f"{self.hint_prefix}{hint}{self.hint_suffix}"
return hint
class PlanNotebook(StateModule):
"""The plan notebook to manage the plan, providing hints and plan related
tool functions to the agent."""
_plan_change_hooks: dict[str, Callable[["PlanNotebook", Plan], None]]
"""The hooks that will be triggered when the plan is changed. For example,
used to display the plan on the frontend."""
description: str = (
"The plan-related tools. Activate this tool when you need to execute "
"complex task, e.g. building a website or a game. Once activated, "
"you'll enter the plan mode, where you will be guided to complete "
"the given query by creating and following a plan, and hint message "
"wrapped by <system-hint></system-hint> will guide you to complete "
"the task. If you think the user no longer wants to perform the "
"current task, you need to confirm with the user and call the "
"'finish_plan' function."
)
def __init__(
self,
max_subtasks: int | None = None,
plan_to_hint: Callable[[Plan | None], str | None] | None = None,
storage: PlanStorageBase | None = None,
) -> None:
"""Initialize the plan notebook.
Args:
max_subtasks (`int | None`, optional):
The maximum number of subtasks in a plan.
plan_to_hint (`Callable[[Plan | None], str | None] | None`, \
optional):
The function to generate the hint message based on the
current plan. If not provided, a default `DefaultPlanToHint`
object will be used.
storage (`PlanStorageBase | None`, optional):
The plan storage. If not provided, an in-memory storage will
be used.
"""
super().__init__()
self.max_tasks = max_subtasks
self.plan_to_hint = plan_to_hint or DefaultPlanToHint()
self.storage = storage or InMemoryPlanStorage()
self.current_plan: Plan | None = None
self._plan_change_hooks = OrderedDict()
# Register the current_plan state for state management
self.register_state(
"current_plan",
custom_to_json=lambda _: _.model_dump() if _ else None,
custom_from_json=lambda _: Plan.model_validate(_) if _ else None,
)
async def create_plan(
self,
name: str,
description: str,
expected_outcome: str,
subtasks: list[SubTask],
) -> ToolResponse:
"""Create a plan by given name and sub-tasks.
Args:
name (`str`):
The plan name, should be concise, descriptive and not exceed
10 words.
description (`str`):
The plan description, including the constraints, target and
outcome to be achieved. The description should be clear,
specific and concise, and all the constraints, target and
outcome should be specific and measurable.
expected_outcome (`str`):
The expected outcome of the plan, which should be specific,
concrete and measurable.
subtasks (`list[SubTask]`):
A list of sequential sub-tasks that make up the plan.
Returns:
`ToolResponse`:
The response of the tool call.
"""
plan = Plan(
name=name,
description=description,
expected_outcome=expected_outcome,
subtasks=subtasks,
)
if self.current_plan is None:
res = ToolResponse(
content=[
TextBlock(
type="text",
text=f"Plan '{name}' created successfully.",
),
],
)
else:
res = ToolResponse(
content=[
TextBlock(
type="text",
text=(
"The current plan named "
f"'{self.current_plan.name}' is replaced by the "
f"newly created plan named '{name}'."
),
),
],
)
self.current_plan = plan
await self._trigger_plan_change_hooks()
return res
def _validate_current_plan(self) -> None:
"""Validate the current plan."""
if self.current_plan is None:
raise ValueError(
"The current plan is None, you need to create a plan by "
"calling create_plan() first.",
)
async def revise_current_plan(
self,
subtask_idx: int,
action: Literal["add", "revise", "delete"],
subtask: SubTask | None = None,
) -> ToolResponse:
"""Revise the current plan by adding, revising or deleting a sub-task.
Args:
subtask_idx (`int`):
The index of the sub-task to be revised, starting from 0.
action (`Literal["add", "revise", "delete"]`):
The action to be performed on the sub-task. If "add", the
sub-task will be inserted before the given index. If "revise",
the sub-task at the given index will be revised. If "delete",
the sub-task at the given index will be deleted.
subtask (`SubTask | None`, optional):
The sub-task to be added or revised. Required if action is
"add" or "revise".
Raises:
`ValueError`:
If the current plan is `None`, `ValueError` will be raised.
Returns:
`ToolResponse`:
The response of the tool call.
"""
# Validate the arguments first
response: list[str] = []
if isinstance(subtask_idx, str):
try:
subtask_idx = int(subtask_idx)
except ValueError:
pass
if not isinstance(subtask_idx, int):
response.append(
f"Invalid type for argument 'subtask_idx'. "
f"Expected 'int', but got '{type(subtask_idx)}'.",
)
if action not in ["add", "revise", "delete"]:
response.append(
f"Invalid action '{action}'. Must be one of 'add', 'revise', "
f"'delete'.",
)
if action in ["add", "revise"] and subtask is None:
response.append(
f"The subtask must be provided when action is '{action}', "
"but got None.",
)
self._validate_current_plan()
# validate subtask_idx
if action != "add" and subtask_idx >= len(self.current_plan.subtasks):
response.append(
f"Invalid subtask_idx '{subtask_idx}' for action '{action}'. "
f"Must be between 0 "
f"and {len(self.current_plan.subtasks) - 1}.",
)
if action == "add" and not (
0 <= subtask_idx <= len(self.current_plan.subtasks)
):
response.append(
f"Invalid subtask_idx '{subtask_idx}' for action 'add'. "
f"Must be between 0 and {len(self.current_plan.subtasks)}.",
)
if response:
return ToolResponse(
content=[
TextBlock(
type="text",
text="Error: " + response[0],
),
],
)
if subtask is not None:
# Convert to SubTask model if it's a dict
subtask = SubTask.model_validate(subtask)
if action == "delete":
subtask = self.current_plan.subtasks.pop(subtask_idx)
await self._trigger_plan_change_hooks()
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Subtask (named '{subtask.name}') at index "
f"{subtask_idx} is deleted successfully.",
),
],
)
if action == "add" and subtask:
self.current_plan.subtasks.insert(subtask_idx, subtask)
await self._trigger_plan_change_hooks()
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"New subtask is added successfully at index "
f"{subtask_idx}.",
),
],
)
# revise
self.current_plan.subtasks[subtask_idx] = subtask
await self._trigger_plan_change_hooks()
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Subtask at index {subtask_idx} is revised "
f"successfully.",
),
],
)
async def update_subtask_state(
self,
subtask_idx: int,
state: Literal["todo", "in_progress", "deprecated"],
) -> ToolResponse:
"""Update the state of a subtask by given index and state. Note if you
want to mark a subtask as done, you SHOULD call `finish_subtask`
instead with the specific outcome.
Args:
subtask_idx (`int`):
The index of the subtask to be updated, starting from 0.
state (`Literal["todo", "in_progress", "abandoned"]`):
The new state of the subtask. If you want to mark a subtask
as done, you SHOULD call `finish_subtask` instead with the
specific outcome.
"""
self._validate_current_plan()
if isinstance(subtask_idx, str):
try:
subtask_idx = int(subtask_idx)
except ValueError:
pass
if not isinstance(subtask_idx, int):
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Invalid type for argument 'subtask_idx'. "
f"Expected 'int', but got '{type(subtask_idx)}'.",
),
],
)
if not 0 <= subtask_idx < len(self.current_plan.subtasks):
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Invalid subtask_idx '{subtask_idx}'. Must "
f"be between 0 and "
f"{len(self.current_plan.subtasks) - 1}.",
),
],
)
if state not in ["todo", "in_progress", "abandoned"]:
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Invalid state '{state}'. Must be one of "
"'todo', 'in_progress', 'abandoned'.",
),
],
)
# Only one subtask can be in_progress at a time
if state == "in_progress":
# Check only one subtask is in_progress
for idx, subtask in enumerate(self.current_plan.subtasks):
# Check all previous subtasks are done or deprecated
if idx < subtask_idx and subtask.state not in [
"done",
"deprecated",
]:
return ToolResponse(
content=[
TextBlock(
type="text",
text=(
f"Subtask (at index {idx}) named "
f"'{subtask.name}' is not done yet. You "
"should finish the previous subtasks "
"first."
),
),
],
)
# Check no other subtask is in_progress
if subtask.state == "in_progress":
return ToolResponse(
content=[
TextBlock(
type="text",
text=(
f"Subtask (at index {idx}) named "
f"'{subtask.name}' is already "
"'in_progress'. You should finish it "
"first before starting another subtask."
),
),
],
)
self.current_plan.subtasks[subtask_idx].state = state
await self._trigger_plan_change_hooks()
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Subtask at index {subtask_idx}, named "
f"'{self.current_plan.subtasks[subtask_idx].name}' "
f"is marked as '{state}' successfully.",
),
],
)
async def finish_subtask(
self,
subtask_idx: int,
subtask_outcome: str,
) -> ToolResponse:
"""Label the subtask as done by given index and outcome.
Args:
subtask_idx (`int`):
The index of the sub-task to be marked as done, starting
from 0.
subtask_outcome (`str`):
The specific outcome of the sub-task, should exactly match the
expected outcome in the sub-task description. SHOULDN't be
what you did or general description, e.g. "I have searched
xxx", "I have written the code for xxx", etc. It SHOULD be
the specific data, information, or path to the file, e.g.
"There are 5 articles about xxx, they are\n- xxx\n- xxx\n..."
"""
self._validate_current_plan()
if isinstance(subtask_idx, str):
try:
subtask_idx = int(subtask_idx)
except ValueError:
pass
if not isinstance(subtask_idx, int):
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Invalid type for argument 'subtask_idx'. "
f"Expected 'int', but got '{type(subtask_idx)}'.",
),
],
)
if not 0 <= subtask_idx < len(self.current_plan.subtasks):
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Invalid subtask_idx '{subtask_idx}'. Must "
f"be between 0 and "
f"{len(self.current_plan.subtasks) - 1}.",
),
],
)
for idx, subtask in enumerate(
self.current_plan.subtasks[0:subtask_idx],
):
if subtask.state not in ["done", "deprecated"]:
return ToolResponse(
content=[
TextBlock(
type="text",
text=(
"Cannot finish subtask at index "
f"{subtask_idx} because the previous "
f"subtask (at index {idx}) named "
f"'{subtask.name}' is not done yet. You "
"should finish the previous subtasks first."
),
),
],
)
# Label the subtask as done
self.current_plan.subtasks[subtask_idx].finish(subtask_outcome)
# Auto activate the next subtask if exists
if subtask_idx + 1 < len(self.current_plan.subtasks):
self.current_plan.subtasks[subtask_idx + 1].state = "in_progress"
next_subtask = self.current_plan.subtasks[subtask_idx + 1]
await self._trigger_plan_change_hooks()
return ToolResponse(
content=[
TextBlock(
type="text",
text=(
f"Subtask (at index {subtask_idx}) named "
f"'{self.current_plan.subtasks[subtask_idx].name}'"
" is marked as done successfully. The next "
f"subtask named '{next_subtask.name}' is "
f"activated."
),
),
],
)
await self._trigger_plan_change_hooks()
return ToolResponse(
content=[
TextBlock(
type="text",
text=(
f"Subtask (at index {subtask_idx}) named "
f"'{self.current_plan.subtasks[subtask_idx].name}'"
" is marked as done successfully. "
),
),
],
)
async def view_subtasks(self, subtask_idx: list[int]) -> ToolResponse:
"""View the details of the sub-tasks by given indexes.
Args:
subtask_idx (`list[int]`):
The indexes of the sub-tasks to be viewed, starting from 0.
"""
self._validate_current_plan()
gathered_strs = []
invalid_subtask_idx = []
for idx in subtask_idx:
if not 0 <= idx < len(self.current_plan.subtasks):
invalid_subtask_idx.append(idx)
continue
subtask_markdown = self.current_plan.subtasks[idx].to_markdown(
detailed=True,
)
gathered_strs.append(
f"Subtask at index {idx}:\n"
"```\n"
f"{subtask_markdown}\n"
"```\n",
)
if invalid_subtask_idx:
gathered_strs.append(
f"Invalid subtask_idx '{invalid_subtask_idx}'. Must be "
f"between 0 and {len(self.current_plan.subtasks) - 1}.",
)
return ToolResponse(
content=[
TextBlock(
type="text",
text="\n".join(gathered_strs),
),
],
)
async def finish_plan(
self,
state: Literal["done", "abandoned"],
outcome: str,
) -> ToolResponse:
"""Finish the current plan by given outcome, or abandon it with the
given reason if the user no longer wants to perform it. Note that you
SHOULD confirm with the user before abandoning the plan.
Args:
state (`Literal["done", "abandoned"]`):
The state to finish the plan. If "done", the plan will be
marked as done with the given outcome. If "abandoned", the
plan will be abandoned with the given reason.
outcome (`str`):
The specific outcome of the plan if state is "done", or the
reason for abandoning the plan if state is "abandoned".
"""
if self.current_plan is None:
return ToolResponse(
content=[
TextBlock(
type="text",
text="There is no plan to finish.",
),
],
)
self.current_plan.finish(state, outcome)
# Store the finished plan into history
await self.storage.add_plan(self.current_plan)
self.current_plan = None
await self._trigger_plan_change_hooks()
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"The current plan is finished successfully as "
f"'{state}'.",
),
],
)
async def view_historical_plans(self) -> ToolResponse:
"""View the historical plans."""
historical_plans = await self.storage.get_plans()
plans_str = [
f"""Plan named '{_.name}':
- ID: {_.id}
- Created at: {_.created_at}
- Description: {_.description}
- State: {_.state}
"""
for _ in historical_plans
]
return ToolResponse(
content=[
TextBlock(
type="text",
text="\n".join(plans_str),
),
],
)
async def recover_historical_plan(self, plan_id: str) -> ToolResponse:
"""Recover a historical plan by given plan ID, the plan ID can be
obtained by calling `view_historical_plans`. Note the recover
operation will override the current plan if exists.
Args:
plan_id (`str`):
The ID of the historical plan to be recovered.
"""
historical_plan = await self.storage.get_plan(plan_id)
if historical_plan is None:
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Cannot find the plan with ID '{plan_id}'.",
),
],
)
# Store the current plan into history if exists
if self.current_plan:
if self.current_plan.state != "done":
self.current_plan.finish(
"abandoned",
f"The plan execution is interrupted by a new plan "
f"with ID '{historical_plan.id}'.",
)
await self.storage.add_plan(self.current_plan)
res = ToolResponse(
content=[
TextBlock(
type="text",
text=(
"The current plan named "
f"'{self.current_plan.name}' is replaced by the "
f"historical plan named '{historical_plan.name}' "
f"with ID '{historical_plan.id}'."
),
),
],
)
else:
res = ToolResponse(
content=[
TextBlock(
type="text",
text=(
f"Historical plan named '{historical_plan.name}' "
f"with ID '{historical_plan.id}' is recovered "
"successfully."
),
),
],
)
self.current_plan = historical_plan
return res
def list_tools(
self,
) -> list[Callable[..., Coroutine[Any, Any, ToolResponse]]]:
"""List all tool functions provided to agent
Returns:
`list[Callable[..., ToolResponse]]`:
A list of all tool functions provided by the plan notebook to
the agent.
"""
return [
self.view_subtasks,
self.update_subtask_state,
self.finish_subtask,
self.create_plan,
self.revise_current_plan,
self.finish_plan,
self.view_historical_plans,
self.recover_historical_plan,
]
async def get_current_hint(self) -> Msg | None:
"""Get the hint message based on the current plan and subtasks states.
This function will call the `plan_to_hint` function to generate the
hint message.
Returns:
`Msg | None`:
The hint message wrapped by <system-hint></system-hint>, or
None if there is no relevant hint.
"""
hint_content = self.plan_to_hint(self.current_plan)
if hint_content:
msg = Msg(
"user",
hint_content,
"user",
)
return msg
return None
def register_plan_change_hook(
self,
hook_name: str,
hook: Callable[["PlanNotebook", Plan], None],
) -> None:
"""Register a plan hook that will be triggered when the plan is
changed.
Args:
hook_name (`str`):
The name of the hook, should be unique.
hook (`Callable[[Plan], None]`):
The hook function, which takes the current plan as input and
returns nothing.
"""
self._plan_change_hooks[hook_name] = hook
def remove_plan_change_hook(self, hook_name: str) -> None:
"""Remove a plan change hook by given name.
Args:
hook_name (`str`):
The name of the hook to be removed.
"""
if hook_name in self._plan_change_hooks:
self._plan_change_hooks.pop(hook_name)
else:
raise ValueError(f"Hook '{hook_name}' not found.")
async def _trigger_plan_change_hooks(self) -> None:
"""Trigger all the plan change hooks."""
for hook in self._plan_change_hooks.values():
await _execute_async_or_sync_func(
hook,
self,
self.current_plan,
)
---- _storage_base.py ----
# -*- coding: utf-8 -*-
"""The base class for plan storage."""
from abc import abstractmethod
from agentscope.module import StateModule
from agentscope.plan._plan_model import Plan
class PlanStorageBase(StateModule):
"""The base class for plan storage."""
@abstractmethod
async def add_plan(self, plan: Plan) -> None:
"""Add a plan to the storage."""
@abstractmethod
async def delete_plan(self, plan_id: str) -> None:
"""Delete a plan from the storage."""
@abstractmethod
async def get_plans(self) -> list[Plan]:
"""Get all plans from the storage."""
@abstractmethod
async def get_plan(self, plan_id: str) -> Plan | None:
"""Get a plan by its ID."""
==== rag ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The retrieval-augmented generation (RAG) module in AgentScope."""
from ._document import (
DocMetadata,
Document,
)
from ._reader import (
ReaderBase,
TextReader,
PDFReader,
ImageReader,
)
from ._store import (
VDBStoreBase,
QdrantStore,
MilvusLiteStore,
)
from ._knowledge_base import KnowledgeBase
from ._simple_knowledge import SimpleKnowledge
__all__ = [
"ReaderBase",
"TextReader",
"PDFReader",
"ImageReader",
"DocMetadata",
"Document",
"VDBStoreBase",
"QdrantStore",
"MilvusLiteStore",
"KnowledgeBase",
"SimpleKnowledge",
]
---- _document.py ----
# -*- coding: utf-8 -*-
"""The document data structure used in RAG as the data chunk and
retrieval result."""
from dataclasses import dataclass, field
import shortuuid
from dashscope.api_entities.dashscope_response import DictMixin
from ..message import (
TextBlock,
ImageBlock,
VideoBlock,
)
from ..types import Embedding
@dataclass
class DocMetadata(DictMixin):
"""The metadata of the document."""
content: TextBlock | ImageBlock | VideoBlock
"""The data content, e.g., text, image, video."""
doc_id: str
"""The document ID."""
chunk_id: int
"""The chunk ID."""
total_chunks: int
"""The total number of chunks."""
@dataclass
class Document:
"""The data chunk."""
metadata: DocMetadata
"""The metadata of the data chunk."""
id: str = field(default_factory=shortuuid.uuid)
"""The unique ID of the data chunk."""
# The fields that will be filled when the document is added to or
# retrieved from the knowledge base.
embedding: Embedding | None = field(default_factory=lambda: None)
"""The embedding of the data chunk."""
score: float | None = None
"""The relevance score of the data chunk."""
---- _knowledge_base.py ----
# -*- coding: utf-8 -*-
"""The knowledge base abstraction for retrieval-augmented generation (RAG)."""
from abc import abstractmethod
from typing import Any
from ._reader import Document
from ..embedding import EmbeddingModelBase
from ._store import VDBStoreBase
from ..message import TextBlock
from ..tool import ToolResponse
class KnowledgeBase:
"""The knowledge base abstraction for retrieval-augmented generation
(RAG).
The ``retrieve`` and ``add_documents`` methods need to be implemented
in the subclasses. We also provide a quick method ``retrieve_knowledge``
that enables the agent to retrieve knowledge easily.
"""
embedding_store: VDBStoreBase
"""The embedding store for the knowledge base."""
embedding_model: EmbeddingModelBase
"""The embedding model for the knowledge base."""
def __init__(
self,
embedding_store: VDBStoreBase,
embedding_model: EmbeddingModelBase,
) -> None:
"""Initialize the knowledge base."""
self.embedding_store = embedding_store
self.embedding_model = embedding_model
@abstractmethod
async def retrieve(
self,
query: str,
limit: int = 5,
score_threshold: float | None = None,
**kwargs: Any,
) -> list[Document]:
"""Retrieve relevant documents by the given query.
Args:
query (`str`):
The query string to retrieve relevant documents.
limit (`int`, defaults to 5):
The number of relevant documents to retrieve.
score_threshold (`float | None`, defaults to `None`):
The score threshold to filter the retrieved documents. If
provided, only documents with a score higher than the
threshold will be returned.
**kwargs (`Any`):
Other keyword arguments for the vector database search API.
"""
@abstractmethod
async def add_documents(
self,
documents: list[Document],
**kwargs: Any,
) -> None:
"""Add documents to the knowledge base, which will embed the documents
and store them in the embedding store.
Args:
documents (`list[Document]`):
A list of documents to add.
"""
# A quick method that enable the agent to retrieve knowledge
# Developers can wrap the `retrieve` method by themselves to support
# more flexible usage
async def retrieve_knowledge(
self,
query: str,
limit: int = 5,
score_threshold: float | None = None,
**kwargs: Any,
) -> ToolResponse:
"""Retrieve relevant documents from the knowledge base. Note the
`query` parameter is directly related to the retrieval quality, and
for the same question, you can try many different queries to get the
best results. Adjust the `limit` and `score_threshold` parameters
to get more or fewer results.
Args:
query (`str`):
The query string, which should be specific and concise. For
example, you should provide the specific name instead of
"you", "my", "he", "she", etc.
limit (`int`, defaults to 3):
The number of relevant documents to retrieve.
score_threshold (`float`, defaults to 0.8):
A threshold in [0, 1] and only the relevance score above this
threshold will be returned. Reduce this value to get more
results.
"""
docs = await self.retrieve(
query=query,
limit=limit,
score_threshold=score_threshold,
**kwargs,
)
if len(docs):
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Score: {_.score}, "
f"Content: {_.metadata.content['text']}",
)
for _ in docs
],
)
return ToolResponse(
content=[
TextBlock(
type="text",
text="No relevant documents found. TRY to reduce the "
"`score_threshold` parameter to get "
"more results.",
),
],
)
---- _simple_knowledge.py ----
# -*- coding: utf-8 -*-
"""A general implementation of the knowledge class in AgentScope RAG module."""
from typing import Any
from ._reader import Document
from ..message import TextBlock
from ._knowledge_base import KnowledgeBase
class SimpleKnowledge(KnowledgeBase):
"""A simple knowledge base implementation."""
async def retrieve(
self,
query: str,
limit: int = 5,
score_threshold: float | None = None,
**kwargs: Any,
) -> list[Document]:
"""Retrieve relevant documents by the given queries.
Args:
query (`str`):
The query string to retrieve relevant documents.
limit (`int`, defaults to 5):
The number of relevant documents to retrieve.
score_threshold: float | None = None,
The threshold of the score to filter the results.
**kwargs (`Any`):
Other keyword arguments for the vector database search API.
Returns:
`list[Document]`:
A list of relevant documents.
TODO: handle the case when the query is too long.
"""
res_embedding = await self.embedding_model(
[
TextBlock(
type="text",
text=query,
),
],
)
res = await self.embedding_store.search(
res_embedding.embeddings[0],
limit=limit,
score_threshold=score_threshold,
**kwargs,
)
return res
async def add_documents(
self,
documents: list[Document],
**kwargs: Any,
) -> None:
"""Add documents to the knowledge
Args:
documents (`list[Document]`):
The list of documents to add.
"""
# Prepare the content to be embedded
for doc in documents:
if (
doc.metadata.content["type"]
not in self.embedding_model.supported_modalities
):
raise ValueError(
f"The embedding model {self.embedding_model.model_name} "
f"does not support {doc.metadata.content['type']} data.",
)
# Get the embeddings
res_embeddings = await self.embedding_model(
[_.metadata.content for _ in documents],
)
for doc, embedding in zip(documents, res_embeddings.embeddings):
doc.embedding = embedding
await self.embedding_store.add(documents)
==== _reader ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The reader abstraction for retrieval-augmented generation (RAG)."""
from ._reader_base import ReaderBase, Document
from ._text_reader import TextReader
from ._pdf_reader import PDFReader
from ._image_reader import ImageReader
__all__ = [
"Document",
"ReaderBase",
"TextReader",
"PDFReader",
"ImageReader",
]
---- _image_reader.py ----
# -*- coding: utf-8 -*-
"""The Image reader modules"""
import hashlib
from .. import DocMetadata
from ...message import ImageBlock, URLSource
from .._reader import ReaderBase, Document
class ImageReader(ReaderBase):
"""A simple image reader that wraps the image into a Document object.
This class is only a simple implementation to support multimodal RAG.
"""
async def __call__(self, image_url: str | list[str]) -> list[Document]:
"""Read an image and return the wrapped Document object.
Args:
image_url (`str | list[str]`):
The image URL(s) or path(s).
Returns:
`list[Document]`:
A list of Document objects containing the image data.
"""
# Read the image data and wrap it into a Document object.
if isinstance(image_url, str):
image_url = [image_url]
image_blocks: list[ImageBlock] = [
ImageBlock(
type="image",
source=URLSource(
type="url",
url=_,
),
)
for _ in image_url
]
doc_idx = [self.get_doc_id(_) for _ in image_url]
return [
Document(
metadata=DocMetadata(
content=image_block,
doc_id=doc_id,
chunk_id=0,
total_chunks=1,
),
)
for doc_id, image_block in zip(doc_idx, image_blocks)
]
def get_doc_id(self, image_path: str) -> str:
"""Generate a document ID based on the image path.
Args:
image_path (`str`):
The image path or URL.
Returns:
`str`:
The generated document ID.
"""
return hashlib.md5(image_path.encode("utf-8")).hexdigest()
---- _pdf_reader.py ----
# -*- coding: utf-8 -*-
"""The PDF reader to read and chunk PDF files."""
import hashlib
from typing import Literal
from ._reader_base import ReaderBase
from ._text_reader import TextReader
from .._document import Document
class PDFReader(ReaderBase):
"""The PDF reader that splits text into chunks by a fixed chunk size."""
def __init__(
self,
chunk_size: int = 512,
split_by: Literal["char", "sentence", "paragraph"] = "sentence",
) -> None:
"""Initialize the text reader.
Args:
chunk_size (`int`, default to 512):
The size of each chunk, in number of characters.
split_by (`Literal["char", "sentence", "paragraph"]`, default to \
"sentence"):
The unit to split the text, can be "char", "sentence", or
"paragraph". The "sentence" option is implemented using the
"nltk" library, which only supports English text.
"""
if chunk_size <= 0:
raise ValueError(
f"The chunk_size must be positive, got {chunk_size}",
)
if split_by not in ["char", "sentence", "paragraph"]:
raise ValueError(
"The split_by must be one of 'char', 'sentence' or "
f"'paragraph', got {split_by}",
)
self.chunk_size = chunk_size
self.split_by = split_by
# To avoid code duplication, we use TextReader to do the chunking.
self._text_reader = TextReader(
self.chunk_size,
self.split_by,
)
async def __call__(
self,
pdf_path: str,
) -> list[Document]:
"""Read a PDF file, split it into chunks, and return a list of
Document objects.
Args:
pdf_path (`str`):
The input PDF file path.
"""
try:
from pypdf import PdfReader
except ImportError as e:
raise ImportError(
"Please install pypdf to use the PDF reader. "
"You can install it by `pip install pypdf`.",
) from e
reader = PdfReader(pdf_path)
gather_texts = []
for page in reader.pages:
gather_texts.append(page.extract_text())
doc_id = hashlib.sha256(pdf_path.encode("utf-8")).hexdigest()
docs = await self._text_reader("\n\n".join(gather_texts))
for doc in docs:
doc.id = doc_id
return docs
def get_doc_id(self, pdf_path: str) -> str:
"""Get the document ID. This function can be used to check if the
doc_id already exists in the knowledge base."""
return hashlib.sha256(pdf_path.encode("utf-8")).hexdigest()
---- _reader_base.py ----
# -*- coding: utf-8 -*-
"""The reader base class for retrieval-augmented generation (RAG)."""
from abc import abstractmethod
from typing import Any
from .._document import Document
class ReaderBase:
"""The reader base class, which is responsible for reading the original
data, splitting it into chunks, and converting each chunk into a `Document`
object."""
@abstractmethod
async def __call__(self, *args: Any, **kwargs: Any) -> list[Document]:
"""The async call function that takes the input files and returns the
vector records"""
@abstractmethod
def get_doc_id(self, *args: Any, **kwargs: Any) -> str:
"""Get a unique document ID for the input data. This method is to
expose the document ID generation logic to the developers
Returns:
`str`:
A unique document ID for the input data.
"""
---- _text_reader.py ----
# -*- coding: utf-8 -*-
"""The text reader that reads text into vector records."""
import hashlib
import os
from typing import Literal
from ._reader_base import ReaderBase, Document
from .._document import DocMetadata
from ..._logging import logger
from ...message import TextBlock
class TextReader(ReaderBase):
"""The text reader that splits text into chunks by a fixed chunk size
and chunk overlap."""
def __init__(
self,
chunk_size: int = 512,
split_by: Literal["char", "sentence", "paragraph"] = "sentence",
) -> None:
"""Initialize the text reader.
Args:
chunk_size (`int`, default to 512):
The size of each chunk, in number of characters.
split_by (`Literal["char", "paragraph"]`, default to \
"sentence"):
The unit to split the text, can be "char", "sentence", or
"paragraph". Note that "sentence" is implemented by "nltk"
library, which only supports English text.
"""
if chunk_size <= 0:
raise ValueError(
f"The chunk_size must be positive, got {chunk_size}",
)
if split_by not in ["char", "sentence", "paragraph"]:
raise ValueError(
"The split_by must be one of 'char', 'sentence' or "
f"'paragraph', got {split_by}",
)
self.chunk_size = chunk_size
self.split_by = split_by
async def __call__(
self,
text: str,
) -> list[Document]:
"""Read a text string, split it into chunks, and return a list of
Document objects.
Args:
text (`str`):
The input text string, or a path to the local text file.
Returns:
`list[Document]`:
A list of Document objects, where the metadata contains the
chunked text, doc id and chunk id.
"""
if os.path.exists(text) and os.path.isfile(text):
logger.info("Reading text from local file: %s", text)
with open(text, "r", encoding="utf-8") as file:
text = file.read()
logger.info(
"Reading text with chunk_size=%d, split_by=%s",
self.chunk_size,
self.split_by,
)
splits = []
if self.split_by == "char":
# Split by character
for i in range(0, len(text), self.chunk_size):
start = max(0, i)
end = min(i + self.chunk_size, len(text))
splits.append(text[start:end])
elif self.split_by == "sentence":
try:
import nltk
nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)
except ImportError as e:
raise ImportError(
"nltk is not installed. Please install it with "
"`pip install nltk`.",
) from e
sentences = nltk.sent_tokenize(text)
# Handle the chunk_size for sentences
processed_sentences = []
for _ in sentences:
if len(_) <= self.chunk_size:
processed_sentences.append(_)
else:
# If the sentence itself exceeds chunk size, we need to
# truncate it
chunks = [
_[j : j + self.chunk_size]
for j in range(0, len(_), self.chunk_size)
]
processed_sentences.extend(chunks)
splits.extend(processed_sentences)
elif self.split_by == "paragraph":
paragraphs = [_ for _ in text.split("\n") if len(_)]
for para in paragraphs:
if len(para) <= self.chunk_size:
splits.append(para)
else:
# If the paragraph itself exceeds chunk size, we need to
# truncate it
chunks = [
para[k : k + self.chunk_size]
for k in range(0, len(para), self.chunk_size)
]
splits.extend(chunks)
logger.info(
"Finished splitting the text into %d chunks.",
len(splits),
)
doc_id = self.get_doc_id(text)
return [
Document(
id=doc_id,
metadata=DocMetadata(
content=TextBlock(type="text", text=_),
doc_id=doc_id,
chunk_id=idx,
total_chunks=len(splits),
),
)
for idx, _ in enumerate(splits)
]
def get_doc_id(self, text: str) -> str:
"""Get the document ID. This function can be used to check if the
doc_id already exists in the knowledge base."""
return hashlib.sha256(text.encode("utf-8")).hexdigest()
==== _store ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The vector database store abstraction in AgentScope RAG module."""
from ._store_base import (
VDBStoreBase,
)
from ._qdrant_store import QdrantStore
from ._milvuslite_store import MilvusLiteStore
__all__ = [
"VDBStoreBase",
"QdrantStore",
"MilvusLiteStore",
]
---- _milvuslite_store.py ----
# -*- coding: utf-8 -*-
"""The Milvus Lite vector store implementation."""
import json
from typing import Any, Literal, TYPE_CHECKING
from .._reader import Document
from ._store_base import VDBStoreBase
from .._document import DocMetadata
from ..._utils._common import _map_text_to_uuid
from ...types import Embedding
if TYPE_CHECKING:
from pymilvus import MilvusClient
else:
MilvusClient = "pymilvus.MilvusClient"
class MilvusLiteStore(VDBStoreBase):
"""The Milvus Lite vector store implementation, supporting both local and
remote Milvus instances.
.. note:: In Milvus Lite, we use the scalar fields to store the metadata,
including the document ID, chunk ID, and original content. The new
MilvusClient API is used for simplified operations.
.. note:: Milvus Lite is not supported on Windows OS for now (2025-10-21).
"""
def __init__(
self,
uri: str,
collection_name: str,
dimensions: int,
distance: Literal["COSINE", "L2", "IP"] = "COSINE",
token: str = "",
client_kwargs: dict[str, Any] | None = None,
collection_kwargs: dict[str, Any] | None = None,
) -> None:
"""Initialize the Milvus Lite vector store.
Args:
uri (`str`):
The URI of the Milvus instance. For Milvus Lite, use a local
file path like "./milvus_demo.db". For remote Milvus server,
use URI like "http://localhost:19530".
collection_name (`str`):
The name of the collection to store the embeddings.
dimensions (`int`):
The dimension of the embeddings.
distance (`Literal["COSINE", "L2", "IP"]`, default to "COSINE"):
The distance metric to use for the collection. Can be one of
"COSINE", "L2", or "IP". Defaults to "COSINE".
token (`str`, defaults to ""):
The token for authentication when connecting to remote Milvus.
Format: "username:password". Not needed for Milvus Lite.
client_kwargs (`dict[str, Any] | None`, optional):
Other keyword arguments for the Milvus client.
collection_kwargs (`dict[str, Any] | None`, optional):
Other keyword arguments for creating the collection.
"""
try:
from pymilvus import MilvusClient
except ImportError as e:
raise ImportError(
"Milvus client is not installed. Please install it with "
"`pip install pymilvus[milvus_lite]`.",
) from e
client_kwargs = client_kwargs or {}
# Initialize MilvusClient with uri and optional token
init_params = {"uri": uri, **client_kwargs}
if token:
init_params["token"] = token
self._client = MilvusClient(**init_params)
self.collection_name = collection_name
self.dimensions = dimensions
self.distance = distance
self.collection_kwargs = collection_kwargs or {}
async def _validate_collection(self) -> None:
"""Validate the collection exists, if not, create it."""
if not self._client.has_collection(self.collection_name):
# Create collection with the new MilvusClient API
# By default, it creates an auto-incrementing integer ID field
kwargs = {
"collection_name": self.collection_name,
"dimension": self.dimensions,
"metric_type": self.distance,
**self.collection_kwargs,
}
self._client.create_collection(**kwargs)
async def add(self, documents: list[Document], **kwargs: Any) -> None:
"""Add embeddings to the Milvus vector store.
Args:
documents (`list[Document]`):
A list of embedding records to be recorded in the Milvus store.
**kwargs (`Any`):
Additional arguments for the insert operation.
"""
await self._validate_collection()
# Prepare data for insertion using the new MilvusClient API
data = []
for doc in documents:
# Generate a unique integer ID based on hash
unique_string = json.dumps(
{
"doc_id": doc.metadata.doc_id,
"chunk_id": doc.metadata.chunk_id,
"content": doc.metadata.content,
},
ensure_ascii=False,
)
id_type = self.collection_kwargs.get("id_type", "int")
if id_type == "string":
unique_id = _map_text_to_uuid(unique_string)[:6]
else:
unique_id = abs(hash(unique_string)) % (10**10)
# Prepare data entry with vector and metadata
entry = {
# Fixed fields for Milvus
"id": unique_id,
"vector": doc.embedding,
# fields that will be returned in the "entity" field during
# search
"doc_id": doc.metadata.doc_id,
"chunk_id": doc.metadata.chunk_id,
"content": doc.metadata.content,
"total_chunks": doc.metadata.total_chunks,
}
data.append(entry)
# Insert data using MilvusClient
self._client.insert(
collection_name=self.collection_name,
data=data,
)
async def search(
self,
query_embedding: Embedding,
limit: int,
score_threshold: float | None = None,
**kwargs: Any,
) -> list[Document]:
"""Search relevant documents from the Milvus vector store.
Args:
query_embedding (`Embedding`):
The embedding of the query text.
limit (`int`):
The number of relevant documents to retrieve.
score_threshold (`float | None`, optional):
The threshold of the score to filter the results.
**kwargs (`Any`):
Additional arguments for the Milvus client search API.
- filter (`str`): Expression to filter the search results.
- output_fields (`list[str]`): Fields to include in results.
"""
# Get output fields if specified
if "output_fields" not in kwargs:
kwargs["output_fields"] = [
"doc_id",
"chunk_id",
"content",
"total_chunks",
]
# Execute search using MilvusClient
results = self._client.search(
collection_name=self.collection_name,
data=[query_embedding],
limit=limit,
**kwargs,
)
# Process results
collected_res = []
for hits in results:
for hit in hits:
# Check score threshold
if (
score_threshold is not None
and hit["distance"] < score_threshold
):
continue
# Get metadata from entity
entity = hit["entity"]
doc_metadata = DocMetadata(
content=entity.get("content", ""),
doc_id=entity.get("doc_id", ""),
chunk_id=entity.get("chunk_id", 0),
total_chunks=entity.get("total_chunks", 0),
)
# Create Document
collected_res.append(
Document(
embedding=None, # Vector not returned by default
score=hit["distance"],
metadata=doc_metadata,
),
)
return collected_res
async def delete(
self,
ids: list[str] | None = None,
filter: str | None = None, # pylint: disable=redefined-builtin
**kwargs: Any,
) -> None:
"""Delete documents from the Milvus vector store.
Args:
ids (`list[str] | None`, optional):
List of entity IDs to delete.
filter (`str | None`, optional):
Expression to filter documents to delete.
**kwargs (`Any`):
Additional arguments for the delete operation.
"""
if ids is None and filter is None:
raise ValueError(
"Either ids or filter_expr must be provided for deletion.",
)
# Delete data using MilvusClient
self._client.delete(
collection_name=self.collection_name,
ids=ids,
filter=filter,
)
def get_client(self) -> MilvusClient:
"""Get the underlying Milvus client, so that developers can access
the full functionality of Milvus.
Returns:
`MilvusClient`:
The underlying Milvus client.
"""
return self._client
---- _qdrant_store.py ----
# -*- coding: utf-8 -*-
"""The Qdrant local vector store implementation."""
import json
from typing import Any, Literal, TYPE_CHECKING
from .._reader import Document
from ._store_base import VDBStoreBase
from .._document import DocMetadata
from ..._utils._common import _map_text_to_uuid
from ...types import Embedding
if TYPE_CHECKING:
from qdrant_client import AsyncQdrantClient
else:
AsyncQdrantClient = "qdrant_client.AsyncQdrantClient"
class QdrantStore(VDBStoreBase):
"""The Qdrant vector store implementation, supporting both local and
remote Qdrant instances.
.. note:: In Qdrant, we use the ``payload`` field to store the metadata,
including the document ID, chunk ID, and original content.
"""
def __init__(
self,
location: Literal[":memory:"] | str,
collection_name: str,
dimensions: int,
distance: Literal["Cosine", "Euclid", "Dot", "Manhattan"] = "Cosine",
client_kwargs: dict[str, Any] | None = None,
collection_kwargs: dict[str, Any] | None = None,
) -> None:
"""Initialize the local Qdrant vector store.
Args:
location (`Literal[":memory:"] | str`):
The location of the Qdrant instance. Use ":memory:" for
in-memory Qdrant instance, or url for remote Qdrant instance,
e.g. "http://localhost:6333" or a path to a directory.
collection_name (`str`):
The name of the collection to store the embeddings.
dimensions (`int`):
The dimension of the embeddings.
distance (`Literal["Cosine", "Euclid", "Dot", "Manhattan"]`, \
default to "Cosine"):
The distance metric to use for the collection. Can be one of
"Cosine", "Euclid", "Dot", or "Manhattan". Defaults to
"Cosine".
client_kwargs (`dict[str, Any] | None`, optional):
Other keyword arguments for the Qdrant client.
collection_kwargs (`dict[str, Any] | None`, optional):
Other keyword arguments for creating the collection.
"""
try:
from qdrant_client import AsyncQdrantClient
except ImportError as e:
raise ImportError(
"Qdrant client is not installed. Please install it with "
"`pip install qdrant-client`.",
) from e
client_kwargs = client_kwargs or {}
self._client = AsyncQdrantClient(location=location, **client_kwargs)
self.collection_name = collection_name
self.dimensions = dimensions
self.distance = distance
self.collection_kwargs = collection_kwargs or {}
async def _validate_collection(self) -> None:
"""Validate the collection exists, if not, create it."""
if not await self._client.collection_exists(self.collection_name):
from qdrant_client import models
collections_kwargs = {
"collection_name": self.collection_name,
"vectors_config": models.VectorParams(
size=self.dimensions,
distance=getattr(models.Distance, self.distance.upper()),
),
**self.collection_kwargs,
}
await self._client.create_collection(**collections_kwargs)
async def add(self, documents: list[Document], **kwargs: Any) -> None:
"""Add embeddings to the Qdrant vector store.
Args:
documents (`list[Document]`):
A list of embedding records to be recorded in the Qdrant store.
"""
await self._validate_collection()
from qdrant_client.models import PointStruct
await self._client.upsert(
collection_name=self.collection_name,
points=[
PointStruct(
id=_map_text_to_uuid(
json.dumps(
{
"doc_id": _.metadata.doc_id,
"chunk_id": _.metadata.chunk_id,
"content": _.metadata.content,
},
ensure_ascii=False,
),
),
vector=_.embedding,
payload=_.metadata,
)
for _ in documents
],
)
async def search(
self,
query_embedding: Embedding,
limit: int,
score_threshold: float | None = None,
**kwargs: Any,
) -> list[Document]:
"""Search relevant documents from the Qdrant vector store.
Args:
query_embedding (`Embedding`):
The embedding of the query text.
limit (`int`):
The number of relevant documents to retrieve.
score_threshold (`float | None`, optional):
The threshold of the score to filter the results.
**kwargs (`Any`):
Other keyword arguments for the Qdrant client search API.
"""
res = await self._client.query_points(
collection_name=self.collection_name,
query=query_embedding,
limit=limit,
score_threshold=score_threshold,
**kwargs,
)
collected_res = []
for point in res.points:
collected_res.append(
Document(
embedding=point.vector,
score=point.score,
metadata=DocMetadata(**point.payload),
),
)
return collected_res
async def delete(self, *args: Any, **kwargs: Any) -> None:
"""Delete is not implemented for QdrantStore."""
raise NotImplementedError(
"Delete is not implemented for QdrantStore.",
)
def get_client(self) -> AsyncQdrantClient:
"""Get the underlying Qdrant client, so that developers can access
the full functionality of Qdrant.
Returns:
`AsyncQdrantClient`:
The underlying Qdrant client.
"""
return self._client
---- _store_base.py ----
# -*- coding: utf-8 -*-
"""The embedding store base class."""
from abc import abstractmethod
from typing import Any
from .. import Document
from ...types import Embedding
class VDBStoreBase:
"""The vector database store base class, serving as a middle layer between
the knowledge base and the actual vector database implementation."""
@abstractmethod
async def add(self, documents: list[Document], **kwargs: Any) -> None:
"""Record the documents into the vector database."""
@abstractmethod
async def delete(self, *args: Any, **kwargs: Any) -> None:
"""Delete texts from the embedding store."""
@abstractmethod
async def search(
self,
query_embedding: Embedding,
limit: int,
score_threshold: float | None = None,
**kwargs: Any,
) -> list[Document]:
"""Retrieve relevant texts for the given queries.
Args:
query_embedding (`Embedding`):
The embedding of the query text.
limit (`int`):
The number of relevant documents to retrieve.
score_threshold (`float | None`, optional):
The threshold of the score to filter the results.
**kwargs (`Any`):
Other keyword arguments for the vector database search API.
"""
def get_client(self) -> Any:
"""Get the underlying vector database client, so that developers can
access the full functionality of the vector database."""
raise NotImplementedError(
"``get_client`` is not implemented for "
f"{self.__class__.__name__}.",
)
==== session ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The session module in agentscope."""
from ._session_base import SessionBase
from ._json_session import JSONSession
__all__ = [
"SessionBase",
"JSONSession",
]
---- _json_session.py ----
# -*- coding: utf-8 -*-
"""The JSON session class."""
import json
import os
from ._session_base import SessionBase
from .._logging import logger
from ..module import StateModule
class JSONSession(SessionBase):
"""The JSON session class."""
def __init__(
self,
session_id: str | None = None,
save_dir: str = "./",
) -> None:
"""Initialize the JSON session class.
Args:
session_id (`str`):
The session id, deprecated and move to the `save_session_state`
and `load_session_state` methods to support different session
ids.
save_dir (`str`, defaults to `"./"):
The directory to save the session state.
"""
self.save_dir = save_dir
if session_id is not None:
logger.warning(
"The `session_id` argument in the JSONSession constructor is "
"deprecated and will be removed in future versions. Please "
"pass the `session_id` to the `save_session_state` and "
"`load_session_state` methods instead.",
)
def _get_save_path(self, session_id: str) -> str:
"""The path to save the session state.
Args:
session_id (`str`):
The session id.
Returns:
`str`:
The path to save the session state.
"""
os.makedirs(self.save_dir, exist_ok=True)
return os.path.join(self.save_dir, f"{session_id}.json")
async def save_session_state(
self,
session_id: str,
**state_modules_mapping: StateModule,
) -> None:
"""Load the state dictionary from a JSON file.
Args:
session_id (`str`):
The session id.
**state_modules_mapping (`dict[str, StateModule]`):
A dictionary mapping of state module names to their instances.
"""
state_dicts = {
name: state_module.state_dict()
for name, state_module in state_modules_mapping.items()
}
with open(
self._get_save_path(session_id),
"w",
encoding="utf-8",
) as file:
json.dump(state_dicts, file, ensure_ascii=False)
async def load_session_state(
self,
session_id: str,
allow_not_exist: bool = True,
**state_modules_mapping: StateModule,
) -> None:
"""Get the state dictionary to be saved to a JSON file.
Args:
session_id (`str`):
The session id.
allow_not_exist (`bool`, defaults to `True`):
Whether to allow the session to not exist. If `False`, raises
an error if the session does not exist.
state_modules_mapping (`list[StateModule]`):
The list of state modules to be loaded.
"""
session_save_path = self._get_save_path(session_id)
if os.path.exists(session_save_path):
with open(session_save_path, "r", encoding="utf-8") as file:
states = json.load(file)
for name, state_module in state_modules_mapping.items():
if name in states:
state_module.load_state_dict(states[name])
logger.info(
"Load session state from %s successfully.",
session_save_path,
)
elif allow_not_exist:
logger.info(
"Session file %s does not exist. Skip loading session state.",
session_save_path,
)
else:
raise ValueError(
f"Failed to load session state for file {session_save_path} "
"does not exist.",
)
---- _session_base.py ----
# -*- coding: utf-8 -*-
"""The session base class in agentscope."""
from abc import abstractmethod
from ..module import StateModule
class SessionBase:
"""The base class for session in agentscope."""
@abstractmethod
async def save_session_state(
self,
session_id: str,
**state_modules_mapping: StateModule,
) -> None:
"""Save the session state
Args:
session_id (`str`):
The session id.
**state_modules_mapping (`dict[str, StateModule]`):
A dictionary mapping of state module names to their instances.
"""
@abstractmethod
async def load_session_state(
self,
session_id: str,
allow_not_exist: bool = True,
**state_modules_mapping: StateModule,
) -> None:
"""Load the session state"""
==== token ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The token module in agentscope"""
from ._token_base import TokenCounterBase
from ._gemini_token_counter import GeminiTokenCounter
from ._openai_token_counter import OpenAITokenCounter
from ._anthropic_token_counter import AnthropicTokenCounter
from ._huggingface_token_counter import HuggingFaceTokenCounter
__all__ = [
"TokenCounterBase",
"GeminiTokenCounter",
"OpenAITokenCounter",
"AnthropicTokenCounter",
"HuggingFaceTokenCounter",
]
---- _anthropic_token_counter.py ----
# -*- coding: utf-8 -*-
"""The Anthropic token counter class."""
from typing import Any
class AnthropicTokenCounter:
"""The Anthropic token counter class."""
def __init__(self, model_name: str, api_key: str, **kwargs: Any) -> None:
"""Initialize the Anthropic token counter.
Args:
model_name (`str`):
The name of the Anthropic model to use, e.g. "claude-2".
api_key (`str`):
The API key for Anthropic.
"""
import anthropic
self.client = anthropic.AsyncAnthropic(api_key=api_key, **kwargs)
self.model_name = model_name
async def count(
self,
messages: list[dict],
tools: list[dict] | None = None,
**kwargs: Any,
) -> int:
"""Count the number of tokens for the given messages
.. note:: The Anthropic token counting API requires the multimodal
data to be in base64 format,
Args:
messages (`list[dict]`):
A list of dictionaries, where `role` and `content` fields are
required.
tools (`list[dict] | None`, defaults to `None`):
The tools JSON schemas that the model can use.
**kwargs (`Any`):
Additional keyword arguments for the token counting API.
"""
system_message = None
if messages[0].get("role") == "system":
system_message = messages.pop(0)
extra_kwargs: dict = {
"model": self.model_name,
"messages": messages,
**kwargs,
}
if tools:
extra_kwargs["tools"] = tools
if system_message:
extra_kwargs["system"] = system_message
res = await self.client.messages.count_tokens(**extra_kwargs)
return res.input_tokens
---- _gemini_token_counter.py ----
# -*- coding: utf-8 -*-
"""The gemini token counter class in agentscope."""
from typing import Any
from agentscope.token._token_base import TokenCounterBase
class GeminiTokenCounter(TokenCounterBase):
"""The Gemini token counter class."""
def __init__(self, model_name: str, api_key: str, **kwargs: Any) -> None:
"""Initialize the Gemini token counter.
Args:
model_name (`str`):
The name of the Gemini model to use, e.g. "gemini-2.5-flash".
api_key (`str`):
The API key for Google Gemini.
**kwargs:
Additional keyword arguments that will be passed to the
Gemini client.
"""
from google import genai
self.client = genai.Client(
api_key=api_key,
**kwargs,
)
self.model_name = model_name
async def count(
self,
messages: list[dict],
tools: list[dict] | None = None,
**config_kwargs: Any,
) -> int:
"""Count the number of tokens of gemini models."""
kwargs = {
"model": self.model_name,
"contents": messages,
"config": {
"tools": tools,
**config_kwargs,
},
}
res = self.client.models.count_tokens(**kwargs)
return res.total_tokens
---- _huggingface_token_counter.py ----
# -*- coding: utf-8 -*-
"""The huggingface token counter class."""
import os
from typing import Any
from agentscope.token._token_base import TokenCounterBase
class HuggingFaceTokenCounter(TokenCounterBase):
"""The token counter for Huggingface models."""
def __init__(
self,
pretrained_model_name_or_path: str,
use_mirror: bool = False,
use_fast: bool = False,
trust_remote_code: bool = False,
**kwargs: Any,
) -> None:
"""Initialize the huggingface token counter.
Args:
pretrained_model_name_or_path (`str`):
The name or path of the pretrained model, which will be used
to download the tokenizer from Huggingface Hub.
use_mirror (`bool`, defaults to `False`):
Whether to enable the HuggingFace mirror, which is useful for
users in China.
use_fast (`bool`, defaults to `False`):
The argument that will be passed to the tokenizer.
trust_remote_code (`bool`, defaults to `False`):
The argument that will be passed to the tokenizer.
**kwargs:
Additional keyword arguments that will be passed to the
tokenizer.
"""
if use_mirror:
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from transformers import AutoTokenizer
self.tokenizer = AutoTokenizer.from_pretrained(
pretrained_model_name_or_path,
use_fast=use_fast,
trust_remote_code=trust_remote_code,
**kwargs,
)
if self.tokenizer.chat_template is None:
raise ValueError(
f"The tokenizer for model {pretrained_model_name_or_path} in "
f"transformers does not have chat template.",
)
async def count(
self,
messages: list[dict],
tools: list[dict] | None = None,
**kwargs: Any,
) -> int:
"""Count the number of tokens with the tokenizer download from
HuggingFace hub.
Args:
messages (`list[dict]`):
A list of message dictionaries
tools (`list[dict] | None`, defaults to `None`):
The JSON schema of the tools, which will also be involved in
the token counting.
**kwargs (`Any`):
The additional keyword arguments that will be passed to the
tokenizer, e.g. `chat_template`, `padding`, etc.
"""
tokenized_msgs = self.tokenizer.apply_chat_template(
messages,
add_generation_prompt=False,
tokenize=True,
return_tensors="np",
tools=tools,
**kwargs,
)[0]
return len(tokenized_msgs)
---- _openai_token_counter.py ----
# -*- coding: utf-8 -*-
"""The OpenAI token counting class. The token calculation of vision models
follows
https://platform.openai.com/docs/guides/images-vision?api-mode=chat#calculating-costs
"""
import base64
import io
import json
import math
from http import HTTPStatus
from typing import Any
import requests
from ._token_base import TokenCounterBase
def _calculate_tokens_for_high_quality_image(
base_tokens: int,
tile_tokens: int,
width: int,
height: int,
) -> int:
"""Calculate the number of tokens for a high-quality image, which follows
https://platform.openai.com/docs/guides/images-vision?api-mode=chat#calculating-costs
"""
# Step1: scale to fit within a 2048x2048 box
if width > 2048 or height > 2048:
ratio = min(2048 / width, 2048 / height)
width = int(width * ratio)
height = int(height * ratio)
# Step2: Scale to make the shortest side 768 pixels
shortest_side = min(width, height)
if shortest_side != 768:
ratio = 768 / shortest_side
width = int(width * ratio)
height = int(height * ratio)
# Step3: Calculate how many 512px tiles are needed
tiles_width = (width + 511) // 512
tiles_height = (height + 511) // 512
total_tiles = tiles_width * tiles_height
# Step4: Calculate the total tokens
total_tokens = (total_tiles * tile_tokens) + base_tokens
return total_tokens
def _get_size_of_image_url(url: str) -> tuple[int, int]:
"""Get the size of an image from the given URL.
Args:
url (`str`):
A web URL or base64 encoded image URL.
Returns:
`tuple[int, int]`:
A tuple containing the width and height of the image.
"""
if url.startswith("data:image/"):
base64_data = url.split("base64,")[1]
image_data = base64.b64decode(base64_data)
else:
response = None
for _ in range(3):
response = requests.get(url)
if response.status_code == HTTPStatus.OK:
break
response.raise_for_status()
image_data = response.content
from PIL import Image
image = Image.open(io.BytesIO(image_data))
width, height = image.size
return width, height
def _get_base_and_tile_tokens(model_name: str) -> tuple[int, int]:
"""Get the base and tile tokens for the given OpenAI model.
Args:
model_name (`str`):
The name of the model.
Returns:
`tuple[int, int]`:
A tuple containing the base tokens and tile tokens.
"""
if any(
model_name.startswith(_)
for _ in [
"gpt-4o",
"gpt-4.1",
"gpt-4.5",
]
):
return 85, 170
if any(
model_name.startswith(_)
for _ in [
"o1",
"o1-pro",
"o3",
]
):
return 75, 150
if model_name.startswith("4o-mini"):
return 2833, 5667
raise ValueError(
f"Unsupported OpenAI model {model_name} for token counting. ",
)
def _calculate_tokens_for_tools(
model_name: str,
tools: list[dict],
encoding: Any,
) -> int:
"""Calculate the tokens for the given tools JSON schema, which follows the
OpenAI cookbook
https://github.com/openai/openai-cookbook/blob/6dfb7920b59a45291f7df4ea41338d1faf9ef1e8/examples/How_to_count_tokens_with_tiktoken.ipynb
"""
if not tools:
return 0
func_init = 10
prop_init = 3
prop_key = 3
enum_init = -3
enum_item = 3
func_end = 12
if model_name.startswith("gpt-4o"):
func_init = 7
func_token_count = 0
for f in tools:
func_token_count += func_init
function = f["function"]
f_name = function["name"]
f_desc = function.get("description", "").removesuffix(".")
func_token_count += len(encoding.encode(f"{f_name}:{f_desc}"))
properties = function["parameters"]["properties"]
if len(properties) > 0:
func_token_count += prop_init
for key in properties.keys():
func_token_count += prop_key
p_name = key
p_type = properties[key]["type"]
p_desc = (
properties[key].get("description", "").removesuffix(".")
)
if "enum" in properties[key].keys():
func_token_count += enum_init
for item in properties[key]["enum"]:
func_token_count += enum_item
func_token_count += len(encoding.encode(item))
func_token_count += len(
encoding.encode(f"{p_name}:{p_type}:{p_desc}"),
)
func_token_count += func_end
return func_token_count
def _count_content_tokens_for_openai_vision_model(
model_name: str,
content: list[dict],
encoding: Any,
) -> int:
"""Yield the number of tokens for the content of an OpenAI vision model.
Implemented according to https://platform.openai.com/docs/guides/vision.
Args:
model_name (`str`):
The name of the model.
content (`list[dict]`):
A list of dictionaries.
encoding (`Any`):
The encoding object.
Example:
.. code-block:: python
_yield_tokens_for_openai_vision_model(
[
{
"type": "text",
"text": "xxx",
},
{
"type": "image_url",
"image_url": {
"url": "xxx",
"detail": "auto",
}
},
# ...
]
)
Returns:
`Generator[int, None, None]`: Generate the number of tokens in a
generator.
"""
num_tokens = 0
for item in content:
assert isinstance(item, dict), (
"The content field should be a list of dictionaries, but got "
f"{type(item)}."
)
typ = item.get("type", None)
if typ == "text":
num_tokens += len(
encoding.encode(item["text"]),
)
elif typ == "image_url":
width, height = _get_size_of_image_url(item["image_url"]["url"])
# Different counting logic for different models
if any(
model_name.startswith(_)
for _ in [
"gpt-4.1-mini",
"gpt-4.1-nano",
"o4-mini",
]
):
patches = min(
math.ceil(width / 32) * math.ceil(height / 32),
1536,
)
if model_name.startswith("gpt-4.1-mini"):
num_tokens += math.ceil(patches * 1.62)
elif model_name.startswith("gpt-4.1-nano"):
num_tokens += math.ceil(patches * 2.46)
else:
num_tokens += math.ceil(patches * 1.72)
elif any(
model_name.startswith(_)
for _ in [
"gpt-4o",
"gpt-4.1",
"gpt-4o-mini",
"o",
]
):
base_tokens, tile_tokens = _get_base_and_tile_tokens(
model_name,
)
# By default, we use high here to avoid undercounting tokens
detail = item.get("image_url").get("detail", "high")
if detail == "low":
num_tokens += base_tokens
elif detail in ["auto", "high"]:
num_tokens += _calculate_tokens_for_high_quality_image(
base_tokens,
tile_tokens,
width,
height,
)
else:
raise ValueError(
f"Unsupported image detail {detail}, expected "
f"one of ['low', 'auto', 'high'].",
)
else:
raise ValueError(
"The type field currently only supports 'text' "
f"and 'image_url', but got {typ}.",
)
return num_tokens
class OpenAITokenCounter(TokenCounterBase):
"""The OpenAI token counting class."""
def __init__(self, model_name: str) -> None:
"""Initialize the OpenAI token counter.
Args:
model_name (`str`):
The name of the OpenAI model to use for token counting.
"""
self.model_name = model_name
async def count(
self,
messages: list[dict[str, Any]],
tools: list[dict] = None,
**kwargs: Any,
) -> int:
"""Count the token numbers of the given messages.
.. note:: OpenAI hasn't provided an official guide for counting tokens
with tools. If you have any ideas, please open an issue on
our GitHub repository.
Args:
messages (`list[dict[str, Any]]`):
A list of dictionaries, where `role` and `content` fields are
required.
tools (`list[dict]`, defaults to `None`):
"""
import tiktoken
try:
encoding = tiktoken.encoding_for_model(self.model_name)
except KeyError:
encoding = tiktoken.get_encoding("o200k_base")
tokens_per_message = 3
tokens_per_name = 1
# every reply is primed with <|start|>assistant<|message|>
num_tokens = 3
for message in messages:
num_tokens += tokens_per_message
for key, value in message.items():
# Considering vision models
if key == "content" and isinstance(value, list):
num_tokens += (
_count_content_tokens_for_openai_vision_model(
self.model_name,
value,
encoding,
)
)
elif isinstance(value, str):
num_tokens += len(encoding.encode(value))
elif value is None:
continue
elif key == "tool_calls":
# TODO: This is only a temporary solution, since OpenAI
# hasn't provided an official guide for counting tokens
# with tool results.
num_tokens += len(
encoding.encode(
json.dumps(value, ensure_ascii=False),
),
)
else:
raise TypeError(
f"Invalid type {type(value)} in the {key} field: "
f"{value}",
)
if key == "name":
num_tokens += tokens_per_name
if tools:
num_tokens += _calculate_tokens_for_tools(
self.model_name,
tools,
encoding,
)
return num_tokens
---- _token_base.py ----
# -*- coding: utf-8 -*-
"""The token base class in agentscope."""
from abc import abstractmethod
from typing import Any
class TokenCounterBase:
"""The base class for token counting."""
@abstractmethod
async def count(
self,
messages: list[dict],
**kwargs: Any,
) -> int:
"""Count the number of tokens by the given model and messages."""
==== tool ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The tool module in agentscope."""
from ._response import ToolResponse
from ._coding import (
execute_python_code,
execute_shell_command,
)
from ._text_file import (
view_text_file,
write_text_file,
insert_text_file,
)
from ._multi_modality import (
dashscope_text_to_image,
dashscope_text_to_audio,
dashscope_image_to_text,
openai_text_to_image,
openai_text_to_audio,
openai_edit_image,
openai_create_image_variation,
openai_image_to_text,
openai_audio_to_text,
)
from ._toolkit import Toolkit
__all__ = [
"Toolkit",
"ToolResponse",
"execute_python_code",
"execute_shell_command",
"view_text_file",
"write_text_file",
"insert_text_file",
"dashscope_text_to_image",
"dashscope_text_to_audio",
"dashscope_image_to_text",
"openai_text_to_image",
"openai_text_to_audio",
"openai_edit_image",
"openai_create_image_variation",
"openai_image_to_text",
"openai_audio_to_text",
]
---- _async_wrapper.py ----
# -*- coding: utf-8 -*-
"""The functions that wrap object, sync generator, and async generator
into async generators.
TODO: handle the exception raised when yielding from async generator
into a normal ToolResponse instance.
"""
import asyncio
from typing import AsyncGenerator, Generator, Callable
from ._response import ToolResponse
from ..message import TextBlock
async def _postprocess_tool_response(
tool_response: ToolResponse,
postprocess_func: Callable[[ToolResponse], ToolResponse | None] | None,
) -> ToolResponse:
"""Post-process a ToolResponse object with the given function."""
if postprocess_func:
processed_response = postprocess_func(tool_response)
if processed_response:
return processed_response
return tool_response
async def _object_wrapper(
obj: ToolResponse,
postprocess_func: Callable[[ToolResponse], ToolResponse | None] | None,
) -> AsyncGenerator[ToolResponse, None]:
"""Wrap a ToolResponse object to an async generator."""
yield await _postprocess_tool_response(obj, postprocess_func)
async def _sync_generator_wrapper(
sync_generator: Generator[ToolResponse, None, None],
postprocess_func: Callable[[ToolResponse], ToolResponse | None] | None,
) -> AsyncGenerator[ToolResponse, None]:
"""Wrap a sync generator to an async generator."""
for chunk in sync_generator:
yield await _postprocess_tool_response(chunk, postprocess_func)
async def _async_generator_wrapper(
async_func: AsyncGenerator[ToolResponse, None],
postprocess_func: Callable[[ToolResponse], ToolResponse | None] | None,
) -> AsyncGenerator[ToolResponse, None]:
"""When the function is interrupted during generating the tool
response, add an interrupted message to the response, and postpone
the CancelledError to the caller."""
last_chunk = None
try:
async for chunk in async_func:
processed_chunk = await _postprocess_tool_response(
chunk,
postprocess_func,
)
yield processed_chunk
last_chunk = processed_chunk
except asyncio.CancelledError:
interrupted_info = TextBlock(
type="text",
text="<system-info>"
"The tool call has been interrupted by the user."
"</system-info>",
)
if last_chunk:
last_chunk.content.append(interrupted_info)
last_chunk.is_interrupted = True
last_chunk.is_last = True
yield await _postprocess_tool_response(
last_chunk,
postprocess_func,
)
else:
yield await _postprocess_tool_response(
ToolResponse(
content=[interrupted_info],
is_interrupted=True,
is_last=True,
),
postprocess_func,
)
---- _registered_tool_function.py ----
# -*- coding: utf-8 -*-
"""The data model for registered tool functions in AgentScope."""
from copy import deepcopy
from dataclasses import field, dataclass
from typing import Callable, Literal, Type
from pydantic import BaseModel
from ._response import ToolResponse
from .._utils._common import _remove_title_field
from ..message import ToolUseBlock
from ..types import ToolFunction, JSONSerializableObject
@dataclass
class RegisteredToolFunction:
"""The registered tool function class."""
name: str
"""The name of the tool function."""
group: str | Literal["basic"]
"""The belonging group of the tool function"""
source: Literal["function", "mcp_server", "function_group"]
""""The type of the tool function, can be `function` or `mcp_server`."""
original_func: ToolFunction
"""The original function"""
json_schema: dict
"""The JSON schema of the tool function, which is used to validate the """
preset_kwargs: dict[str, JSONSerializableObject] = field(
default_factory=dict,
)
"""The preset keyword arguments, which won't be presented in the JSON
schema and exposed to the user."""
extended_model: Type[BaseModel] | None = None
"""The base model used to extend the JSON schema of the original tool
function, so that we can dynamically adjust the tool function."""
mcp_name: str | None = None
"""The name of the MCP, if the tool function comes from an MCP server."""
postprocess_func: Callable[
[ToolUseBlock, ToolResponse],
ToolResponse | None,
] | None = None
"""The post-processing function that will be called after the tool
function is executed, taking the tool call block and tool
response as arguments. If it returns `None`, the tool result will be
returned as is. If it returns a `ToolResponse`, the returned block
will be used as the final tool response."""
@property
def extended_json_schema(self) -> dict:
"""Get the JSON schema of the tool function, if an extended model is
set, the merged JSON schema will be returned."""
if self.extended_model is None:
return self.json_schema
# Merge the extended model with the original JSON schema
extended_schema = self.extended_model.model_json_schema()
merged_schema = deepcopy(self.json_schema)
_remove_title_field( # pylint: disable=protected-access
extended_schema,
)
for key, value in extended_schema["properties"].items():
if key in self.json_schema["function"]["parameters"]["properties"]:
raise ValueError(
f"The field `{key}` already exists in the original "
f"function schema of `{self.name}`. Try to use a "
"different name.",
)
merged_schema["function"]["parameters"]["properties"][key] = value
if key in extended_schema.get("required", []):
if "required" not in merged_schema["function"]["parameters"]:
merged_schema["function"]["parameters"]["required"] = []
merged_schema["function"]["parameters"]["required"].append(key)
return merged_schema
---- _response.py ----
# -*- coding: utf-8 -*-
"""The tool response class."""
from dataclasses import dataclass, field
from typing import Optional, List
from .._utils._common import _get_timestamp
from ..message import AudioBlock, ImageBlock, TextBlock
@dataclass
class ToolResponse:
"""The result chunk of a tool call."""
content: List[TextBlock | ImageBlock | AudioBlock]
"""The execution output of the tool function."""
metadata: Optional[dict] = None
"""The metadata to be accessed within the agent, so that we don't need to
parse the tool result block."""
stream: bool = False
"""Whether the tool output is streamed."""
is_last: bool = True
"""Whether this is the last response in a stream tool execution."""
is_interrupted: bool = False
"""Whether the tool execution is interrupted."""
id: str = field(default_factory=lambda: _get_timestamp(True))
"""The identity of the tool response."""
---- _toolkit.py ----
# -*- coding: utf-8 -*-
"""The toolkit class for tool calls in agentscope."""
import asyncio
import inspect
from copy import deepcopy
from dataclasses import dataclass
from functools import partial
from typing import (
AsyncGenerator,
Literal,
Dict,
Any,
Type,
Generator,
Callable,
)
from pydantic import (
BaseModel,
Field,
create_model,
ConfigDict,
)
from docstring_parser import parse
from ._async_wrapper import (
_async_generator_wrapper,
_object_wrapper,
_sync_generator_wrapper,
)
from ._registered_tool_function import RegisteredToolFunction
from ._response import ToolResponse
from .._utils._common import _remove_title_field
from ..mcp import (
MCPToolFunction,
MCPClientBase,
StatefulClientBase,
)
from ..message import (
ToolUseBlock,
TextBlock,
)
from ..module import StateModule
from ..types import (
JSONSerializableObject,
ToolFunction,
)
from ..tracing._trace import trace_toolkit
from .._logging import logger
@dataclass
class ToolGroup:
"""The tool group class"""
name: str
"""The group name, which will be used in the reset function as the group
identifier."""
active: bool
"""If the tool group is active, meaning the tool functions in this group
is included in the JSON schema"""
description: str
"""The description of the tool group to tell the agent what the tool
group is about."""
notes: str | None = None
"""The using notes of the tool group, to remind the agent how to use"""
class Toolkit(StateModule):
"""The class that supports both function- and group-level tool management.
Use the following methods to manage the tool functions:
- `register_tool_function`
- `remove_tool_function`
For group-level management:
- `create_tool_group`
- `update_tool_groups`
- `remove_tool_groups`
MCP related methods:
- `register_mcp_server`
- `remove_mcp_servers`
To run the tool functions or get the data from the activated tools:
- `call_tool_function`
- `get_json_schemas`
- `get_tool_group_notes`
"""
def __init__(self) -> None:
"""Initialize the toolkit."""
super().__init__()
self.tools: dict[str, RegisteredToolFunction] = {}
self.groups: dict[str, ToolGroup] = {}
def create_tool_group(
self,
group_name: str,
description: str,
active: bool = False,
notes: str | None = None,
) -> None:
"""Create a tool group to organize tool functions
Args:
group_name (`str`):
The name of the tool group.
description (`str`):
The description of the tool group.
active (`bool`, defaults to `False`):
If the group is active, meaning the tool functions in this
group are included in the JSON schema.
notes (`str | None`, optional):
The notes used to remind the agent how to use the tool
functions properly, which can be combined into the system
prompt.
"""
if group_name in self.groups or group_name == "basic":
raise ValueError(
f"Tool group '{group_name}' is already registered in the "
"toolkit.",
)
self.groups[group_name] = ToolGroup(
name=group_name,
description=description,
notes=notes,
active=active,
)
def update_tool_groups(self, group_names: list[str], active: bool) -> None:
"""Update the activation status of the given tool groups.
Args:
group_names (`list[str]`):
The list of tool group names to be updated.
active (`bool`):
If the tool groups should be activated or deactivated.
"""
for group_name in group_names:
if group_name == "basic":
logger.warning(
"The 'basic' tool group is always active, skipping it.",
)
if group_name in self.groups:
self.groups[group_name].active = active
def remove_tool_groups(self, group_names: str | list[str]) -> None:
"""Remove tool functions from the toolkit by their group names.
Args:
group_names (`str | list[str]`):
The group names to be removed from the toolkit.
"""
if isinstance(group_names, str):
group_names = [group_names]
if not isinstance(group_names, list) or not all(
isinstance(_, str) for _ in group_names
):
raise TypeError(
f"The group_names must be a list of strings, "
f"but got {type(group_names)}.",
)
if "basic" in group_names:
raise ValueError(
"Cannot remove the default 'basic' tool group.",
)
for group_name in group_names:
self.groups.pop(group_name, None)
# Remove the tool functions in the given groups
tool_names = deepcopy(list(self.tools.keys()))
for tool_name in tool_names:
if self.tools[tool_name].group in group_names:
self.tools.pop(tool_name)
def register_tool_function( # pylint: disable=too-many-branches
self,
tool_func: ToolFunction,
group_name: str | Literal["basic"] = "basic",
preset_kwargs: dict[str, JSONSerializableObject] | None = None,
func_description: str | None = None,
json_schema: dict | None = None,
include_long_description: bool = True,
include_var_positional: bool = False,
include_var_keyword: bool = False,
postprocess_func: Callable[
[
ToolUseBlock,
ToolResponse,
],
ToolResponse | None,
]
| None = None,
) -> None:
"""Register a tool function to the toolkit.
Args:
tool_func (`ToolFunction`):
The tool function, which can be async or sync, streaming or
not-streaming, but the response must be a `ToolResponse`
object.
group_name (`str | Literal["basic"]`, defaults to `"basic"`):
The belonging group of the tool function. Tools in "basic"
group is always included in the JSON schema, while the others
are only included when their group is active.
preset_kwargs (`dict[str, JSONSerializableObject] | None`, \
optional):
Preset arguments by the user, which will not be included in
the JSON schema, nor exposed to the agent.
func_description (`str | None`, optional):
The function description. If not provided, the description
will be extracted from the docstring automatically.
json_schema (`dict | None`, optional):
Manually provided JSON schema for the tool function, which
should be `{"type": "function", "function": {"name":
"function_name": "xx", "description": "xx",
"parameters": {...}}}`
include_long_description (`bool`, defaults to `True`):
When extracting function description from the docstring, if
the long description will be included.
include_var_positional (`bool`, defaults to `False`):
Whether to include the variable positional arguments (`*args`)
in the function schema.
include_var_keyword (`bool`, defaults to `False`):
Whether to include the variable keyword arguments (`**kwargs`)
in the function schema.
postprocess_func (`Callable[[ToolUseBlock, ToolResponse], \
ToolResponse | None] | None`, optional):
A post-processing function that will be called after the tool
function is executed, taking the tool call block and tool
response as arguments. If it returns `None`, the tool
result will be returned as is. If it returns a
`ToolResponse`, the returned block will be used as the
final tool result.
"""
# Arguments checking
if group_name not in self.groups and group_name != "basic":
raise ValueError(
f"Tool group '{group_name}' not found.",
)
# Check the manually provided JSON schema if provided
if json_schema:
assert (
isinstance(json_schema, dict)
and "type" in json_schema
and json_schema["type"] == "function"
and "function" in json_schema
and isinstance(json_schema["function"], dict)
), "Invalid JSON schema for the tool function."
# Handle MCP tool function and regular function respectively
mcp_name = None
if isinstance(tool_func, MCPToolFunction):
func_name = tool_func.name
original_func = tool_func.__call__
self._validate_tool_function(func_name)
json_schema = json_schema or tool_func.json_schema
mcp_name = tool_func.mcp_name
elif isinstance(tool_func, partial):
# partial function
kwargs = tool_func.keywords
# Turn args into keyword arguments
if tool_func.args:
param_names = list(
inspect.signature(tool_func.func).parameters.keys(),
)
for i, arg in enumerate(tool_func.args):
if i < len(param_names):
kwargs[param_names[i]] = arg
preset_kwargs = {
**kwargs,
**(preset_kwargs or {}),
}
func_name = tool_func.func.__name__
original_func = tool_func.func
self._validate_tool_function(func_name)
json_schema = json_schema or self._parse_tool_function(
tool_func.func,
include_long_description=include_long_description,
include_var_positional=include_var_positional,
include_var_keyword=include_var_keyword,
)
else:
# normal function
func_name = tool_func.__name__
original_func = tool_func
self._validate_tool_function(func_name)
json_schema = json_schema or self._parse_tool_function(
tool_func,
include_long_description=include_long_description,
include_var_positional=include_var_positional,
include_var_keyword=include_var_keyword,
)
# Override the description if provided
if func_description:
json_schema["function"]["description"] = func_description
# Remove the preset kwargs from the JSON schema
for arg_name in preset_kwargs or {}:
if arg_name in json_schema["function"]["parameters"]["properties"]:
json_schema["function"]["parameters"]["properties"].pop(
arg_name,
)
if "required" in json_schema["function"]["parameters"]:
for arg_name in preset_kwargs or {}:
if (
arg_name
in json_schema["function"]["parameters"]["required"]
):
json_schema["function"]["parameters"]["required"].remove(
arg_name,
)
# Remove the required field if it is empty
if len(json_schema["function"]["parameters"]["required"]) == 0:
json_schema["function"]["parameters"].pop("required", None)
func_obj = RegisteredToolFunction(
name=func_name,
group=group_name,
source="function",
original_func=original_func,
json_schema=json_schema,
preset_kwargs=preset_kwargs or {},
extended_model=None,
mcp_name=mcp_name,
postprocess_func=postprocess_func,
)
self.tools[func_name] = func_obj
def remove_tool_function(self, tool_name: str) -> None:
"""Remove tool function from the toolkit by its name.
Args:
tool_name (`str`):
The name of the tool function to be removed.
"""
if tool_name not in self.tools:
logger.warning(
"Skipping removing tool function '%s' as it does not exist.",
tool_name,
)
self.tools.pop(tool_name, None)
def get_json_schemas(
self,
) -> list[dict]:
"""Get the JSON schemas from the tool functions that belong to the
active groups.
.. note:: The preset keyword arguments is removed from the JSON
schema, and the extended model is applied if it is set.
Example:
.. code-block:: JSON
:caption: Example of tool function JSON schemas
[
{
"type": "function",
"function": {
"name": "google_search",
"description": "Search on Google.",
"parameters": {
"type": "object",
"properties": {
"query": {
"type": "string",
"description": "The search query."
}
},
"required": ["query"]
}
}
},
...
]
Returns:
`list[dict]`:
A list of function JSON schemas.
"""
# If meta tool is set here, update its extended model here
if "reset_equipped_tools" in self.tools:
fields = {}
for group_name, group in self.groups.items():
if group_name == "basic":
continue
fields[group_name] = (
bool,
Field(
default=False,
description=group.description,
),
)
extended_model = create_model("_DynamicModel", **fields)
self.set_extended_model(
"reset_equipped_tools",
extended_model,
)
return [
tool.extended_json_schema
for tool in self.tools.values()
if tool.group == "basic" or self.groups[tool.group].active
]
def set_extended_model(
self,
func_name: str,
model: Type[BaseModel] | None,
) -> None:
"""Set the extended model for a tool function, so that the original
JSON schema will be extended.
Args:
func_name (`str`):
The name of the tool function.
model (`Union[Type[BaseModel], None]`):
The extended model to be set.
"""
if model is not None and not issubclass(model, BaseModel):
raise TypeError(
"The extended model must be a child class of pydantic "
f"BaseModel, but got {type(model)}.",
)
if func_name in self.tools:
self.tools[func_name].extended_model = model
else:
raise ValueError(
f"Tool function '{func_name}' not found in the toolkit.",
)
async def remove_mcp_clients(
self,
client_names: list[str],
) -> None:
"""Remove tool functions from the MCP clients by their names.
Args:
client_names (`list[str]`):
The names of the MCP client, which used to initialize the
client instance.
"""
if isinstance(client_names, str):
client_names = [client_names]
if isinstance(client_names, list) and not all(
isinstance(_, str) for _ in client_names
):
raise TypeError(
f"The client_names must be a list of strings, "
f"but got {type(client_names)}.",
)
to_removed = []
func_names = deepcopy(list(self.tools.keys()))
for func_name in func_names:
if self.tools[func_name].mcp_name in client_names:
self.tools.pop(func_name)
to_removed.append(func_name)
logger.info(
"Removed %d tool functions from %d MCP: %s",
len(to_removed),
len(client_names),
", ".join(to_removed),
)
@trace_toolkit
async def call_tool_function(
self,
tool_call: ToolUseBlock,
) -> AsyncGenerator[ToolResponse, None]:
"""Execute the tool function by the `ToolUseBlock` and return the
tool response chunk in unified streaming mode, i.e. an async
generator of `ToolResponse` objects.
.. note:: The tool response chunk is **accumulated**.
Args:
tool_call (`ToolUseBlock`):
A tool call block.
Yields:
`ToolResponse`:
The tool response chunk, in accumulative manner.
"""
# Check
if tool_call["name"] not in self.tools:
return _object_wrapper(
ToolResponse(
content=[
TextBlock(
type="text",
text="FunctionNotFoundError: Cannot find the "
f"function named {tool_call['name']}",
),
],
),
None,
)
# Prepare function and keyword arguments
tool_func = self.tools[tool_call["name"]]
kwargs = {
**tool_func.preset_kwargs,
**(tool_call.get("input", {}) or {}),
}
# Prepare postprocess function
if tool_func.postprocess_func:
partial_postprocess_func = partial(
tool_func.postprocess_func,
tool_call,
)
else:
partial_postprocess_func = None
# Async function
try:
if inspect.iscoroutinefunction(tool_func.original_func):
try:
res = await tool_func.original_func(**kwargs)
except asyncio.CancelledError:
res = ToolResponse(
content=[
TextBlock(
type="text",
text="<system-info>"
"The tool call has been interrupted "
"by the user."
"</system-info>",
),
],
stream=True,
is_last=True,
is_interrupted=True,
)
else:
# When `tool_func.original_func` is Async generator function or
# Sync function
res = tool_func.original_func(**kwargs)
except Exception as e:
res = ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error: {e}",
),
],
)
# Handle different return type
# If return an async generator
if isinstance(res, AsyncGenerator):
return _async_generator_wrapper(res, partial_postprocess_func)
# If return a sync generator
if isinstance(res, Generator):
return _sync_generator_wrapper(res, partial_postprocess_func)
if isinstance(res, ToolResponse):
return _object_wrapper(res, partial_postprocess_func)
raise TypeError(
"The tool function must return a ToolResponse object, or an "
"AsyncGenerator/Generator of ToolResponse objects, "
f"but got {type(res)}.",
)
async def register_mcp_client(
self,
mcp_client: MCPClientBase,
group_name: str = "basic",
enable_funcs: list[str] | None = None,
disable_funcs: list[str] | None = None,
preset_kwargs_mapping: dict[str, dict[str, Any]] | None = None,
postprocess_func: Callable[
[
ToolUseBlock,
ToolResponse,
],
ToolResponse | None,
]
| None = None,
) -> None:
"""Register tool functions from an MCP client.
Args:
mcp_client (`MCPClientBase`):
The MCP client instance to connect to the MCP server.
group_name (`str`, defaults to `"basic"`):
The group name that the tool functions will be added to.
enable_funcs (`list[str] | None`, optional):
The functions to be added into the toolkit. If `None`, all
tool functions within the MCP servers will be added.
disable_funcs (`list[str] | None`, optional):
The functions that will be filtered out. If `None`, no
tool functions will be filtered out.
preset_kwargs_mapping: (`Optional[dict[str, dict[str, Any]]]`, \
defaults to `None`):
The preset keyword arguments mapping, whose keys are the tool
function names and values are the preset keyword arguments.
postprocess_func (`Callable[[ToolUseBlock, ToolResponse], \
ToolResponse | None] | None`, optional):
A post-processing function that will be called after the tool
function is executed, taking the tool call block and tool
response as arguments. If it returns `None`, the tool
result will be returned as is. If it returns a
`ToolResponse`, the returned block will be used as the
final tool result.
"""
if (
isinstance(mcp_client, StatefulClientBase)
and not mcp_client.is_connected
):
raise RuntimeError(
"The MCP client is not connected to the server. Use the "
"`connect()` method first.",
)
# Check arguments for enable_funcs and disabled_funcs
if enable_funcs is not None and disable_funcs is not None:
assert isinstance(enable_funcs, list) and all(
isinstance(_, str) for _ in enable_funcs
), (
"Enable functions should be a list of strings, but got "
f"{enable_funcs}."
)
assert isinstance(disable_funcs, list) and all(
isinstance(_, str) for _ in disable_funcs
), (
"Disable functions should be a list of strings, but got "
f"{disable_funcs}."
)
intersection = set(enable_funcs).intersection(
set(disable_funcs),
)
assert len(intersection) == 0, (
f"The functions in enable_funcs and disable_funcs "
f"should not overlap, but got {intersection}."
)
if not (
preset_kwargs_mapping is None
or isinstance(preset_kwargs_mapping, dict)
):
raise TypeError(
f"The preset_kwargs_mapping must be a dictionary or None, "
f"but got {type(preset_kwargs_mapping)}.",
)
tool_names = []
for mcp_tool in await mcp_client.list_tools():
# Skip the functions that are not in the enable_funcs if
# enable_funcs is not None
if enable_funcs is not None and mcp_tool.name not in enable_funcs:
continue
# Skip the disabled functions
if disable_funcs is not None and mcp_tool.name in disable_funcs:
continue
tool_names.append(mcp_tool.name)
# Obtain callable function object
func_obj = await mcp_client.get_callable_function(
func_name=mcp_tool.name,
wrap_tool_result=True,
)
# Prepare preset kwargs
preset_kwargs = None
if preset_kwargs_mapping is not None:
preset_kwargs = preset_kwargs_mapping.get(mcp_tool.name, {})
# TODO: handle mcp_server_name
self.register_tool_function(
tool_func=func_obj,
group_name=group_name,
preset_kwargs=preset_kwargs,
postprocess_func=postprocess_func,
)
logger.info(
"Registered %d tool functions from MCP: %s.",
len(tool_names),
", ".join(tool_names),
)
def state_dict(self) -> dict[str, Any]:
"""Get the state dictionary of the toolkit.
Returns:
`dict[str, Any]`:
A dictionary containing the active tool group names.
"""
return {
"active_groups": [
name for name, group in self.groups.items() if group.active
],
}
def load_state_dict(
self,
state_dict: dict[str, Any],
strict: bool = True,
) -> None:
"""Load the state dictionary into the toolkit.
Args:
state_dict (`dict`):
The state dictionary to load, which should have "active_groups"
key and its value must be a list of group names.
strict (`bool`, defaults to `True`):
If `True`, raises an error if any key in the module is not
found in the state_dict. If `False`, skips missing keys.
"""
if (
not isinstance(state_dict, dict)
or "active_groups" not in state_dict
or not isinstance(state_dict["active_groups"], list)
):
raise ValueError(
"The state_dict for toolkit must be a dictionary with "
"active_groups key and its value must be a list, "
f"but got {type(state_dict)}.",
)
if strict and list(state_dict.keys()) != ["active_groups"]:
raise ValueError(
"Get additional keys in the state_dict: "
f'{list(state_dict.keys())}, but only "active_groups" '
"is expected.",
)
for group_name, group in self.groups.items():
if group_name in state_dict["active_groups"]:
group.active = True
else:
group.active = False
def get_activated_notes(self) -> str:
"""Get the notes from the active tool groups, which can be used to
construct the system prompt for the agent.
Returns:
`str`:
The combined notes from the active tool groups.
"""
collected_notes = []
for group_name, group in self.groups.items():
if group.active and group.notes:
collected_notes.append(
"\n".join(
[f"## About {group_name} Tools", group.notes],
),
)
return "\n".join(collected_notes)
def reset_equipped_tools(self, **kwargs: Any) -> ToolResponse:
"""Choose appropriate tools to equip yourself with, so that you can
finish your task. Each argument in this function represents a group
of related tools, and the value indicates whether to activate the
group or not. Besides, the tool response of this function will
contain the precaution notes for using them, which you
**MUST pay attention to and follow**. You can also reuse this function
to check the notes of the tool groups.
Note this function will `reset` the tools, so that the original tools
will be removed first.
"""
to_activate = []
for key, value in kwargs.items():
if not isinstance(value, bool):
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Invalid arguments: the argument {key} "
f"should be a bool value, but got {type(value)}.",
),
],
)
if value:
to_activate.append(key)
self.update_tool_groups(to_activate, active=True)
notes = self.get_activated_notes()
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Active tool groups successfully: {to_activate}. "
"You MUST follow these notes to use the tools:\n"
f"<notes>{notes}</notes>",
),
],
)
def clear(self) -> None:
"""Clear the toolkit, removing all tool functions and groups."""
self.tools.clear()
self.groups.clear()
def _validate_tool_function(self, func_name: str) -> None:
"""Check if the tool function already registered in the toolkit. If
so, raise a ValueError."""
if func_name in self.tools:
raise ValueError(
f"A function with name '{func_name} is already registered "
"in the toolkit.",
)
@staticmethod
def _parse_tool_function(
tool_func: ToolFunction,
include_long_description: bool,
include_var_positional: bool,
include_var_keyword: bool,
) -> dict:
"""Extract JSON schema from the tool function's docstring"""
docstring = parse(tool_func.__doc__)
params_docstring = {
_.arg_name: _.description for _ in docstring.params
}
# Function description
descriptions = []
if docstring.short_description is not None:
descriptions.append(docstring.short_description)
if include_long_description and docstring.long_description is not None:
descriptions.append(docstring.long_description)
func_description = "\n\n".join(descriptions)
# Create a dynamic model with the function signature
fields = {}
for name, param in inspect.signature(tool_func).parameters.items():
# Skip the `self` and `cls` parameters
if name in ["self", "cls"]:
continue
# Handle `**kwargs`
if param.kind == inspect.Parameter.VAR_KEYWORD:
if not include_var_keyword:
continue
fields[name] = (
Dict[str, Any]
if param.annotation == inspect.Parameter.empty
else Dict[str, param.annotation], # type: ignore
Field(
description=params_docstring.get(
f"**{name}",
params_docstring.get(name, None),
),
default={}
if param.default is param.empty
else param.default,
),
)
elif param.kind == inspect.Parameter.VAR_POSITIONAL:
if not include_var_positional:
continue
fields[name] = (
list[Any]
if param.annotation == inspect.Parameter.empty
else list[param.annotation], # type: ignore
Field(
description=params_docstring.get(
f"*{name}",
params_docstring.get(name, None),
),
default=[]
if param.default is param.empty
else param.default,
),
)
else:
fields[name] = (
Any
if param.annotation == inspect.Parameter.empty
else param.annotation,
Field(
description=params_docstring.get(name, None),
default=...
if param.default is param.empty
else param.default,
),
)
base_model = create_model(
"_StructuredOutputDynamicClass",
__config__=ConfigDict(arbitrary_types_allowed=True),
**fields,
)
params_json_schema = base_model.model_json_schema()
# Remove the title from the json schema
_remove_title_field(params_json_schema)
func_json_schema: dict = {
"type": "function",
"function": {
"name": tool_func.__name__,
"parameters": params_json_schema,
},
}
if func_description not in [None, ""]:
func_json_schema["function"]["description"] = func_description
return func_json_schema
==== _coding ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The coding-related tools module in agentscope."""
from ._python import execute_python_code
from ._shell import execute_shell_command
__all__ = [
"execute_python_code",
"execute_shell_command",
]
---- _python.py ----
# -*- coding: utf-8 -*-
# pylint: disable=unused-argument
"""The Python code execution tool in agentscope."""
import asyncio
import os
import sys
import tempfile
from typing import Any
import shortuuid
from ...message import TextBlock
from .._response import ToolResponse
async def execute_python_code(
code: str,
timeout: float = 300,
**kwargs: Any,
) -> ToolResponse:
"""Execute the given python code in a temp file and capture the return
code, standard output and error. Note you must `print` the output to get
the result, and the tmp file will be removed right after the execution.
Args:
code (`str`):
The Python code to be executed.
timeout (`float`, defaults to `300`):
The maximum time (in seconds) allowed for the code to run.
Returns:
`ToolResponse`:
The response containing the return code, standard output, and
standard error of the executed code.
"""
with tempfile.TemporaryDirectory() as temp_dir:
temp_file = os.path.join(temp_dir, f"tmp_{shortuuid.uuid()}.py")
with open(temp_file, "w", encoding="utf-8") as f:
f.write(code)
proc = await asyncio.create_subprocess_exec(
sys.executable,
"-u",
temp_file,
stdout=asyncio.subprocess.PIPE,
stderr=asyncio.subprocess.PIPE,
)
try:
await asyncio.wait_for(proc.wait(), timeout=timeout)
stdout, stderr = await proc.communicate()
stdout_str = stdout.decode("utf-8")
stderr_str = stderr.decode("utf-8")
returncode = proc.returncode
except asyncio.TimeoutError:
stderr_suffix = (
f"TimeoutError: The code execution exceeded "
f"the timeout of {timeout} seconds."
)
returncode = -1
try:
proc.terminate()
stdout, stderr = await proc.communicate()
stdout_str = stdout.decode("utf-8")
stderr_str = stderr.decode("utf-8")
if stderr_str:
stderr_str += f"\n{stderr_suffix}"
else:
stderr_str = stderr_suffix
except ProcessLookupError:
stdout_str = ""
stderr_str = stderr_suffix
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"<returncode>{returncode}</returncode>"
f"<stdout>{stdout_str}</stdout>"
f"<stderr>{stderr_str}</stderr>",
),
],
)
---- _shell.py ----
# -*- coding: utf-8 -*-
# pylint: disable=unused-argument
"""The shell command tool in agentscope."""
import asyncio
from typing import Any
from .._response import ToolResponse
from ...message import TextBlock
async def execute_shell_command(
command: str,
timeout: int = 300,
**kwargs: Any,
) -> ToolResponse:
"""Execute given command and return the return code, standard output and
error within <returncode></returncode>, <stdout></stdout> and
<stderr></stderr> tags.
Args:
command (`str`):
The shell command to execute.
timeout (`float`, defaults to `300`):
The maximum time (in seconds) allowed for the command to run.
Returns:
`ToolResponse`:
The tool response containing the return code, standard output, and
standard error of the executed command.
"""
proc = await asyncio.create_subprocess_shell(
command,
stdout=asyncio.subprocess.PIPE,
stderr=asyncio.subprocess.PIPE,
bufsize=0,
)
try:
await asyncio.wait_for(proc.wait(), timeout=timeout)
stdout, stderr = await proc.communicate()
stdout_str = stdout.decode("utf-8")
stderr_str = stderr.decode("utf-8")
returncode = proc.returncode
except asyncio.TimeoutError:
stderr_suffix = (
f"TimeoutError: The command execution exceeded "
f"the timeout of {timeout} seconds."
)
returncode = -1
try:
proc.terminate()
stdout, stderr = await proc.communicate()
stdout_str = stdout.decode("utf-8")
stderr_str = stderr.decode("utf-8")
if stderr_str:
stderr_str += f"\n{stderr_suffix}"
else:
stderr_str = stderr_suffix
except ProcessLookupError:
stdout_str = ""
stderr_str = stderr_suffix
return ToolResponse(
content=[
TextBlock(
type="text",
text=(
f"<returncode>{returncode}</returncode>"
f"<stdout>{stdout_str}</stdout>"
f"<stderr>{stderr_str}</stderr>"
),
),
],
)
==== _multi_modality ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The multi-modal-related tools module in agentscope."""
from ._dashscope_tools import (
dashscope_image_to_text,
dashscope_text_to_audio,
dashscope_text_to_image,
)
from ._openai_tools import (
openai_text_to_image,
openai_edit_image,
openai_text_to_audio,
openai_create_image_variation,
openai_image_to_text,
openai_audio_to_text,
)
__all__ = [
"dashscope_image_to_text",
"dashscope_text_to_audio",
"dashscope_text_to_image",
"openai_text_to_image",
"openai_text_to_audio",
"openai_edit_image",
"openai_create_image_variation",
"openai_image_to_text",
"openai_audio_to_text",
]
---- _dashscope_tools.py ----
# -*- coding: utf-8 -*-
"""Use DashScope API to generate images,
convert text to audio, and convert images to text.
Please refer to the `official documentation <https://dashscope.aliyun.com/>`_
for more details.
"""
import base64
from typing import Literal, Sequence
import os
from ..._utils._common import _get_bytes_from_web_url
from ...message import ImageBlock, TextBlock, AudioBlock
from ...tool import ToolResponse
def dashscope_text_to_image(
prompt: str,
api_key: str,
n: int = 1,
size: Literal["1024*1024", "720*1280", "1280*720"] = "1024*1024",
model: str = "wanx-v1",
use_base64: bool = False,
) -> ToolResponse:
"""Generate image(s) based on the given prompt, and return image url(s)
or base64 data.
Args:
prompt (`str`):
The text prompt to generate image.
api_key (`str`):
The api key for the dashscope api.
n (`int`, defaults to `1`):
The number of images to generate.
size (`Literal["1024*1024", "720*1280", "1280*720"]`, defaults to \
`"1024*1024"`):
Size of the image.
model (`str`, defaults to '"wanx-v1"'):
The model to use, such as "wanx-v1", "qwen-image",
"wan2.2-t2i-flash", etc.
use_base64 (`bool`, defaults to 'False'):
Whether to use base64 data for images.
Returns:
`ToolResponse`:
A ToolResponse containing the generated content
(ImageBlock/TextBlock/AudioBlock) or error information if the
operation failed.
"""
try:
import dashscope
response = dashscope.ImageSynthesis.call(
model=model,
prompt=prompt,
api_key=api_key,
n=n,
size=size,
)
images = response.output["results"]
urls = [_["url"] for _ in images]
image_blocks: list = []
if urls is not None:
for url in urls:
if use_base64:
extension = url.split(".")[-1].lower()
image_data = _get_bytes_from_web_url(url)
image_blocks.append(
ImageBlock(
type="image",
source={
"type": "base64",
"media_type": f"image/{extension}",
"data": image_data,
},
),
)
else:
image_blocks.append(
ImageBlock(
type="image",
source={
"type": "url",
"url": url,
},
),
)
return ToolResponse(
content=image_blocks,
)
else:
return ToolResponse(
[
TextBlock(
type="text",
text="Error: Failed to generate images",
),
],
)
except Exception as e:
return ToolResponse(
[
TextBlock(
type="text",
text=f"Failed to generate images: {str(e)}",
),
],
)
def dashscope_image_to_text(
image_urls: str | Sequence[str],
api_key: str,
prompt: str = "Describe the image",
model: str = "qwen-vl-plus",
) -> ToolResponse:
"""Generate text based on the given images.
Args:
image_urls (`str | Sequence[str]`):
The url of single or multiple images.
api_key (`str`):
The api key for the dashscope api.
prompt (`str`, defaults to 'Describe the image' ):
The text prompt.
model (`str`, defaults to 'qwen-vl-plus'):
The model to use in DashScope MultiModal API.
Returns:
`ToolResponse`:
A ToolResponse containing the generated content
(ImageBlock/TextBlock/AudioBlock) or error information if the
operation failed.
"""
if isinstance(image_urls, str):
image_urls = [image_urls]
# Check if the local url is valid
img_abs_urls = []
for url in image_urls:
if os.path.exists(url):
if os.path.isfile(url):
img_abs_urls.append(os.path.abspath(url))
else:
return ToolResponse(
[
TextBlock(
type="text",
text=f'Error: The input image url "{url}" is '
f"not a file.",
),
],
)
else:
# Maybe a web url or an invalid url, we leave it to the API
# to handle
img_abs_urls.append(url)
# Convert image paths according to the model requirements
contents = []
for url in img_abs_urls:
contents.append(
{
"image": url,
},
)
contents.append({"text": prompt})
# currently only support one round of conversation
# if multiple rounds of conversation are needed,
# it would be better to implement an Agent class
sys_message = {
"role": "system",
"content": [{"text": "You are a helpful assistant."}],
}
user_message = {
"role": "user",
"content": contents,
}
messages = [sys_message, user_message]
try:
import dashscope
response = dashscope.MultiModalConversation.call(
model=model,
messages=messages,
api_key=api_key,
)
content = response.output["choices"][0]["message"]["content"]
if isinstance(content, list):
content = content[0]["text"]
if content is not None:
return ToolResponse(
[
TextBlock(
type="text",
text=content,
),
],
)
else:
return ToolResponse(
[
TextBlock(
type="text",
text="Error: Failed to generate text",
),
],
)
except Exception as e:
return ToolResponse(
[
TextBlock(
type="text",
text=f"Failed to generate text: {str(e)}",
),
],
)
def dashscope_text_to_audio(
text: str,
api_key: str,
model: str = "sambert-zhichu-v1",
sample_rate: int = 48000,
) -> ToolResponse:
"""Convert the given text to audio.
Args:
text (`str`):
The text to be converted into audio.
api_key (`str`):
The api key for the dashscope API.
model (`str`, defaults to 'sambert-zhichu-v1'):
The model to use. Full model list can be found in the
`official document
<https://help.aliyun.com/zh/model-studio/sambert-python-sdk>`_.
sample_rate (`int`, defaults to 48000):
Sample rate of the audio.
Returns:
`ToolResponse`:
A ToolResponse containing the generated content
(ImageBlock/TextBlock/AudioBlock) or error information if the
operation failed.
"""
try:
import dashscope
dashscope.api_key = api_key
res = dashscope.audio.tts.SpeechSynthesizer.call(
model=model,
text=text,
sample_rate=sample_rate,
format="wav",
)
audio_data = res.get_audio_data()
if audio_data is not None:
audio_base64 = base64.b64encode(audio_data).decode("utf-8")
return ToolResponse(
[
AudioBlock(
type="audio",
source={
"type": "base64",
"media_type": "audio/wav",
"data": audio_base64,
},
),
],
)
else:
return ToolResponse(
[
TextBlock(
type="text",
text="Error: Failed to generate audio",
),
],
)
except Exception as e:
return ToolResponse(
[
TextBlock(
type="text",
text=f"Failed to generate audio: {str(e)}",
),
],
)
---- _openai_tools.py ----
# -*- coding: utf-8 -*-
"""
Wrap OpenAI API calls as tools. Refer the official
`OpenAI API documentation <https://platform.openai.com/docs/overview>`_ for
more details.
"""
import base64
from io import BytesIO
import os
from typing import Literal, IO
import requests
from .. import ToolResponse
from ...formatter._openai_formatter import _to_openai_image_url
from ...message import (
ImageBlock,
TextBlock,
Base64Source,
URLSource,
AudioBlock,
)
def _parse_url(url: str) -> BytesIO | IO[bytes]:
"""
If url is a local file path, return a BytesIO of the file content.
If url is a web URL, fetch the content and return as BytesIO.
"""
if url.startswith(("http://", "https://")):
response = requests.get(url)
response.raise_for_status() # Raise an exception for HTTP errors
return BytesIO(response.content)
else:
if not os.path.exists(url):
raise FileNotFoundError(f"File not found: {url}")
return open(os.path.abspath(url), "rb")
def openai_text_to_image(
prompt: str,
api_key: str,
n: int = 1,
model: Literal["dall-e-2", "dall-e-3", "gpt-image-1"] = "dall-e-2",
size: Literal[
"256x256",
"512x512",
"1024x1024",
"1792x1024",
"1024x1792",
] = "256x256",
quality: Literal[
"auto",
"standard",
"hd",
"high",
"medium",
"low",
] = "auto",
style: Literal["vivid", "natural"] = "vivid",
response_format: Literal["url", "b64_json"] = "url",
) -> ToolResponse:
"""
Generate image(s) based on the given prompt, and return image URL(s) or
base64 data.
Args:
prompt (`str`):
The text prompt to generate images.
api_key (`str`):
The API key for the OpenAI API.
n (`int`, defaults to `1`):
The number of images to generate.
model (`Literal["dall-e-2", "dall-e-3"]`, defaults to `"dall-e-2"`):
The model to use for image generation.
size (`Literal["256x256", "512x512", "1024x1024", "1792x1024", \
"1024x1792"]`, defaults to `"256x256"`):
The size of the generated images.
Must be one of 1024x1024, 1536x1024 (landscape), 1024x1536 (
portrait), or auto (default value) for gpt-image-1,
one of 256x256, 512x512, or 1024x1024 for dall-e-2,
and one of 1024x1024, 1792x1024, or 1024x1792 for dall-e-3.
quality (`Literal["auto", "standard", "hd", "high", "medium", \
"low"]`, defaults to `"auto"`):
The quality of the image that will be generated.
- `auto` (default value) will automatically select the best
quality for the given model.
- `high`, `medium` and `low` are supported for gpt-image-1.
- `hd` and `standard` are supported for dall-e-3.
- `standard` is the only option for dall-e-2.
style (`Literal["vivid", "natural"]`, defaults to `"vivid"`):
The style of the generated images.
This parameter is only supported for dall-e-3.
Must be one of `vivid` or `natural`.
- `Vivid` causes the model to lean towards generating hyper-real
and dramatic images.
- `Natural` causes the model to produce more natural,
less hyper-real looking images.
response_format (`Literal["url", "b64_json"]`, defaults to `"url"`):
The format in which generated images with dall-e-2 and dall-e-3
are returned.
- Must be one of "url" or "b64_json".
- URLs are only valid for 60 minutes after the image has been
generated.
- This parameter isn't supported for gpt-image-1 which will always
return base64-encoded images.
Returns:
`ToolResponse`:
A ToolResponse containing the generated content
(ImageBlock/TextBlock/AudioBlock) or error information if the
operation failed.
"""
kwargs = {
"model": model,
"prompt": prompt,
"n": n,
"size": size,
}
if model == "dall-e-3":
kwargs["style"] = style
if model != "dall-e-2":
kwargs["quality"] = quality
if model != "gpt-image-1":
kwargs["response_format"] = response_format
if model == "gpt-image-1":
response_format = "b64_json"
try:
import openai
client = openai.OpenAI(
api_key=api_key,
)
response = client.images.generate(
**kwargs,
)
image_blocks: list = []
if response_format == "url":
image_urls = [_.url for _ in response.data]
for image_url in image_urls:
image_blocks.append(
ImageBlock(
type="image",
source=URLSource(
type="url",
url=image_url,
),
),
)
else:
image_datas = [_.b64_json for _ in response.data]
for image_data in image_datas:
image_blocks.append(
ImageBlock(
type="image",
source=Base64Source(
type="base64",
media_type="image/png",
data=image_data,
),
),
)
return ToolResponse(
content=image_blocks,
)
except Exception as e:
return ToolResponse(
[
TextBlock(
type="text",
text=f"Failed to generate image: {str(e)}",
),
],
)
def openai_edit_image(
image_url: str,
prompt: str,
api_key: str,
model: Literal["dall-e-2", "gpt-image-1"] = "dall-e-2",
mask_url: str | None = None,
n: int = 1,
size: Literal[
"256x256",
"512x512",
"1024x1024",
] = "256x256",
response_format: Literal["url", "b64_json"] = "url",
) -> ToolResponse:
"""
Edit an image based on the provided mask and prompt, and return the edited
image URL(s) or base64 data.
Args:
image_url (`str`):
The file path or URL to the image that needs editing.
prompt (`str`):
The text prompt describing the edits to be made to the image.
api_key (`str`):
The API key for the OpenAI API.
model (`Literal["dall-e-2", "gpt-image-1"]`, defaults to `"dall-e-2"`):
The model to use for image generation.
mask_url (`str | None`, defaults to `None`):
The file path or URL to the mask image that specifies the regions
to be edited.
n (`int`, defaults to `1`):
The number of edited images to generate.
size (`Literal["256x256", "512x512", "1024x1024"]`, defaults to \
`"256x256"`):
The size of the edited images.
response_format (`Literal["url", "b64_json"]`, defaults to `"url"`):
The format in which generated images are returned.
- Must be one of "url" or "b64_json".
- URLs are only valid for 60 minutes after generation.
- This parameter isn't supported for gpt-image-1 which will
always return base64-encoded images.
Returns:
`ToolResponse`:
A ToolResponse containing the generated content
(ImageBlock/TextBlock/AudioBlock) or error information if the
operation failed.
"""
try:
import openai
client = openai.OpenAI(
api_key=api_key,
)
def prepare_image(url_or_path: str) -> BytesIO:
from PIL import Image
if url_or_path.startswith(("http://", "https://")):
response = requests.get(url_or_path)
response.raise_for_status()
img = Image.open(BytesIO(response.content))
else:
img = Image.open(url_or_path)
if img.mode != "RGBA":
img = img.convert("RGBA")
img_buffer = BytesIO()
img.save(img_buffer, format="PNG")
img_buffer.seek(0)
img_buffer.name = "image.png"
return img_buffer
image_file = prepare_image(image_url)
kwargs = {
"model": model,
"image": image_file,
"prompt": prompt,
"n": n,
"size": size,
}
if mask_url:
kwargs["mask"] = prepare_image(mask_url)
if model == "dall-e-2":
kwargs["response_format"] = response_format
else:
response_format = "b64_json"
response = client.images.edit(**kwargs)
if response_format == "url":
urls = [_.url for _ in response.data]
image_blocks: list = []
for url in urls:
image_blocks.append(
ImageBlock(
type="image",
source=URLSource(
type="url",
url=url,
),
),
)
return ToolResponse(
content=image_blocks,
)
elif response_format == "b64_json":
image_datas = [_.b64_json for _ in response.data]
image_blocks = []
for image_data in image_datas:
image_blocks.append(
ImageBlock(
type="image",
source=Base64Source(
type="base64",
media_type="image/png",
data=image_data,
),
),
)
return ToolResponse(
content=image_blocks,
)
except Exception as e:
return ToolResponse(
[
TextBlock(
type="text",
text=f"Failed to generate image: {str(e)}",
),
],
)
def openai_create_image_variation(
image_url: str,
api_key: str,
n: int = 1,
model: Literal["dall-e-2"] = "dall-e-2",
size: Literal[
"256x256",
"512x512",
"1024x1024",
] = "256x256",
response_format: Literal["url", "b64_json"] = "url",
) -> ToolResponse:
"""
Create variations of an image and return the image URL(s) or base64 data.
Args:
image_url (`str`):
The file path or URL to the image from which variations will be
generated.
api_key (`str`):
The API key for the OpenAI API.
n (`int`, defaults to `1`):
The number of image variations to generate.
model (` Literal["dall-e-2"]`, default to `dall-e-2`):
The model to use for image variation.
size (`Literal["256x256", "512x512", "1024x1024"]`, defaults to \
`"256x256"`):
The size of the generated image variations.
response_format (`Literal["url", "b64_json"]`, defaults to `"url"`):
The format in which generated images are returned.
- Must be one of url or b64_json.
- URLs are only valid for 60 minutes after the image has been
generated.
Returns:
`ToolResponse`:
A ToolResponse containing the generated content
(ImageBlock/TextBlock/AudioBlock) or error information if the
operation failed.
"""
# _parse_url handles both local and web URLs and returns BytesIO
image = _parse_url(image_url)
try:
import openai
client = openai.OpenAI(
api_key=api_key,
)
response = client.images.create_variation(
model=model,
image=image,
n=n,
size=size,
)
image_blocks: list = []
if response_format == "url":
urls = [_.url for _ in response.data]
for url in urls:
image_blocks.append(
ImageBlock(
type="image",
source=URLSource(
type="url",
url=url,
),
),
)
else:
image_datas = [_.b64_json for _ in response.data]
for image_data in image_datas:
image_blocks.append(
ImageBlock(
type="image",
source=Base64Source(
type="base64",
media_type="image/png",
data=image_data,
),
),
)
return ToolResponse(
content=image_blocks,
)
except Exception as e:
return ToolResponse(
[
TextBlock(
type="text",
text=f"Failed to generate image: {str(e)}",
),
],
)
def openai_image_to_text(
image_urls: str | list[str],
api_key: str,
prompt: str = "Describe the image",
model: str = "gpt-4o",
) -> ToolResponse:
"""
Generate descriptive text for given image(s) using a specified model, and
return the generated text.
Args:
image_urls (`str | list[str]`):
The URL or list of URLs pointing to the images that need to be
described.
api_key (`str`):
The API key for the OpenAI API.
prompt (`str`, defaults to `"Describe the image"`):
The prompt that instructs the model on how to describe
the image(s).
model (`str`, defaults to `"gpt-4o"`):
The model to use for generating the text descriptions.
Returns:
`ToolResponse`:
A ToolResponse containing the generated content
(ImageBlock/TextBlock/AudioBlock) or error information if the
operation failed.
"""
if isinstance(image_urls, str):
image_urls = [image_urls]
content = []
for url in image_urls:
content.append(
{
"type": "image_url",
"image_url": {
"url": _to_openai_image_url(url),
},
},
)
content.append(
{
"type": "text",
"text": prompt,
},
)
messages = [
{
"role": "user",
"content": content,
},
]
try:
import openai
client = openai.OpenAI(
api_key=api_key,
)
response = client.chat.completions.create(
messages=messages,
model=model,
)
return ToolResponse(
[
TextBlock(
type="text",
text=response.choices[0].message.content,
),
],
)
except Exception as e:
return ToolResponse(
[
TextBlock(
type="text",
text=f"Failed to generate text: {str(e)}",
),
],
)
def openai_text_to_audio(
text: str,
api_key: str,
model: Literal["tts-1", "tts-1-hd", "gpt-4o-mini-tts"] = "tts-1",
voice: Literal[
"alloy",
"ash",
"ballad",
"coral",
"echo",
"fable",
"nova",
"onyx",
"sage",
"shimmer",
] = "alloy",
speed: float = 1.0,
res_format: Literal[
"mp3",
"opus",
"aac",
"flac",
"wav",
"pcm",
] = "mp3",
) -> ToolResponse:
"""
Convert text to an audio file using a specified model and voice.
Args:
text (`str`):
The text to convert to audio.
api_key (`str`):
The API key for the OpenAI API.
model (`Literal["tts-1", "tts-1-hd"]`, defaults to `"tts-1"`):
The model to use for text-to-speech conversion.
voice (`Literal["alloy", "echo", "fable", "onyx", "nova", \
"shimmer"]`, defaults to `"alloy"`):
The voice to use for the audio output.
speed (`float`, defaults to `1.0`):
The speed of the audio playback. A value of 1.0 is normal speed.
res_format (`Literal["mp3", "wav", "opus", "aac", "flac", \
"wav", "pcm"]`, defaults to `"mp3"`):
The format of the audio file.
Returns:
`ToolResponse`:
A ToolResponse containing the generated content
(ImageBlock/TextBlock/AudioBlock) or error information if the
operation failed.
"""
try:
import openai
client = openai.OpenAI(
api_key=api_key,
)
response = client.audio.speech.create(
model=model,
voice=voice,
speed=speed,
input=text,
response_format=res_format,
)
audio_bytes = response.content
audio_base64 = base64.b64encode(audio_bytes).decode("utf-8")
return ToolResponse(
[
AudioBlock(
type="audio",
source=Base64Source(
type="base64",
media_type=f"audio/{res_format}",
data=audio_base64,
),
),
],
)
except Exception as e:
return ToolResponse(
[
TextBlock(
type="text",
text=f"Error: Failed to generate audio. {str(e)}",
),
],
)
def openai_audio_to_text(
audio_file_url: str,
api_key: str,
language: str = "en",
temperature: float = 0.2,
) -> ToolResponse:
"""
Convert an audio file to text using OpenAI's transcription service.
Args:
audio_file_url (`str`):
The file path or URL to the audio file that needs to be
transcribed.
api_key (`str`):
The API key for the OpenAI API.
language (`str`, defaults to `"en"`):
The language of the input audio in
`ISO-639-1 format \
<https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes>`_
(e.g., "en", "zh", "fr"). Improves accuracy and latency.
temperature (`float`, defaults to `0.2`):
The temperature for the transcription, which affects the
randomness of the output.
Returns:
`ToolResponse`:
A ToolResponse containing the generated content
(ImageBlock/TextBlock/AudioBlock) or error information if the
operation failed.
"""
try:
import openai
client = openai.OpenAI(
api_key=api_key,
)
if audio_file_url.startswith(("http://", "https://")):
response = requests.get(audio_file_url)
response.raise_for_status()
audio_buffer = BytesIO(response.content)
import urllib.parse
from pathlib import Path
parsed_url = urllib.parse.urlparse(audio_file_url)
filename = Path(parsed_url.path).name or "audio.mp3"
audio_buffer.name = filename
audio_file = audio_buffer
transcription = client.audio.transcriptions.create(
model="whisper-1",
file=audio_file,
language=language,
temperature=temperature,
)
else:
if not os.path.exists(audio_file_url):
raise FileNotFoundError(f"File not found: {audio_file_url}")
with open(audio_file_url, "rb") as audio_file:
transcription = client.audio.transcriptions.create(
model="whisper-1",
file=audio_file,
language=language,
temperature=temperature,
)
return ToolResponse(
[
TextBlock(
type="text",
text=transcription.text,
),
],
)
except Exception as e:
return ToolResponse(
[
TextBlock(
type="text",
text=f"Error: Failed to transcribe audio: {str(e)}",
),
],
)
==== _text_file ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The text file tool module in agentscope."""
from ._view_text_file import view_text_file
from ._write_text_file import (
insert_text_file,
write_text_file,
)
__all__ = [
"insert_text_file",
"write_text_file",
"view_text_file",
]
---- _utils.py ----
# -*- coding: utf-8 -*-
"""The utility functions for text file tools in agentscope."""
from ...exception import ToolInvalidArgumentsError
def _calculate_view_ranges(
old_n_lines: int,
new_n_lines: int,
start: int,
end: int,
extra_view_n_lines: int = 5,
) -> tuple[int, int]:
"""Calculate after writing the new content, the view ranges of the file.
Args:
old_n_lines (`int`):
The number of lines before writing the new content.
new_n_lines (`int`):
The number of lines after writing the new content.
start (`int`):
The start line of the writing range.
end (`int`):
The end line of the writing range.
extra_view_n_lines (`int`, optional):
The number of extra lines to view before and after the range.
"""
view_start = max(1, start - extra_view_n_lines)
delta_lines = new_n_lines - old_n_lines
view_end = min(end + delta_lines + extra_view_n_lines, new_n_lines)
return view_start, view_end
def _assert_ranges(
ranges: list[int],
) -> None:
"""Check if the ranges are valid.
Raises:
ToolInvalidArgumentsError: If the ranges are invalid.
"""
if (
isinstance(ranges, list)
and len(ranges) == 2
and all(isinstance(i, int) for i in ranges)
):
start, end = ranges
if start > end:
raise ToolInvalidArgumentsError(
f"InvalidArgumentError: The start line is greater than the "
f"end line in the given range {ranges}.",
)
else:
raise ToolInvalidArgumentsError(
f"InvalidArgumentError: Invalid range format. Expected a list of "
f"two integers, but got {ranges}.",
)
def _view_text_file(
file_path: str,
ranges: list[int] | None = None,
) -> str:
"""Return the file content in the specified range with line numbers."""
with open(file_path, "r", encoding="utf-8") as file:
lines = file.readlines()
if ranges:
_assert_ranges(ranges)
start, end = ranges
if start > len(lines):
raise ToolInvalidArgumentsError(
f"InvalidArgumentError: The range '{ranges}' is out of bounds "
f"for the file '{file_path}', which has only {len(lines)} "
f"lines.",
)
view_content = [
f"{index + start}: {line}"
for index, line in enumerate(lines[start - 1 : end])
]
return "".join(view_content)
return "".join(f"{index + 1}: {line}" for index, line in enumerate(lines))
---- _view_text_file.py ----
# -*- coding: utf-8 -*-
# flake8: noqa: E501
# pylint: disable=line-too-long
"""The view text file tool in agentscope."""
import os
from ._write_text_file import _view_text_file
from .._response import ToolResponse
from ...exception import ToolInvalidArgumentsError
from ...message import TextBlock
async def view_text_file(
file_path: str,
ranges: list[int] | None = None,
) -> ToolResponse:
"""View the file content in the specified range with line numbers. If `ranges` is not provided, the entire file will be returned.
Args:
file_path (`str`):
The target file path.
ranges:
The range of lines to be viewed (e.g. lines 1 to 100: [1, 100]), inclusive. If not provided, the entire file will be returned. To view the last 100 lines, use [-100, -1].
Returns:
`ToolResponse`:
The tool response containing the file content or an error message.
"""
if not os.path.exists(file_path):
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error: The file {file_path} does not exist.",
),
],
)
if not os.path.isfile(file_path):
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error: The path {file_path} is not a file.",
),
],
)
try:
content = _view_text_file(file_path, ranges)
except ToolInvalidArgumentsError as e:
return ToolResponse(
content=[
TextBlock(
type="text",
text=e.message,
),
],
)
if ranges is None:
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"""The content of {file_path}:
```
{content}```""",
),
],
)
else:
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"""The content of {file_path} in {ranges} lines:
```
{content}```""",
),
],
)
---- _write_text_file.py ----
# -*- coding: utf-8 -*-
# flake8: noqa: E501
# pylint: disable=line-too-long
"""The text file tools in agentscope."""
import os
from ._utils import _calculate_view_ranges, _view_text_file
from .._response import ToolResponse
from ...message import TextBlock
async def insert_text_file(
file_path: str,
content: str,
line_number: int,
) -> ToolResponse:
"""Insert the content at the specified line number in a text file.
Args:
file_path (`str`):
The target file path.
content (`str`):
The content to be inserted.
line_number (`int`):
The line number at which the content should be inserted, starting
from 1. If exceeds the number of lines in the file, it will be
appended to the end of the file.
Returns:
`ToolResponse`:
The tool response containing the result of the insertion operation.
"""
if line_number <= 0:
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"InvalidArgumentsError: "
f"The line number {line_number} is invalid. ",
),
],
)
if not os.path.exists(file_path):
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"InvalidArgumentsError: The target file "
f"{file_path} does not exist. ",
),
],
)
with open(file_path, "r", encoding="utf-8") as file:
original_lines = file.readlines()
if line_number == len(original_lines) + 1:
new_lines = original_lines + ["\n" + content]
elif line_number < len(original_lines) + 1:
new_lines = (
original_lines[: line_number - 1]
+ [content + "\n"]
+ original_lines[line_number - 1 :]
)
else:
return ToolResponse(
content=[
TextBlock(
type="text",
text="InvalidArgumentsError: The given line_number "
f"({line_number}) is not in the valid range "
f"[1, {len(original_lines) + 1}].",
),
],
)
with open(file_path, "w", encoding="utf-8") as file:
file.writelines(new_lines)
with open(file_path, "r", encoding="utf-8") as file:
new_lines = file.readlines()
start, end = _calculate_view_ranges(
len(original_lines),
len(new_lines),
line_number,
line_number,
extra_view_n_lines=5,
)
show_content = _view_text_file(file_path, [start, end])
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Insert content into {file_path} at line "
f"{line_number} successfully. The new content "
f"between lines {start}-{end} is:\n"
f"```\n{show_content}```",
),
],
)
async def write_text_file(
file_path: str,
content: str,
ranges: None | list[int] = None,
) -> ToolResponse:
"""Create/Replace/Overwrite content in a text file. When `ranges` is provided, the content will be replaced in the specified range. Otherwise, the entire file (if exists) will be overwritten.
Args:
file_path (`str`):
The target file path.
content (`str`):
The content to be written.
ranges (`list[int] | None`, defaults to `None`):
The range of lines to be replaced. If `None`, the entire file will
be overwritten.
Returns:
`ToolResponse`:
The tool response containing the result of the writing operation.
"""
if not os.path.exists(file_path):
with open(file_path, "w", encoding="utf-8") as file:
file.write(content)
if ranges:
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Create and write {file_path} successfully. "
f"The ranges {ranges} is ignored because the "
f"file does not exist.",
),
],
)
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Create and write {file_path} successfully.",
),
],
)
with open(file_path, "r", encoding="utf-8") as file:
original_lines = file.readlines()
if ranges is not None:
if (
isinstance(ranges, list)
and len(ranges) == 2
and all(isinstance(i, int) for i in ranges)
):
# Replace content in the specified range
start, end = ranges
if start > len(original_lines):
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error: The start line {start} is invalid. "
f"The file only has {len(original_lines)} "
f"lines.",
),
],
)
new_content = (
original_lines[: start - 1]
+ [
content,
]
+ original_lines[end:]
)
with open(file_path, "w", encoding="utf-8") as file:
file.write("".join(new_content))
# The written content may contain multiple "\n", to avoid mis
# counting the lines, we read the file again to get the new content
with open(file_path, "r", encoding="utf-8") as file:
new_lines = file.readlines()
view_start, view_end = _calculate_view_ranges(
len(original_lines),
len(new_lines),
start,
end,
)
content = "".join(
[
f"{index + view_start}: {line}"
for index, line in enumerate(
new_lines[view_start - 1 : view_end],
)
],
)
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"""Write {file_path} successfully. The new content snippet:
```
{content}```""",
),
],
)
else:
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Error: Invalid range format. Expected a list "
f"of two integers, but got {ranges}.",
),
],
)
with open(file_path, "w", encoding="utf-8") as file:
file.write(content)
return ToolResponse(
content=[
TextBlock(
type="text",
text=f"Overwrite {file_path} successfully.",
),
],
)
==== tracing ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The tracing interface class in agentscope."""
from ._setup import setup_tracing
from ._trace import (
trace,
trace_llm,
trace_reply,
trace_format,
trace_toolkit,
trace_embedding,
)
__all__ = [
"setup_tracing",
"trace",
"trace_llm",
"trace_reply",
"trace_format",
"trace_toolkit",
"trace_embedding",
]
---- _attributes.py ----
# -*- coding: utf-8 -*-
"""Attributes processor for span attributes."""
import datetime
import enum
import inspect
import json
from dataclasses import is_dataclass
from typing import Any
from pydantic import BaseModel
from ..message import Msg
def _to_serializable(
obj: Any,
) -> Any:
"""Convert an object to a JSON serializable type.
Args:
obj (`Any`):
The object to be converted to JSON serializable.
Returns:
`Any`:
The converted JSON serializable object
"""
# Handle primitive types first
if isinstance(obj, (str, int, bool, float, type(None))):
res = obj
elif isinstance(obj, (list, tuple, set, frozenset)):
res = [_to_serializable(x) for x in obj]
elif isinstance(obj, dict):
res = {str(key): _to_serializable(val) for (key, val) in obj.items()}
elif isinstance(obj, (Msg, BaseModel)) or is_dataclass(obj):
res = repr(obj)
elif inspect.isclass(obj) and issubclass(obj, BaseModel):
res = repr(obj)
elif isinstance(obj, (datetime.date, datetime.datetime, datetime.time)):
res = obj.isoformat()
elif isinstance(obj, datetime.timedelta):
res = obj.total_seconds()
elif isinstance(obj, enum.Enum):
res = _to_serializable(obj.value)
else:
res = str(obj)
return res
def _serialize_to_str(value: Any) -> str:
"""Get input attributes
Args:
value (`Any`):
The input value
Returns:
`str`:
JSON serialized string of the input value
"""
try:
return json.dumps(value, ensure_ascii=False)
except TypeError:
return json.dumps(
_to_serializable(value),
ensure_ascii=False,
)
---- _setup.py ----
# -*- coding: utf-8 -*-
"""The tracing interface class in agentscope."""
from agentscope import _config
def setup_tracing(endpoint: str) -> None:
"""Set up the AgentScope tracing by configuring the endpoint URL.
Args:
endpoint (`str`):
The endpoint URL for the tracing exporter.
"""
# Lazy import
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import (
OTLPSpanExporter,
)
from opentelemetry import trace
tracer_provider = TracerProvider()
exporter = OTLPSpanExporter(endpoint=endpoint)
span_processor = BatchSpanProcessor(exporter)
tracer_provider.add_span_processor(span_processor)
trace.set_tracer_provider(tracer_provider)
_config.trace_enabled = True
---- _trace.py ----
# -*- coding: utf-8 -*-
"""The tracing decorators for agent, formatter, toolkit, chat and embedding
models."""
import inspect
from functools import wraps
from typing import (
Generator,
AsyncGenerator,
Callable,
Any,
Coroutine,
TypeVar,
TYPE_CHECKING,
)
import aioitertools
from ._attributes import _serialize_to_str
from .. import _config
from ..embedding._embedding_base import EmbeddingModelBase
from ..model._model_base import ChatModelBase
from .._logging import logger
from ._types import SpanKind, SpanAttributes
if TYPE_CHECKING:
from ..agent import AgentBase
from ..formatter import FormatterBase
from ..tool import (
Toolkit,
ToolResponse,
)
from ..message import (
Msg,
ToolUseBlock,
)
from ..embedding import EmbeddingResponse
from ..model import ChatResponse
from opentelemetry.trace import Span
else:
Toolkit = "Toolkit"
ToolResponse = "ToolResponse"
Msg = "Msg"
ToolUseBlock = "ToolUseBlock"
EmbeddingResponse = "EmbeddingResponse"
ChatResponse = "ChatResponse"
Span = "Span"
T = TypeVar("T")
def _check_tracing_enabled() -> bool:
"""Check if the OpenTelemetry tracer is initialized in AgentScope with an
endpoint.
TODO: We expect an OpenTelemetry official interface to check if the
tracer is initialized. Leaving this function here as a temporary
solution.
"""
return _config.trace_enabled
def _trace_sync_generator_wrapper(
res: Generator[T, None, None],
span: Span,
) -> Generator[T, None, None]:
"""Trace the sync generator output with OpenTelemetry."""
import opentelemetry
has_error = False
try:
last_chunk = None
for chunk in res:
last_chunk = chunk
yield chunk
except Exception as e:
has_error = True
span.set_status(opentelemetry.trace.StatusCode.ERROR, str(e))
span.record_exception(e)
raise e from None
finally:
if not has_error:
# Set the last chunk as output
span.set_attributes(
{
SpanAttributes.OUTPUT: _serialize_to_str(last_chunk),
},
)
span.set_status(opentelemetry.trace.StatusCode.OK)
span.end()
async def _trace_async_generator_wrapper(
res: AsyncGenerator[T, None],
span: Span,
) -> AsyncGenerator[T, None]:
"""Trace the async generator output with OpenTelemetry.
Args:
res (`AsyncGenerator[T, None]`):
The generator or async generator to be traced.
span (`Span`):
The OpenTelemetry span to be used for tracing.
Yields:
`T`:
The output of the async generator.
"""
import opentelemetry
has_error = False
try:
last_chunk = None
async for chunk in aioitertools.iter(res):
last_chunk = chunk
yield chunk
except Exception as e:
has_error = True
span.set_status(opentelemetry.trace.StatusCode.ERROR, str(e))
span.record_exception(e)
raise e from None
finally:
if not has_error:
# Set the last chunk as output
span.set_attributes(
{
SpanAttributes.OUTPUT: _serialize_to_str(last_chunk),
},
)
span.set_status(opentelemetry.trace.StatusCode.OK)
span.end()
def trace(
name: str,
) -> Callable:
"""A generic tracing decorator for synchronous and asynchronous functions.
Args:
name (`str`):
The name of the span to be created.
Returns:
`Callable`:
Returns a decorator that wraps the given function with
OpenTelemetry tracing.
"""
def decorator(
func: Callable,
) -> Callable:
"""A decorator that wraps the given function with OpenTelemetry tracing
Args:
func (`Callable`):
The function to be traced, which can be sync or async function,
and returns an object or a generator.
Returns:
`Callable`:
A wrapper function that traces the function call and handles
input/output and exceptions.
"""
# Async function
if inspect.iscoroutinefunction(func):
@wraps(func)
async def wrapper(
*args: Any,
**kwargs: Any,
) -> Any:
"""The wrapper function for tracing the sync function call."""
if not _check_tracing_enabled():
return await func(*args, **kwargs)
import opentelemetry
tracer = opentelemetry.trace.get_tracer(__name__)
attributes = {
SpanAttributes.SPAN_KIND: SpanKind.COMMON,
SpanAttributes.PROJECT_RUN_ID: _serialize_to_str(
_config.run_id,
),
SpanAttributes.INPUT: _serialize_to_str(
{
"args": args,
"kwargs": kwargs,
},
),
SpanAttributes.META: _serialize_to_str({}),
}
with tracer.start_as_current_span(
name=name,
attributes=attributes,
end_on_exit=False,
) as span:
try:
res = await func(*args, **kwargs)
# If generator or async generator
if isinstance(res, AsyncGenerator):
return _trace_async_generator_wrapper(res, span)
if isinstance(res, Generator):
return _trace_sync_generator_wrapper(res, span)
# non-generator result
span.set_attributes(
{SpanAttributes.OUTPUT: _serialize_to_str(res)},
)
span.set_status(opentelemetry.trace.StatusCode.OK)
span.end()
return res
except Exception as e:
span.set_status(
opentelemetry.trace.StatusCode.ERROR,
str(e),
)
span.record_exception(e)
span.end()
raise e from None
return wrapper
# Sync function
@wraps(func)
def sync_wrapper(
*args: Any,
**kwargs: Any,
) -> Any:
"""The wrapper function for tracing the sync function call."""
if not _check_tracing_enabled():
return func(*args, **kwargs)
import opentelemetry
tracer = opentelemetry.trace.get_tracer(__name__)
attributes = {
SpanAttributes.SPAN_KIND: SpanKind.COMMON,
SpanAttributes.PROJECT_RUN_ID: _serialize_to_str(
_config.run_id,
),
SpanAttributes.INPUT: _serialize_to_str(
{
"args": args,
"kwargs": kwargs,
},
),
SpanAttributes.META: _serialize_to_str({}),
}
with tracer.start_as_current_span(
name=name,
attributes=attributes,
end_on_exit=False,
) as span:
try:
res = func(*args, **kwargs)
# If generator or async generator
if isinstance(res, AsyncGenerator):
return _trace_async_generator_wrapper(res, span)
if isinstance(res, Generator):
return _trace_sync_generator_wrapper(res, span)
# non-generator result
span.set_attributes(
{SpanAttributes.OUTPUT: _serialize_to_str(res)},
)
span.set_status(opentelemetry.trace.StatusCode.OK)
span.end()
return res
except Exception as e:
span.set_status(
opentelemetry.trace.StatusCode.ERROR,
str(e),
)
span.record_exception(e)
span.end()
raise e from None
return sync_wrapper
return decorator
def trace_toolkit(
func: Callable[
...,
Coroutine[Any, Any, AsyncGenerator[ToolResponse, None]],
],
) -> Callable[..., Coroutine[Any, Any, AsyncGenerator[ToolResponse, None]]]:
"""Trace the toolkit `call_tool_function` method with OpenTelemetry."""
@wraps(func)
async def wrapper(
self: Toolkit,
tool_call: ToolUseBlock,
) -> AsyncGenerator[ToolResponse, None]:
"""The wrapper function for tracing the toolkit call_tool_function
method."""
if not _check_tracing_enabled():
return await func(self, tool_call=tool_call)
import opentelemetry
tracer = opentelemetry.trace.get_tracer(__name__)
# Prepare the attributes for the span
attributes = {
SpanAttributes.SPAN_KIND: _serialize_to_str(SpanKind.TOOL),
SpanAttributes.PROJECT_RUN_ID: _serialize_to_str(_config.run_id),
SpanAttributes.INPUT: _serialize_to_str(
{
"tool_call": tool_call,
},
),
SpanAttributes.META: _serialize_to_str(
{
**tool_call,
},
),
}
with tracer.start_as_current_span(
f"{func.__name__}",
attributes=attributes,
end_on_exit=False,
) as span:
try:
# Call the toolkit function
res = await func(self, tool_call=tool_call)
# The result must be an AsyncGenerator of ToolResponse objects
return _trace_async_generator_wrapper(res, span)
except Exception as e:
span.set_status(
opentelemetry.trace.StatusCode.ERROR,
str(e),
)
span.record_exception(e)
span.end()
raise e from None
return wrapper
def trace_reply(
func: Callable[..., Coroutine[Any, Any, Msg]],
) -> Callable[..., Coroutine[Any, Any, Msg]]:
"""Trace the agent reply call with OpenTelemetry.
Args:
func (`Callable[..., Coroutine[Any, Any, Msg]]`):
The agent async reply function to be traced.
Returns:
`Callable[..., Coroutine[Any, Any, Msg]]`:
A wrapper function that traces the agent reply call and handles
input/output and exceptions.
"""
@wraps(func)
async def wrapper(
self: "AgentBase",
*args: Any,
**kwargs: Any,
) -> Msg:
"""The wrapper function for tracing the agent reply function call."""
if not _check_tracing_enabled():
return await func(self, *args, **kwargs)
from ..agent import AgentBase
if not isinstance(self, AgentBase):
logger.warning(
"Skipping tracing for %s as the first argument"
"is not an instance of AgentBase, but %s",
func.__name__,
type(self),
)
return await func(self, *args, **kwargs)
import opentelemetry
tracer = opentelemetry.trace.get_tracer(__name__)
# Prepare the attributes for the span
agent_name = self.name if hasattr(self, "name") else None
attributes = {
SpanAttributes.SPAN_KIND: _serialize_to_str(SpanKind.AGENT),
SpanAttributes.PROJECT_RUN_ID: _serialize_to_str(_config.run_id),
SpanAttributes.INPUT: _serialize_to_str(
{
"args": args,
"kwargs": kwargs,
},
),
SpanAttributes.META: _serialize_to_str(
{
"id": self.id,
"name": agent_name,
},
),
}
with tracer.start_as_current_span(
f"{self.__class__.__name__}.{func.__name__}",
attributes=attributes,
end_on_exit=False,
) as span:
try:
# Call the agent reply function
res = await func(self, *args, **kwargs)
# Set the output attribute
span.set_attributes(
{SpanAttributes.OUTPUT: _serialize_to_str(res)},
)
span.set_status(opentelemetry.trace.StatusCode.OK)
span.end()
return res
except Exception as e:
span.set_status(
opentelemetry.trace.StatusCode.ERROR,
str(e),
)
span.record_exception(e)
span.end()
raise e from None
return wrapper
def trace_embedding(
func: Callable[..., Coroutine[Any, Any, EmbeddingResponse]],
) -> Callable[..., Coroutine[Any, Any, EmbeddingResponse]]:
"""Trace the embedding call with OpenTelemetry."""
@wraps(func)
async def wrapper(
self: EmbeddingModelBase,
*args: Any,
**kwargs: Any,
) -> EmbeddingResponse:
"""The wrapper function for tracing the embedding call."""
if not _check_tracing_enabled():
return await func(self, *args, **kwargs)
if not isinstance(self, EmbeddingModelBase):
logger.warning(
"Skipping tracing for %s as the first argument"
"is not an instance of EmbeddingModelBase, but %s",
func.__name__,
type(self),
)
return await func(self, *args, **kwargs)
import opentelemetry
tracer = opentelemetry.trace.get_tracer(__name__)
# Prepare the attributes for the span
attributes = {
SpanAttributes.SPAN_KIND: _serialize_to_str(SpanKind.EMBEDDING),
SpanAttributes.PROJECT_RUN_ID: _serialize_to_str(_config.run_id),
SpanAttributes.INPUT: _serialize_to_str(
{
"args": args,
"kwargs": kwargs,
},
),
SpanAttributes.META: _serialize_to_str(
{
"model_name": self.model_name,
},
),
}
with tracer.start_as_current_span(
f"{self.__class__.__name__}.{func.__name__}",
attributes=attributes,
end_on_exit=False,
) as span:
try:
# Call the embedding function
res = await func(self, *args, **kwargs)
# Set the output attribute
span.set_attributes(
{SpanAttributes.OUTPUT: _serialize_to_str(res)},
)
span.set_status(opentelemetry.trace.StatusCode.OK)
span.end()
return res
except Exception as e:
span.set_status(
opentelemetry.trace.StatusCode.ERROR,
str(e),
)
span.record_exception(e)
span.end()
raise e from None
return wrapper
def trace_format(
func: Callable[..., Coroutine[Any, Any, list[dict]]],
) -> Callable[..., Coroutine[Any, Any, list[dict]]]:
"""Trace the format function of the formatter with OpenTelemetry.
Args:
func (`Callable[..., Coroutine[Any, Any, list[dict]]]`):
The async format function to be traced.
Returns:
`Callable[..., Coroutine[Any, Any, list[dict]]]`:
An async wrapper function that traces the format call and handles
input/output and exceptions.
"""
@wraps(func)
async def wrapper(
self: "FormatterBase",
*args: Any,
**kwargs: Any,
) -> list[dict]:
"""Wrap the formatter __call__ method with OpenTelemetry tracing."""
if not _check_tracing_enabled():
return await func(self, *args, **kwargs)
from ..formatter import FormatterBase
if not isinstance(self, FormatterBase):
logger.warning(
"Skipping tracing for %s as the first argument"
"is not an instance of FormatterBase, but %s",
func.__name__,
type(self),
)
return await func(self, *args, **kwargs)
import opentelemetry
tracer = opentelemetry.trace.get_tracer(__name__)
# Prepare the attributes for the span
attributes = {
SpanAttributes.SPAN_KIND: _serialize_to_str(SpanKind.FORMATTER),
SpanAttributes.PROJECT_RUN_ID: _serialize_to_str(_config.run_id),
SpanAttributes.INPUT: _serialize_to_str(
{
"args": args,
"kwargs": kwargs,
},
),
SpanAttributes.META: _serialize_to_str({}),
}
with tracer.start_as_current_span(
f"{self.__class__.__name__}.{func.__name__}",
attributes=attributes,
end_on_exit=False,
) as span:
try:
# Call the formatter function
res = await func(self, *args, **kwargs)
# Set the output attribute
span.set_attributes(
{SpanAttributes.OUTPUT: _serialize_to_str(res)},
)
span.set_status(opentelemetry.trace.StatusCode.OK)
span.end()
return res
except Exception as e:
span.set_status(
opentelemetry.trace.StatusCode.ERROR,
str(e),
)
span.record_exception(e)
span.end()
raise e from None
return wrapper
def trace_llm(
func: Callable[
...,
Coroutine[
Any,
Any,
ChatResponse | AsyncGenerator[ChatResponse, None],
],
],
) -> Callable[
...,
Coroutine[Any, Any, ChatResponse | AsyncGenerator[ChatResponse, None]],
]:
"""Trace the LLM call with OpenTelemetry.
Args:
func (`Callable`):
The function to be traced, which should be a coroutine that
returns either a `ChatResponse` or an `AsyncGenerator`
of `ChatResponse`.
Returns:
`Callable`:
A wrapper function that traces the LLM call and handles
input/output and exceptions.
"""
@wraps(func)
async def async_wrapper(
self: ChatModelBase,
*args: Any,
**kwargs: Any,
) -> ChatResponse | AsyncGenerator[ChatResponse, None]:
"""The wrapper function for tracing the LLM call."""
if not _check_tracing_enabled():
return await func(self, *args, **kwargs)
if not isinstance(self, ChatModelBase):
logger.warning(
"Skipping tracing for %s as the first argument"
"is not an instance of ChatModelBase, but %s",
func.__name__,
type(self),
)
return await func(self, *args, **kwargs)
import opentelemetry
tracer = opentelemetry.trace.get_tracer(__name__)
# Prepare the attributes for the span
attributes = {
SpanAttributes.SPAN_KIND: _serialize_to_str(SpanKind.LLM),
SpanAttributes.PROJECT_RUN_ID: _serialize_to_str(_config.run_id),
SpanAttributes.INPUT: _serialize_to_str(
{
"args": args,
"kwargs": kwargs,
},
),
SpanAttributes.META: _serialize_to_str(
{
"model_name": self.model_name,
"stream": self.stream,
},
),
}
# Begin the llm call span
with tracer.start_as_current_span(
f"{self.__class__.__name__}.__call__",
attributes=attributes,
end_on_exit=False,
) as span:
try:
# Must be an async calling
res = await func(self, *args, **kwargs)
# If the result is a AsyncGenerator
if isinstance(res, AsyncGenerator):
return _trace_async_generator_wrapper(res, span)
# non-generator result
span.set_attributes(
{SpanAttributes.OUTPUT: _serialize_to_str(res)},
)
span.set_status(opentelemetry.trace.StatusCode.OK)
span.end()
return res
except Exception as e:
span.set_status(
opentelemetry.trace.StatusCode.ERROR,
str(e),
)
span.record_exception(e)
span.end()
raise e from None
return async_wrapper
---- _types.py ----
# -*- coding: utf-8 -*-
"""The tracing types class in agentscope."""
from enum import Enum
from opentelemetry import trace
StatusCode = trace.StatusCode
class SpanKind(str, Enum):
"""The span kind."""
AGENT = "AGENT"
TOOL = "TOOL"
LLM = "LLM"
EMBEDDING = "EMBEDDING"
FORMATTER = "FORMATTER"
COMMON = "COMMON"
class SpanAttributes:
"""The span attributes."""
SPAN_KIND = "span.kind"
OUTPUT = "output"
INPUT = "input"
META = "metadata"
PROJECT_RUN_ID = "project.run_id"
==== types ====
---- __init__.py ----
# -*- coding: utf-8 -*-
"""The types in agentscope"""
from ._hook import (
AgentHookTypes,
ReActAgentHookTypes,
)
from ._object import Embedding
from ._json import (
JSONPrimitive,
JSONSerializableObject,
)
from ._tool import ToolFunction
__all__ = [
"AgentHookTypes",
"ReActAgentHookTypes",
"Embedding",
"JSONPrimitive",
"JSONSerializableObject",
"ToolFunction",
]
---- _hook.py ----
# -*- coding: utf-8 -*-
"""The agent hooks types."""
from typing import Literal
AgentHookTypes = (
str
| Literal[
"pre_reply",
"post_reply",
"pre_print",
"post_print",
"pre_observe",
"post_observe",
]
)
ReActAgentHookTypes = (
AgentHookTypes
| Literal[
"pre_reasoning",
"post_reasoning",
"pre_acting",
"post_acting",
]
)
---- _json.py ----
# -*- coding: utf-8 -*-
"""The JSON related types"""
from typing import Union
JSONPrimitive = Union[
str,
int,
float,
bool,
None,
]
JSONSerializableObject = Union[
JSONPrimitive,
list["JSONSerializableObject"],
dict[
str,
"JSONSerializableObject",
],
]
---- _object.py ----
# -*- coding: utf-8 -*-
"""The object types in agentscope."""
from typing import List
Embedding = List[float]
---- _tool.py ----
# -*- coding: utf-8 -*-
"""The tool related types"""
from typing import (
Callable,
Union,
Awaitable,
AsyncGenerator,
Generator,
Coroutine,
Any,
TYPE_CHECKING,
)
if TYPE_CHECKING:
from ..tool import ToolResponse
else:
ToolResponse = "ToolResponse"
ToolFunction = Callable[
...,
Union[
# sync function
ToolResponse,
# async function
Awaitable[ToolResponse],
# sync generator function
Generator[ToolResponse, None, None],
# async generator function
AsyncGenerator[ToolResponse, None],
# async function that returns async generator
Coroutine[Any, Any, AsyncGenerator[ToolResponse, None]],
# async function that returns sync generator
Coroutine[Any, Any, Generator[ToolResponse, None, None]],
],
]